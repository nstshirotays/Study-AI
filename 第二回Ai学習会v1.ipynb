{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nstshirotays/Study-AI/blob/master/%E7%AC%AC%E4%BA%8C%E5%9B%9EAi%E5%AD%A6%E7%BF%92%E4%BC%9Av1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第二回Ai勉強会\n",
        "\n",
        "![タイトル](http://drive.google.com/uc?export=view&id=1Xjp7AonDpKigq7dreuvhVTyCPQ90BsYE)\n",
        "\n",
        "<hr/>\n",
        "\n",
        "## 前回のおさらい\n",
        "\n",
        "![前回のおさらい](https://drive.google.com/uc?export=view&id=1dgkdwz5-L1NhMkLgACW8JuwxGlfhiG_r)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![Python基本文法](https://drive.google.com/uc?export=view&id=1kVr-tfDFvfHmmEJ-di3oTv-493ppDqU_)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![Python Numpy](https://drive.google.com/uc?export=view&id=1NUCUxdD7fwiwf4cZDfhA7KTAuwHhrjn2)\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![Aiの歴史](https://drive.google.com/uc?export=view&id=1r0LA5wAcL84j5kQ265BKo1GvLO7fLqyc)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![NNの基本構造](https://drive.google.com/uc?export=view&id=1WEd9yb5dzQqATOwSn8uAwCDxXI8gcKRp)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![BP:Step1](https://drive.google.com/uc?export=view&id=1x85KWs_84sCm80BH_vWDOPAhEACJJT4H)\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![BP:Step2](https://drive.google.com/uc?export=view&id=1rd0BEz2byTHSZkenx5vX-l-r7pg9cZqj)\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![BP:Step3](https://drive.google.com/uc?export=view&id=18kuv5Gm1T5Hw6UCDA9966egENIq8D0Ir)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![BP:Step4](https://drive.google.com/uc?export=view&id=1tqKOqtQwz3dfv7ErQhwCZGwBJ9YzfOVM )\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![実際の学習工程](https://drive.google.com/uc?export=view&id=1n5zGSybJYETT_btvfQK9dI1V0U9obX2z)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![CNN](https://drive.google.com/uc?export=view&id=1P8xBYbEtsXyC-aUJ9z_TiEzFiidgK1Eu)\n",
        "\n"
      ],
      "metadata": {
        "id": "nR3KSJMniF9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![title1](https://drive.google.com/uc?export=view&id=11tUva1mAwPPQe49Egc9u9rGgBo4ZgRfB)\n",
        "\n"
      ],
      "metadata": {
        "id": "jfxF30Adkott"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![MLP](https://drive.google.com/uc?export=view&id=1rUr48lzbZePCPiWQ1DS_mT7oz0__tB9g)\n",
        "\n"
      ],
      "metadata": {
        "id": "XDceBD_VkswB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lTXC0zRgVDl",
        "outputId": "10588b90-e1f2-452b-d0b4-74482866da2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               196250    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               25100     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222360 (868.59 KB)\n",
            "Trainable params: 222360 (868.59 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 16s 6ms/step - loss: 0.2137 - accuracy: 0.9359\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0872 - accuracy: 0.9722\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0598 - accuracy: 0.9811\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0440 - accuracy: 0.9856\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0347 - accuracy: 0.9888\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0285 - accuracy: 0.9904\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0227 - accuracy: 0.9924\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0200 - accuracy: 0.9935\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0203 - accuracy: 0.9932\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0134 - accuracy: 0.9953\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1021 - accuracy: 0.9776\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.102086640894413, 0.9775999784469604]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# データの準備\n",
        "# MNISTデータセットをロードし、訓練データとテストデータに分けます。\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# データを浮動小数点型に変換し、0から1の範囲に正規化します。\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# モデルの定義\n",
        "# Sequentialモデルを使用して、層を順に積み重ねていきます。\n",
        "model = keras.Sequential([\n",
        "  # Flatten層で28x28ピクセルの画像を784次元のベクトルに変換します。\n",
        "  layers.Flatten(input_shape=(28, 28)),\n",
        "  # 250ユニットの全結合層（Dense層）を追加し、活性化関数としてReLUを使用します。\n",
        "  layers.Dense(250, activation='relu'),\n",
        "  # 100ユニットの全結合層を追加し、活性化関数としてReLUを使用します。\n",
        "  layers.Dense(100, activation='relu'),\n",
        "  # 10ユニットの出力層を追加し、活性化関数としてsoftmaxを使用して多クラス分類を行います。\n",
        "  layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# モデルのコンパイル\n",
        "# モデルをコンパイルし、最適化アルゴリズムとしてAdamを、\n",
        "# 損失関数として交差エントロピー（sparse_categorical_crossentropy）を、\n",
        "# 評価指標として正答率（accuracy）を使用します。\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# モデルのサマリーを表示\n",
        "# モデルの構造を表示します。\n",
        "model.summary()\n",
        "\n",
        "# モデルの可視化\n",
        "# モデルの構造を図として保存し、層の形状と名前を表示します。\n",
        "plot_model(model, to_file='model_simple_mlp.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# モデルの学習\n",
        "# モデルを訓練データで学習させます。ここではエポック数を10に設定しています。\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# モデルの評価\n",
        "# テストデータを使用してモデルを評価し、精度を確認します。\n",
        "model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![CNN](https://drive.google.com/uc?export=view&id=1gfn2wf0YQ1y4P99EBdyNM2bkzdcE4-J6)\n"
      ],
      "metadata": {
        "id": "tYQRxAmOkv28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras\n",
        "from keras.applications import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# データの準備\n",
        "# MNISTデータセットをロードし、訓練データとテストデータに分けます。\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# データを浮動小数点型に変換し、0から1の範囲に正規化します。\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# モデルの定義\n",
        "# Sequentialモデルを使用して、畳み込み層や全結合層を順に積み重ねていきます。\n",
        "model = keras.Sequential([\n",
        "  # 32個のフィルタを持つ3x3の畳み込み層を追加します。入力形状は28x28ピクセルのグレースケール画像です。\n",
        "  layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  # 2x2のプーリング層を追加します。\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  # さらに64個のフィルタを持つ3x3の畳み込み層を追加します。\n",
        "  layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "  # もう一つの2x2のプーリング層を追加します。\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  # Flatten層で畳み込み層からの出力を1次元配列に変換します。\n",
        "  layers.Flatten(),\n",
        "  # 250ユニットの全結合層を追加します。\n",
        "  layers.Dense(250, activation='relu'),\n",
        "  # 100ユニットの全結合層を追加します。\n",
        "  layers.Dense(100, activation='relu'),\n",
        "  # 10ユニットの出力層を追加します。活性化関数としてsoftmaxを使用します。\n",
        "  layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# モデルのコンパイル\n",
        "# モデルをコンパイルし、最適化アルゴリズムとしてAdamを、\n",
        "# 損失関数として交差エントロピー（sparse_categorical_crossentropy）を、\n",
        "# 評価指標として正答率（accuracy）を使用します。\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# モデルのサマリーを表示\n",
        "# モデルの構造を表示します。\n",
        "model.summary()\n",
        "\n",
        "# モデルの可視化\n",
        "# モデルの構造を図として保存し、層の形状と名前を表示します。\n",
        "plot_model(model, to_file='model_cnn.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# モデルの学習\n",
        "# モデルを訓練データで学習させます。ここではエポック数を10に設定しています。\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# モデルの評価\n",
        "# テストデータを使用してモデルを評価し、精度を確認します。\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "# モデルの保存\n",
        "# 学習済みのモデルをHDF5ファイルとして保存します。\n",
        "model.save('cnn_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "jFcQHhIfwLLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3669cb37-bab0-489a-a178-a989b7eee518"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 250)               400250    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               25100     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 445176 (1.70 MB)\n",
            "Trainable params: 445176 (1.70 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 15s 6ms/step - loss: 0.1289 - accuracy: 0.9602\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0438 - accuracy: 0.9864\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0316 - accuracy: 0.9903\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0238 - accuracy: 0.9924\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0173 - accuracy: 0.9944\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0140 - accuracy: 0.9956\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0122 - accuracy: 0.9962\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0105 - accuracy: 0.9967\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0085 - accuracy: 0.9974\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0082 - accuracy: 0.9975\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0467 - accuracy: 0.9895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![Mobile&Image](https://drive.google.com/uc?export=view&id=1lTxht54AGG8GBIN0WAXz44EwzPJYk7K-)\n"
      ],
      "metadata": {
        "id": "YFY6Po2Qk2dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf study-ai-data01/\n",
        "!git clone https://github.com/acp-tech-heroes/study-ai-data01.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKzY7lA5ZecW",
        "outputId": "38ab1d1f-e818-46aa-c259-6749d9d9a334"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'study-ai-data01'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 85 (delta 1), reused 5 (delta 0), pack-reused 71\u001b[K\n",
            "Receiving objects: 100% (85/85), 87.29 MiB | 15.88 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "sys.path.append('./study-ai-data01')\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input, decode_predictions\n",
        "from ilsvrc2012_japanese import Ilsvrc2012Japanese\n",
        "\n",
        "# MobileNetモデルのロード（重みはImageNetを使用）\n",
        "model = MobileNet(weights='imagenet')\n",
        "\n",
        "# 画像ファイル名\n",
        "file_name = 'study-ai-data01/dog.jpg'\n",
        "\n",
        "# 画像の読み込みと前処理\n",
        "# 画像を読み込みます。target_sizeを(224, 224)に設定することで、\n",
        "# 画像がMobileNetモデルが要求する入力サイズにリサイズされます。\n",
        "img = image.load_img(file_name, target_size=(224, 224))\n",
        "\n",
        "# 読み込んだ画像をNumPy配列に変換します。\n",
        "# この操作により、画像は数値の配列に変換され、モデルによる処理が可能になります。\n",
        "img_array = image.img_to_array(img)\n",
        "\n",
        "# モデルに入力するために配列の次元を変更します。\n",
        "# np.expand_dimsは指定された軸に沿って新しい次元を追加します。\n",
        "# ここでは、バッチサイズの次元を追加しています（バッチサイズ=1）。\n",
        "# この操作により、画像配列がモデルが期待する形式になります。\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# モデルに適した形で画像データを前処理します。\n",
        "# preprocess_inputは、MobileNetモデルに適した画像の前処理を行い、\n",
        "# ピクセル値のスケーリングなどを適切に調整します。\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "# 画像の分類\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# 日本語変換クラスのインスタンス化\n",
        "converter = Ilsvrc2012Japanese()\n",
        "\n",
        "# 予測結果の英語ラベルを日本語に変換し、確率も表示\n",
        "for pred in decode_predictions(predictions, top=5)[0]:\n",
        "    english_label, probability = pred[1], pred[2]\n",
        "    japanese_label = converter.convert(english_label)\n",
        "    print(f\"Predicted: {english_label} ({japanese_label}), Probability: {probability:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MKIiJ1hiJAG",
        "outputId": "6d90ddc7-f695-48b1-97bc-9f696eabdf0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
            "17225924/17225924 [==============================] - 2s 0us/step\n",
            "1/1 [==============================] - 1s 988ms/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 0us/step\n",
            "Predicted: cocker_spaniel (イングリッシュ・コッカー・スパニエル（犬）), Probability: 0.57\n",
            "Predicted: Welsh_springer_spaniel (ウェルシュ・スプリンガー・スパニエル（犬）), Probability: 0.27\n",
            "Predicted: clumber (クランバー), Probability: 0.09\n",
            "Predicted: Sussex_spaniel (サセックス・スパニエル（犬）), Probability: 0.03\n",
            "Predicted: Irish_setter (犬（アイリッシュセッター）), Probability: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![kyouka](https://drive.google.com/uc?export=view&id=117aDiHHNi2-xbGzipPQCrxsXHhvd26BN)\n",
        "\n"
      ],
      "metadata": {
        "id": "DGKkjO70k6JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# データセットのパス\n",
        "train_dir = 'study-ai-data01/janken-train'  # 訓練データのパスを設定\n",
        "validation_dir = 'study-ai-data01/janken-validate'  # テストデータのパスを設定\n",
        "\n",
        "# データ拡張\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# データの読み込み\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=9,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=9,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# MobileNetの読み込み\n",
        "base_model = MobileNet(weights='imagenet', include_top=False)\n",
        "\n",
        "# カスタム層の追加\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)  # 3クラス用の出力層\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# トップレイヤー以外のレイヤーを凍結\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# モデルの訓練\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLO-HKYqToq_",
        "outputId": "bf67fffc-55e5-4772-cea7-44341ab8b491"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 36 images belonging to 3 classes.\n",
            "Found 9 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 6s 1s/step - loss: 6.2430 - accuracy: 0.3611 - val_loss: 1.0078 - val_accuracy: 0.5556\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 3s 864ms/step - loss: 1.8154 - accuracy: 0.6667 - val_loss: 0.8020 - val_accuracy: 0.7778\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 3s 846ms/step - loss: 0.3538 - accuracy: 0.8611 - val_loss: 0.5389 - val_accuracy: 0.7778\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 3s 833ms/step - loss: 0.6462 - accuracy: 0.7778 - val_loss: 2.4517 - val_accuracy: 0.6667\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 3s 854ms/step - loss: 0.6367 - accuracy: 0.8611 - val_loss: 0.4424 - val_accuracy: 0.7778\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.2766 - accuracy: 0.9167 - val_loss: 0.3899 - val_accuracy: 0.7778\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 3s 839ms/step - loss: 1.0567 - accuracy: 0.5833 - val_loss: 0.5666 - val_accuracy: 0.7778\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 3s 828ms/step - loss: 0.3176 - accuracy: 0.9167 - val_loss: 1.2271 - val_accuracy: 0.7778\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 3s 851ms/step - loss: 0.1498 - accuracy: 0.9444 - val_loss: 1.0655 - val_accuracy: 0.7778\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 3s 849ms/step - loss: 0.0945 - accuracy: 1.0000 - val_loss: 0.3352 - val_accuracy: 0.7778\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e67d8982a70>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![fine](https://drive.google.com/uc?export=view&id=13OX1Xo3dZva80QUWuNI4QACDGcxb1t4V)"
      ],
      "metadata": {
        "id": "osxoHX9kk-Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# データセットのパス\n",
        "train_dir = 'study-ai-data01/janken-train'\n",
        "validation_dir = 'study-ai-data01/janken-validate'\n",
        "\n",
        "# MobileNetモデルの読み込み\n",
        "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# 上位層の凍結解除\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[100:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# 新しいトップ層の追加\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 画像データの前処理\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "# データジェネレータの定義（バッチサイズを考慮したsteps_per_epochの計算）\n",
        "batch_size = 32\n",
        "train_steps_per_epoch = np.ceil(train_generator.samples / batch_size)\n",
        "validation_steps = np.ceil(validation_generator.samples / batch_size)\n",
        "\n",
        "# モデルの訓練（steps_per_epochとvalidation_stepsを修正）\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mexwSWHY2aG8",
        "outputId": "a03880c2-673a-4ffd-f58a-eb70aa4c93f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 36 images belonging to 3 classes.\n",
            "Found 9 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 9s 5s/step - loss: 1.3368 - accuracy: 0.2778 - val_loss: 1.4427 - val_accuracy: 0.4444\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 861ms/step - loss: 1.2464 - accuracy: 0.5278 - val_loss: 1.1607 - val_accuracy: 0.4444\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 903ms/step - loss: 0.8296 - accuracy: 0.6667 - val_loss: 1.1566 - val_accuracy: 0.6667\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 3s 965ms/step - loss: 0.6462 - accuracy: 0.6389 - val_loss: 0.8085 - val_accuracy: 0.7778\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 4s 4s/step - loss: 0.3942 - accuracy: 0.8611 - val_loss: 0.5153 - val_accuracy: 0.8889\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 3s/step - loss: 0.4304 - accuracy: 0.8056 - val_loss: 0.4608 - val_accuracy: 0.8889\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 3s 876ms/step - loss: 0.4230 - accuracy: 0.8056 - val_loss: 0.7029 - val_accuracy: 0.7778\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.2925 - accuracy: 0.8889 - val_loss: 1.0111 - val_accuracy: 0.6667\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 3s/step - loss: 0.2933 - accuracy: 0.9167 - val_loss: 0.9345 - val_accuracy: 0.6667\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 3s 862ms/step - loss: 0.3396 - accuracy: 0.8611 - val_loss: 0.5363 - val_accuracy: 0.8889\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e67d8eac400>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}