{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI勉強会V1.1",
      "provenance": [],
      "collapsed_sections": [
        "Mmx0g2eG6NXt",
        "g5PqkPWW1nz7"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nstshirotays/Study-AI/blob/master/AI%E5%8B%89%E5%BC%B7%E4%BC%9AV1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-USdFhOfX6MW",
        "colab_type": "text"
      },
      "source": [
        "# AI勉強会\n",
        "この勉強会では O'REILLY「ゼロから作る Deep Learning」について解説を行い内容の理解を深めることを目的としています。\n",
        "\n",
        "## 本日のキーワード\n",
        "* 活性化関数はリールー\n",
        "* 出力層はソフトマックス\n",
        "* 損失関数は交差エントロピー\n",
        "* 底を見つけるのが勾配降下法\n",
        "* 学習率は当て推量\n",
        "* イテレーション数はPDCAの繰り返し回数\n",
        "* 計算機の能力に合わせて、一回に処理する複数のデータをミニバッチ\n",
        "* ミニバッチの総和が全量と等しいとき、１エポック\n",
        "\n",
        "<hr>\n",
        "学習の準備として下記を実行して下さい\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-PZLcS7KCHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-from-scratch.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfqLQnfynLnC",
        "colab_type": "text"
      },
      "source": [
        "# パーセプトロン\n",
        "ハイ・ローの電気信号による演算を行うCPUの内部構成は、ANDやOR演算を行う論理素子の集合体である。心理学者・計算機科学者のフランク・ローゼンブラットは、人間の脳内でもこのような演算素子と同様のプロセスが、生物学的原理で動作すると想定した。そして実際に回路を作成し画像の学習処理を行った（1957年）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ihZ8xE6aCN",
        "colab_type": "text"
      },
      "source": [
        "## パーセプトロンの概要\n",
        "\n",
        "\n",
        "![論理記号](https://docs.google.com/drawings/d/e/2PACX-1vQ2SKuRbOhYXSsd1vbFCWV3iJzEBX2LUoM6QSHY6jpWt1hiOzBMgbzeRm-Pqs5yS-qdAoZBO57Jxr4G/pub?w=390&h=365)\n",
        "\n",
        "電気回路的な演算素子はそれぞれに実体が異なるが、脳内ではすべて同様の組織体から構成されると想定し、同一の細胞（ニューロン）が外部のコントロールにより様々な２値（真偽値）を出力すると考察し、まずはニューロンを下記のようにモデル化した。\n",
        "\n",
        "![ニューロンモデル](https://docs.google.com/drawings/d/e/2PACX-1vSinEodXTx3n5L_cMTGqzbDcKC1y6ncEjm0vjk5wHCiDPIT476aqJxuNiiniflzJvy0zjylvyEz0vTP/pub?w=302&h=75)\n",
        "\n",
        "\n",
        "この円形はニューロンを表す。ニューロンへの入力は矢印で表され、それぞれの入力信号に対して重み付けがされる。これらの情報を元にニューロンYは入力データに重みを乗算した値を閾値θと比較して出力を０か１の二値に振り分ける。これを一般化し下記のような構成にしたものがパーセプトロンである。\n",
        "\n",
        "![パーセプトロン](https://docs.google.com/drawings/d/e/2PACX-1vSpMVD85wOprci70XqNYFpaA3gNI_rTRVezumjEJ6trAHQ6qMq7gPA-PH7bDzHfOkMb-pEKQXxSOvs_/pub?w=302&h=220)\n",
        "\n",
        "パーセプトロンでは、複数のニューロンからの入力値（X1、X2、、、）にそれぞれの重み（W1、W2、、、、）を掛けた値を合算し、さらに前述のモデルで閾値θを外部からの入力値として引き出す。これをバイアスと改めて再定義し、それらの総合計がゼロ以下であればゼロを、そうでなければ１を結果として出力するというモデルである。\n",
        "\n",
        "CPUにおける演算素子はそれぞれ固有のデバイスであるが、このパーセプトロンでは信号の重みであるWとバイアス値であるｂの値を工夫することで、ニューロンの構成を変えずにAND、OR、NANDの演算を行うことができる。\n",
        "\n",
        "この発見により、神経伝達をそのモデルとしたパーセプトロンはコンピューターのCPUと等価と見なせる事となり、人間の脳活動をコンピューターで摸倣できる可能性が示された。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-RfG8FikpGb",
        "colab_type": "text"
      },
      "source": [
        "## ANDパーセプトロンの例\n",
        "X1とX2のAND（論理積）Yは以下の表の通りとなる。\n",
        "\n",
        "![真理値（AND)](https://docs.google.com/drawings/d/e/2PACX-1vRBDAVLdjYPZdwKgzW7KjJChXCcVpuf2F4BV37uJsMeq9RiKaEOiBm1ku6IWWPfIIAoYMAsXsFP5y4r/pub?w=110&h=104)\n",
        "\n",
        "これをパーセプトロンで実現する場合は例として下記のように重みWとバイアスｂを設定する。\n",
        "\n",
        "![ANDパーセプトロン](https://docs.google.com/drawings/d/e/2PACX-1vS0_fzuAskoeaRAzwEG5YW7YVbXQbVppfbPCA64l-KFjYmMjRuCpsCgvR1Y-k2p24j3Ods_0WE_NumA/pub?w=302&h=220)\n",
        "\n",
        "\n",
        "実際にpythonで実装を行う。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwcofQZYpQ7i",
        "colab_type": "code",
        "outputId": "cef7868c-6ad5-4336-e8ac-81546ddb3f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np    # 行列演算の為のライブラリの読み込み\n",
        "\n",
        "#---------------------------\n",
        "#  論理積を計算する\n",
        "#---------------------------\n",
        "def AND(x1,x2):\n",
        "  x = np.array([x1,x2])\n",
        "  w = np.array([0.5,0.5])\n",
        "  b = -0.7\n",
        "  result = b + np.sum(w*x)\n",
        "  if result <= 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "  \n",
        "print(AND(0,0))\n",
        "print(AND(0,1))\n",
        "print(AND(1,0))\n",
        "print(AND(1,1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ycu4loorf6r",
        "colab_type": "text"
      },
      "source": [
        "## ORパーセプトロン\n",
        "先程のANDパーセプトロンの重みWとバイアスｂを変更することでor（論理和）パーセプトロンも実装できる。\n",
        "\n",
        "ORパーセプトロンの真偽値は下記の通り\n",
        "\n",
        "![ORパーセプトロン](https://docs.google.com/drawings/d/e/2PACX-1vT5n56HwMBreq9nmHCYS3rydXTPR3YQRXs62---PUy9PV6hyP6uSNDVN6A63I-KLN3whl6YPsMwCWYP/pub?w=110&h=104)\n",
        "\n",
        "これをパーセプトロンで実現する場合は例として下記のように重みWとバイアスｂを設定する。\n",
        "* W1 = 0.5\n",
        "* W2 = 0.5\n",
        "* b = -0.2\n",
        "\n",
        "これを同様にpythonで実装する\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzrYtfH9sySp",
        "colab_type": "code",
        "outputId": "75fc5fff-6cb1-495d-d76c-24cb8af76e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#---------------------------\n",
        "#  論理和を計算する\n",
        "#---------------------------\n",
        "def OR(x1,x2):\n",
        "  x = np.array([x1,x2])\n",
        "  w = np.array([0.5,0.5])\n",
        "  b = -0.2\n",
        "  result = b + np.sum(w*x)\n",
        "  if result <= 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "  \n",
        "print(OR(0,0))\n",
        "print(OR(0,1))\n",
        "print(OR(1,0))\n",
        "print(OR(1,1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbrDEEzDtYfg",
        "colab_type": "text"
      },
      "source": [
        "## アルゴリズムとしてのパーセプトロン\n",
        "上記と同様にNANDなども表現できる。さらにこれらのパーセプトロンを複数組み合わせることもできる。これにより単層では表現できないXORなどの論理演算も可能となる。\n",
        "\n",
        "最も重要な点は、このパーセプトロンを構成する一つ一つのニューロンは全て等価であり、単に入力値への重み付けが異なるだけという点である。\n",
        "\n",
        "よってパーセプトロンではこの重みの値を決定することで演算が可能となり、これは演算アルゴリズムをパーセプトロンの重みで表現しているとも言い換えることができる。\n",
        "\n",
        "これまでのコンピューティングはオブジェクト指向であれなんであれ、実体としては命令を逐次実行することで目的を達成している。AIでは情報を伝達するためのプログラムは存在するが、実際の判定ロジックそのものはこの「重み」が担っている。\n",
        "\n",
        "ただし単層のパーセプトロンではXOR回路が生成出来ないことが指摘され、多層パーセプトロンが開発されたものの、そのままでは線形分離不可能なパターンを識別できないことが指摘さた。さらに、重みパラメータの設定方法が不明であったことも大きな問題点であった。このためその後10年にわたってこのパーセプトロン理論は日の目を見ることができなかった。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL-874h9wjIz",
        "colab_type": "text"
      },
      "source": [
        "# ニューラルネットワーク\n",
        "パーセプトロンのコンセプトを元に、これに改良を加えたものがニューラルネットワークである。\n",
        "* 活性化関数の導入\n",
        "* ３層構造の導入（出力層の工夫）\n",
        "* 機械学習(ディープラーニング）の導入\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmx0g2eG6NXt",
        "colab_type": "text"
      },
      "source": [
        "## 活性化関数の導入\n",
        "パーセプトロンはもともと二値演算を生物学的に実現しようという発想であったため、出力値は当然２値である。これをデジタル（離散値）ではなくアナログ（連続値）にもちこんだのが、このニューラルネットワークの優れた着想点である。\n",
        "\n",
        "ニューラルネットワークでは、これを活性化関数として再定義している。\n",
        "\n",
        "\n",
        "主な活性化関数を下記に示す\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQzVnI_66Oz1",
        "colab_type": "text"
      },
      "source": [
        "### ステップ関数\n",
        "パーセプトロンで実装されている方式。\n",
        "\n",
        "評価式の結果がゼロ以下かそれ以外で０、１の二値を返す\n",
        "\n",
        "ステップ関数をグラフに表示する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzO1oCsJ1ykU",
        "colab_type": "code",
        "outputId": "784b906b-95d6-4569-fb51-86479d40caa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "#------------------------\n",
        "# ステップ関数\n",
        "#------------------------\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "#------------------------\n",
        "# グラフへの表示\n",
        "#------------------------\n",
        "X = np.arange(-5.0, 5.0, 0.1)\n",
        "Y = step_function(X)\n",
        "plt.plot(X, Y)\n",
        "plt.ylim(-0.1, 1.1)  # 図で描画するy軸の範囲を指定\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3AGghQh3AGghwh0AWohwB4AW+n9SUyHBkOuTZAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5PqkPWW1nz7",
        "colab_type": "text"
      },
      "source": [
        "### シグモイド関数\n",
        "初期のニューラルネットワークで広く用いられた関数。\n",
        "\n",
        "（城田説）ステップ関数に近い連続的で微分しやすい関数を使ったと思ってる  \n",
        "\n",
        "\n",
        "$h(x)=\\frac{1}{1+e^{-x}}$ \n",
        "\n",
        "\n",
        "\n",
        "グラフに表示する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0yf1TAL2g0P",
        "colab_type": "code",
        "outputId": "fc5b2791-cda5-4d61-cd06-04fe3cd37bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "#------------------------\n",
        "# シグモイド関数\n",
        "#------------------------\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "#------------------------\n",
        "# グラフへの表示\n",
        "#------------------------\n",
        "X = np.arange(-5.0, 5.0, 0.1)\n",
        "Y = sigmoid(X)\n",
        "plt.plot(X, Y)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndxGREKRyFxEJQSp3EZEQ9P8A9nnclg8mQC4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biLbM0kG1qqO",
        "colab_type": "text"
      },
      "source": [
        "上記のようにシグモイド関数では、結果は２値ではなく０から１までの実数が返却される。\n",
        "\n",
        "これは出力値を２つに限定したパーセプトロンからの大きな飛躍であり、これにより複雑な（曖昧な）入力と出力が可能となった。\n",
        "\n",
        "\n",
        "#### （参考）ネイピア数 $e$ について\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "![代替テキスト](https://mathwords.net/wp-content/uploads/2016/10/enoxnogurahu1-300x260.png)\n",
        "\n",
        "**特徴**\n",
        "* $y=e^x$ のグラフの概形は上図の通り。\n",
        "* $x=0$の時、$e^x=1$\n",
        "* $\\frac{d}{dx}e^x = e^x$ 微分しても同じ形\n",
        "* $y=e^x$はプログラム上では  y=exp(x)と記述される。\n",
        "* expはエクスポーネンシャルの略\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLC6sU0y4ARZ",
        "colab_type": "text"
      },
      "source": [
        "### ReLU関数　（別名：ランプ関数　　ramp func 傾斜関数）\n",
        "最近のAIではシグモイド関数の代わりに用いられる事が多い。\n",
        "\n",
        "ReLUは入力値がゼロを超えていればそのまま出力される。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEAtGJId5cji",
        "colab_type": "code",
        "outputId": "2c3f6598-8082-4ffc-efd6-9365d6e8baa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "#------------------------\n",
        "# ReLU関数\n",
        "#------------------------\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "#------------------------\n",
        "# グラフへの表示\n",
        "#------------------------\n",
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "y = relu(x)\n",
        "plt.plot(x, y)\n",
        "plt.ylim(-1.0, 5.5)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8VyVbpH6pvs",
        "colab_type": "text"
      },
      "source": [
        "## 3層構造の導入\n",
        "ニューラルネットワークでは、全体のネットワーク構造（ニューロンの繋がり）を入力層、中間層、出力層の３種類で定義している。\n",
        "\n",
        "下記に３層構造のニューラルネットワークを図示する（入力層が０番目）。\n",
        "\n",
        "![ニューラルネットワーク図](https://docs.google.com/drawings/d/e/2PACX-1vTz9IG4wquQ4WPAyi6vaxoMVAnO_g1E7lA3We7u1x84faTg_pUQ3KWwEuyP-NDn9YQnPu694SUXM12t/pub?w=960&h=720)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFWbHevF_Gl-",
        "colab_type": "text"
      },
      "source": [
        "### 入力層\n",
        "ニューラルネットワークの入力となる層。\n",
        "\n",
        "各ニューロンへの値は一つなので、画像を認識しようとすれば、各画素毎の値が入力値となる。\n",
        "\n",
        "具体的には縦横１６ドットの画像であれば、１６×１６＝２５６の画素で構成されるため、入力層のニューロンは２５６個必要となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GTLGjAsABCf",
        "colab_type": "text"
      },
      "source": [
        "### 隠れ層\n",
        "この層がAIにおけるモデルの中心部分であり日々進化を遂げている。\n",
        "\n",
        "過去には計算機のパワー不足により隠れ層が１つでも多大な負荷であったが、現在ではこの隠れ層を数百に及んで実装することも可能となっている。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgO6jmr__wS0",
        "colab_type": "text"
      },
      "source": [
        "### 出力層\n",
        "ニューラルネットワークの出力となる層。この層の設計は説かれる問題によってニューロンの個数と活性化関数が決まってくる（このため、先程の図では出力層での活性化関数がh()ではなくσ()と記述している）。\n",
        "　　\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>問題の種類</th>\n",
        "    <th>問題の例</th>\n",
        "    <th>活性化関数</th>\n",
        "    <th>出力層の数</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>回帰問題</td>\n",
        "    <td>年齢推定など特定の値を求める</td>\n",
        "    <td>恒等関数</td>\n",
        "    <td>1個</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>２値分類</td>\n",
        "    <td>合格・不合格など２種類に分類する</td>\n",
        "    <td>シグモイド関数</td>\n",
        "    <td>2個</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>多クラス問題</td>\n",
        "    <td>文字認識など複数のクラスに分解する問題</td>\n",
        "    <td>ソフトマックス関数</td>\n",
        "    <td>n個</td>\n",
        "  </tr>\n",
        "</table>\n",
        "</br>\n",
        "\n",
        "\n",
        "今回の手書き数字認識であれば、結果は０から９のいずれかになるので、出力層は１０となり出力層での活性化関数はソフトマックス関数を用いる。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2hRLitDC5hY",
        "colab_type": "text"
      },
      "source": [
        "#### ソフトマックス関数\n",
        "![ソフトマックス関数](https://docs.google.com/drawings/d/e/2PACX-1vSzbhHIZVMWV4YAJgndvhJuHOmPNSNBJYlKqpLh1d0Ffl6Lfbr9HcQcczLk-oWQgYqcno_Mn6TuFAaM/pub?w=185&h=100)\n",
        "\n",
        "この関数は出力値を全体の合計値で割ることにより、必ず０～１の間における割合を示す。これは擬似的に出力値に対する期待度、信頼度を示しているとも言える。\n",
        "\n",
        "なお、expをしていることにより、出力値が高い場合はより大きい値をとることで、微細な違いもはっきりと表示する効果を持っている。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npN6fdBEGp20",
        "colab_type": "text"
      },
      "source": [
        "## 手書き数字（エムニスト）認識の実装\n",
        "それでは、実際にニューラルネットワークを使って手書き文字認識を実装してみる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsgiudRhHFSm",
        "colab_type": "text"
      },
      "source": [
        "### エムニストデータセット\n",
        "oreillyのデータを使ってMNISTデータの認識を行う\n",
        "\n",
        "![MNIST](https://docs.google.com/drawings/d/e/2PACX-1vRmKtGjrW_McthMyjLw9dj_zXxKaZ-Gca_HeNPKumeS7EIZ72ndBVYITGC0VLaRQOcayx97xOt_f40n/pub?w=289&h=173)\n",
        "\n",
        "Mixed Natioal Institute of Standards and Technology database. MNIST [エムニスト] [em-nist]\n",
        "\n",
        "\n",
        "![サンプル](https://docs.google.com/drawings/d/e/2PACX-1vQkslZu9xyboXsva9jgmQDVrynTZZeYnI6yMamdc012Y3z5kTrA68ePcAyBsagxSfyawIsTHhFE-b7j/pub?w=290&h=318)\n",
        "\n",
        "* サイズは２８×２８ドット\n",
        "* 白黒２５６階調（０白：２５５：黒）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I7etWIjtj4R",
        "colab_type": "text"
      },
      "source": [
        "### ニューラルネットワークの設計\n",
        "ここでは隠れ層を２つもつニューラルネットワークを構築する\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vQPJZS9d6upJkCh2yHAc7h5620ZZCjPvGNzHuOTyrXx1kYv8HCC5ORtbB-6FQC-E4oir0peQFe-Q5k9/pub?w=1291&h=743)\n",
        "\n",
        "* 入力層は２８×２８＝７８４個のニューロン\n",
        "* 隠れ層１は５０個のニューロン\n",
        "* 隠れ層２は１００個のニューロン\n",
        "* 出力層は（正解が0～9なので）１０個のニューロン\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJCqoMuMQgOG",
        "colab_type": "text"
      },
      "source": [
        "## 文字認識処理のみの実装\n",
        "上図において機械学習の処理を含まない文字認識処理の部分について実装を行う（損失関数：交差エントロピーについては後で述べる）。  \n",
        "  ここでは、事前に機械学習により導出された重みとバイアスパラメータを定義済みモデルとしてダウンロードして用いる。  \n",
        "　"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-JgC20YPdz5",
        "colab_type": "code",
        "outputId": "e5cda27a-1863-4884-84aa-648c19a6875e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "%cd /content/deep-learning-from-scratch/ch03\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import pickle\n",
        "from dataset.mnist import load_mnist\n",
        "from common.functions import sigmoid, softmax\n",
        "\n",
        "#-----------------------------\n",
        "# データの取得\n",
        "#-----------------------------\n",
        "def get_data():\n",
        "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
        "    return x_test, t_test\n",
        "\n",
        "\n",
        "#-----------------------------\n",
        "# 定義済みのニューラルネットワークの読み込み\n",
        "#-----------------------------\n",
        "def init_network():\n",
        "    with open(\"sample_weight.pkl\", 'rb') as f:\n",
        "        network = pickle.load(f)\n",
        "    return network\n",
        "\n",
        "\n",
        "#-----------------------------\n",
        "# 推論処理\n",
        "#-----------------------------\n",
        "def predict(network, x):\n",
        "    # 重み配列の読み込み\n",
        "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
        "    # バイアスの読み込み\n",
        "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
        "\n",
        "    # 入力層　-> 隠れ層1　の処理\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "\n",
        "    # 隠れ層1　-> 隠れ層2　の処理\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    z2 = sigmoid(a2)\n",
        "\n",
        "    # 隠れ層2　-> 出力層　の処理\n",
        "    a3 = np.dot(z2, W3) + b3\n",
        "    y = softmax(a3)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "#-----------------------------\n",
        "# メイン処理\n",
        "#-----------------------------\n",
        "x, t = get_data()             # 入力データx と正解データt を読み込む\n",
        "network = init_network()      # 訓練済みの重み、バイアスデータを読み込む\n",
        "accuracy_cnt = 0\n",
        "\n",
        "for i in range(len(x)):\n",
        "    # 一枚ごとに推論処理を行う\n",
        "    y = predict(network, x[i])\n",
        "    p= np.argmax(y)           # 最も確率の高い要素のインデックスを取得\n",
        "    if p == t[i]:\n",
        "        # 正解の場合\n",
        "        accuracy_cnt += 1\n",
        "\n",
        "#-----------------------------\n",
        "# 結果表示\n",
        "#-----------------------------\n",
        "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch03\n",
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "Accuracy:0.9352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em0jNpA-XQxM",
        "colab_type": "text"
      },
      "source": [
        "### バッチ処理の実装\n",
        "上記の例は６万枚の画像データについて一枚づつ処理を行っている。\n",
        "\n",
        "numpyなどの科学計算ライブラリは行列計算が高速に行えるようにチューニングされている。このため一般的にはロジックでループ処理を行うより、行列式として一度に計算する量を増やしてやるほうが結果として処理が早くなる。\n",
        "\n",
        "今回の例では一つの画像データ（画素数分の列データ）を複数画像分まとめて（行列データにして）渡すことで、計算が早くなる。\n",
        "\n",
        "このまとめる指定を「バッチ（束）」と呼ぶ。\n",
        "\n",
        "バッチ数を指定した場合のプログラムを下記に示す（結果は変わらない）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXwJLKQPXNKQ",
        "colab_type": "code",
        "outputId": "d5f00fee-8d74-4bc6-c717-ec0d28afc429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "x, t = get_data()\n",
        "network = init_network()\n",
        "\n",
        "batch_size = 100 # バッチの数\n",
        "accuracy_cnt = 0\n",
        "\n",
        "for i in range(0, len(x), batch_size):\n",
        "    x_batch = x[i:i+batch_size]\n",
        "    y_batch = predict(network, x_batch)\n",
        "    p = np.argmax(y_batch, axis=1)\n",
        "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
        "\n",
        "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.9352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-zpOiF0bBxT",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning\n",
        "ディープラーニングとは４層以上のニューラルネットワークにおける各パラメータ（重み、バイアス）を決定する作業である。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-sUaSzaX5Ej",
        "colab_type": "text"
      },
      "source": [
        "## Learning概要\n",
        "具体的な処理手順をPDCAに例えると、Pとして対象データ郡を抽出し、Dとしてニューラルネットの計算を行う。次にCとして結果の評価を行い、Aとしてパラメータの再設定を行う。これを必要回数分行う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPoM_jYM9060",
        "colab_type": "text"
      },
      "source": [
        "## P:対象データの抽出-ミニバッチ\n",
        "ミニバッチとは全量データを使わずに一部のデータをランダムに抜き出すことで処理の高速化を図るテクニックである。これはTV局の視聴率調査と同様の考え方に基づいている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP_Rg3f2DCic",
        "colab_type": "text"
      },
      "source": [
        "## D:ニューラルネットの計算\n",
        "ミニバッチで抽出されたデータを使ってニューラルネットを計算する。これは先に述べたように、中間層の活性化関数をシグモイド関数とし、出力層ではそれをソフトマックスにより確率として計算される。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEl8IIEkdVl9",
        "colab_type": "text"
      },
      "source": [
        "## C:チェック：損失関数の設定\n",
        "ソフトマックスにより計算された値がどの程度正解からズレているか（誤差）を計算する。この関数を損失関数と呼ぶ。\n",
        "\n",
        "損失関数は誤差を数値化することである。数学的に最も有名な誤差計測の方法は２乗和誤差と呼ばれるものであるが、AIの世界では交差エントロピー誤差という手法が利用される。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1y-z9kQAPRa",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### 交差エントロピー誤差\n",
        "![交差エントロピー誤差](https://docs.google.com/drawings/d/e/2PACX-1vRkLbMfJmj-vUDumOXr1Lxk32H9Hu7CNryVJWggm48BZmN8pMkEfX94XdkzyaAiggbWiqrBRTdteCBv/pub?w=150&h=100)\n",
        "\n",
        "　先のmnistデータでは、各画像に対する正解データ（教師データ）は０から９までの配列で表されていた。  \n",
        "　一方、ニューラルネットワークを使って計算された出力結果はソフトマックス関数により０から１までの値として表現される。  \n",
        "　この例で交差エントロピー誤差を計算すると、教師データは正解以外はゼロであるため、結局の所　E=log(0.8)を計算するだけでよい。  \n",
        "\n",
        " \n",
        "<table>\n",
        "  <tr>\n",
        "    <th>ラベル</th>\n",
        "    <th>0</th>\n",
        "    <th>1</th>\n",
        "    <th>2</th>\n",
        "    <th>3</th>\n",
        "    <th>4</th>\n",
        "    <th>5</th>\n",
        "    <th>6</th>\n",
        "    <th>7</th>\n",
        "    <th>8</th>\n",
        "    <th>9</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>正解データ</th>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>SoftMax出力値</th>\n",
        "    <td>0</td>\n",
        "    <td>0.1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0.8</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0.1</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>交差エントロピー誤差</th>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0.22</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "ちなみに、y = log(x)のグラフは"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfa-921_knha",
        "colab_type": "code",
        "outputId": "a7ccc737-254c-46f6-e7f1-110831c927f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "#------------------------\n",
        "# グラフ表示\n",
        "#------------------------\n",
        "x = np.arange(0.0001, 1.0, 0.01)\n",
        "y = np.log(x)\n",
        "plt.plot(x, y)\n",
        "plt.ylim(-5.0, 0.0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQW5iIjLKchFRFxOQS4i4nL/H9GNwUjJxw+EAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcNS1DahlVey",
        "colab_type": "text"
      },
      "source": [
        "となり、Xが１のときにYは０となる。なお、Xが０となるとlog(x)は-∞となるため、プログラミングする際はXがゼロとならないように極小のバイアス値を付加して算出する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUPCEsdrEXHV",
        "colab_type": "text"
      },
      "source": [
        "## A:パラメーターの再計算\n",
        "先に導出された損失関数を指標に各パラメーター（重みとバイアス）を決定してゆく。\n",
        "\n",
        "この値の決定方法として勾配降下法と呼ばれる手法が用いられる。\n",
        "\n",
        "### **勾配降下法**\n",
        "\n",
        "　まず一つのニューロンの重みWについて考えてみる。ニューロンの重みWが変化すれば損失関数の値Lossが変動する。仮にその変動が下図のグラフのようになった場合、最もLossが少ないWの値が求めたい目標の値である。  \n",
        "　今初期値Pからスタートしてゴールとなる目標地点にWを変化させたい場合、どのような操作をすればよいだろうか？　視覚的に記述するなら「傾きを下る方向にWを移動する」となる。これを数学的に記述すると、\n",
        "* 点Pでのグラフの傾きを求め、その値の定数倍をマイナスする\n",
        "\n",
        "これを繰り返して目標の値を求める方式を勾配降下法と呼ぶ。\n",
        "\n",
        "\n",
        "![勾配降下法](https://docs.google.com/drawings/d/e/2PACX-1vRN5EFodaLfgY0qFOPS9jQLNnJFa-e5JnrwZBAEAVqAtdxPtdnFIhW3xN0176LokS0BbOm3IsdLmCVn/pub?w=791&h=386)\n",
        "\n",
        "　なお、次に示すプログラムでは微分操作により傾きを求めるのではなく、点Pの前後のウェイト値を使って再度損失関数の値を求め、その差を使って傾きを求める方法「**数値微分**」で計算する。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yBy0Hh5v66J",
        "colab_type": "text"
      },
      "source": [
        "### 勾配降下法の例題\n",
        "実際に数値微分を使って勾配降下法の動きを見てみる。\n",
        "\n",
        "例として$f(x_1,x_2) = x_1^2 + x_2^2$ としたとき、関数$f$が最小の値となる$x_1,x_2$を求める。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLZnPuQPvcf5",
        "colab_type": "code",
        "outputId": "14d02060-4a8a-471d-8688-c10503e422aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def _numerical_gradient_no_batch(f, x):\n",
        "    h = 1e-4  # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)  # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x)  # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val  # 値を元に戻す\n",
        "        \n",
        "    return grad\n",
        "\n",
        "def numerical_gradient(f, X):\n",
        "    if X.ndim == 1:   #次元数が１の場合\n",
        "        return _numerical_gradient_no_batch(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x = init_x\n",
        "    x_history = []\n",
        "\n",
        "    for i in range(step_num):\n",
        "        x_history.append( x.copy() )\n",
        "\n",
        "        grad = numerical_gradient(f, x)   # 数値微分で傾きを求める\n",
        "        x -= lr * grad                    # 傾き×学習率を重みから引く\n",
        "\n",
        "    return x, np.array(x_history)\n",
        "\n",
        "\n",
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])    \n",
        "\n",
        "lr = 0.3               # Learning Rate 学習率\n",
        "step_num = 20\n",
        "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
        "\n",
        "plt.plot( [-5, 5], [0,0], '--b')\n",
        "plt.plot( [0,0], [-5, 5], '--b')\n",
        "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
        "\n",
        "plt.xlim(-3.5, 3.5)\n",
        "plt.ylim(-4.5, 4.5)\n",
        "plt.xlabel(\"X0\")\n",
        "plt.ylabel(\"X1\")\n",
        "plt.show()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSooCAAAl9f8wIgwHblZwywAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpyIxXvhx6RC",
        "colab_type": "text"
      },
      "source": [
        "### 学習率(Learning Rate)η\n",
        "ここで重要なパラメータとして「学習率η（イータ）」がある。この学習率は傾きに乗じる定数であるが、これが大きすぎても小さすぎても正しい値を導くことができない。\n",
        "\n",
        "* lr=0.1であれば収束する\n",
        "* lr=0.01では収束しない\n",
        "* 逆にlr=0.8であると発散してしまう\n",
        "\n",
        "なお、この学習率は経験的に発見するしかない。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJHpaRxXy6SL",
        "colab_type": "text"
      },
      "source": [
        "## Learningの実装\n",
        "これまでの説明でPDCAを見てきた。ここで実際にプログラムとしてニューラルネットワークの学習を実装する。\n",
        "\n",
        "### 回数に関する用語\n",
        "\n",
        "PDCAの繰り返し回数を　**イテレーション数**と呼ぶ。\n",
        "\n",
        "\n",
        "対象としている訓練データの総数を１エポックという単位で表す。これはPDCAがミニバッチによりランダムに選ばれたデータで訓練をされるため、実際には全部のデータを処理したことにはならないが、便宜的に１エポックで一回りの学習をしたとみなすためである。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWfKRhNW-pab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/deep-learning-from-scratch/ch04\n",
        "\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000  # 繰り返しの回数を適宜設定する\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 勾配の計算\n",
        "    grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    #grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # パラメータの更新\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "# グラフの描画\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01_h0GkeKNBm",
        "colab_type": "text"
      },
      "source": [
        "# 誤差逆伝播法（バックプロパゲーション）\n",
        "　先程の機械学習では数値微分という方法で重みを計算した。この方法では一つの重みパラメータを求めるために２回の全量計算をおこなっており、これをすべてのニューロンについて計算し、やっと一回目のイテレーションが完了する。このように数値微分ではCPU処理量が多いため多層ネットワークの構築には不向きである。このため、計算が高速となる誤差逆伝播法（バックプロパゲーション）が開発された。  \n",
        "　多層に渡った活性化関数を微分するためには、連鎖率を利用して損失関数の値から逆向きに偏微分してゆくことで高速に計算できる。ここではそれを計算グラフにて説明する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI8kuqguNcIH",
        "colab_type": "text"
      },
      "source": [
        "## 計算グラフ\n",
        "　計算グラフという記法を使い、順方向の計算を確認する\n",
        "\n",
        "![計算グラフ](https://docs.google.com/drawings/d/e/2PACX-1vTwwOt7w-NMp-xaUtPVC7ItTlS5IWqvm0IGxWJAQuWD1WSDNrJdowzxfQ9ED53x05uX26WEmkQjt6BL/pub?w=813&h=521)\n",
        "\n",
        "\n",
        "　この計算グラフを使ってリンゴの値段に関する合計金額の微分を求める。つまりリンゴ１個の値段が与える合計金額への影響度は下図のように示される。\n",
        "\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vQRRJ8owch1DvuUawwqgJ_hLX61YB5Lyt4exjIwP6bNnlPawfjN12X2ngO3l0N_9RJ7OKtTBlJ4YBuF/pub?w=809&h=521)\n",
        "\n",
        "　ここで、リンゴの値段の変化が合計金額に及ぼす影響を考えてみる。リンゴが１円値上がりすると、合計金額は２．２円分増加する。これはリンゴの個数が２であり、消費税が１．１であるからであるが、これを計算グラフで解く時、合計金額から出発してルートを逆にたどってゆくことで解を求めることができる。  \n",
        "　まず、合計金額を１と置くと、消費税がこれにかかってくるので、１．１倍した１．１が一つ前のノードに渡される。次の足し算のノードでは値に変化がおこらず、もう一つ後ろのノードに１．１が渡され、そこに個数の２が掛けられrて答えとなる２．２が導出される。この１．１はリンゴの値段に関する支払い金額の微分値にほかならない。  \n",
        "　同様にリンゴの個数に対する支払金額の微分値を求める場合は、直前の１．１までは同様で、その値にリンゴの値段となる１００を掛けた１１０が求める値となる。  　\n",
        "　このように、乗算ノードでは互いの数値を互い違いに掛け算し、加算ノードでは前段の値をそのまま使いながら逆の方向に計算を行うと、最終的に微分値が決定される。これが逆伝播法である。ポイントは以下の通り。\n",
        "\n",
        "* 偏微分値は出力側から逆順に計算してゆくことができる\n",
        "* 計算は各ノードの値のみを使えば良い\n",
        "\n",
        "　この特性をこれまで見てきたニューラルネットワークに応用する。各ニューロンの要素をそれぞれ順方向のプログラム（forward)と、逆方向のプログラム(backword)の２つで構成されるレイヤとして実装を行う。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhEqjSNybWR-",
        "colab_type": "text"
      },
      "source": [
        "## ReLUレイヤの実装\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vSvgNlyRTv8kiaVrxEk4MNumcGohtSS7DDfTkPpTAAUT-I0Ecjhi62dDU8RWQRIlz7eL4XBW2HoAP2g/pub?w=958&h=247)\n",
        "\n",
        "活性化関数ReLUの場合は入力値が０より大きい場合は、その値がそのまま渡される。このため、グラフの傾きすなわち微分値は１であり、逆伝播においては渡された値に１を掛けた値としてそのまま逆伝播させる。\n",
        "\n",
        "\n",
        "pythonでの実装としては、下記のようになる。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbOGTwl7neOP",
        "colab_type": "code",
        "outputId": "d30ea5a6-1bd1-4be8-daed-b8480947ce67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "from common.functions import *\n",
        "from common.util import im2col, col2im\n",
        "\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0  \n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "# ReLUレイヤーのコーディングサンプル\n",
        "x = np.array([[1.0,-1.2],[-2.2,1.3]])\n",
        "print(\"----------------\")\n",
        "print(\"もともとの入力値\")\n",
        "print(x)\n",
        "print(\"----------------\")\n",
        "print(\"mask=(x<=0)   マスク値  \")\n",
        "mask=(x<=0)\n",
        "print(mask)\n",
        "print(\"----------------\")\n",
        "print(\"x[mask]=0      Trueの部分をゼロで埋める\")\n",
        "x[mask]=0\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------\n",
            "もともとの入力値\n",
            "[[ 1.  -1.2]\n",
            " [-2.2  1.3]]\n",
            "----------------\n",
            "mask=(x<=0)   マスク値  \n",
            "[[False  True]\n",
            " [ True False]]\n",
            "----------------\n",
            "x[mask]=0      Trueの部分をゼロで埋める\n",
            "[[1.  0. ]\n",
            " [0.  1.3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjv9Jjb7sjJe",
        "colab_type": "text"
      },
      "source": [
        "## シグモイドレイヤの実装\n",
        "\n",
        "今回のMNSITデータ分析では利用しないが、シグモイド関数の逆伝播について記述する\n",
        "\n",
        "\n",
        "$h(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "![シグモイド関数](https://docs.google.com/drawings/d/e/2PACX-1vSPiq7ffbs0PiyFDXrg93J1t3KdMq0eAnCoVC_JfD62LrszzR9t_4c3U76pF_SvNhDCxeKplvJrBIL_/pub?w=182&h=100)\n",
        "\n",
        "シグモイド関数の微分を計算グラフで解くと最終的に下記のようになる。\n",
        "\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vQbSo7K6xNPYZ6uZsEVr25lCTd-IyL_CvHxUBz_qnTGHR6_9ccFON4N9tWrGwZwPsJJJwKhqVtOu9yE/pub?w=476&h=224)\n",
        "\n",
        "\n",
        "実装としては下記の通り\n",
        "```\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXyEuBJOvZg",
        "colab_type": "text"
      },
      "source": [
        "## Affineレイヤの実装\n",
        "\n",
        "各ニューロンは複数からの信号と重み付けを受け取りそれらの総和を活性化関数に引き渡している。\n",
        "\n",
        "![パーセプトロン](https://docs.google.com/drawings/d/e/2PACX-1vSpMVD85wOprci70XqNYFpaA3gNI_rTRVezumjEJ6trAHQ6qMq7gPA-PH7bDzHfOkMb-pEKQXxSOvs_/pub?w=302&h=220)\n",
        "\n",
        "先程はこの活性化関数を一つのレイヤとして実装した。Affineレイヤはその前段として、各ノードからの値と重みを掛け合わせ、バイアスを付加する部分を受け持つ。\n",
        "\n",
        "\n",
        "* affineとは幾何学における写像を意味する。\n",
        "\n",
        "```\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W =W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        return dx\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6CydMIYl5i",
        "colab_type": "text"
      },
      "source": [
        "## Softmax with Lossレイヤの実装\n",
        "ニューラルネットワークの最終段階としてソフトマックス関数による確信度の計算とそれを評価する損失関数のレイヤを同時に定義する。なお、ここでは損失関数を交差エントロピーとしている。\n",
        "\n",
        "\n",
        "```\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None # softmaxの出力\n",
        "        self.t = None # 教師データ\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8t2eRU2cpbB",
        "colab_type": "text"
      },
      "source": [
        "## 全体実装\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vRz5JqHaZeJ2QZy31JlNOiROYOsYd5q1nZ29G79UH3MEM0cAN9zn_v717IeMl0_rUXsw82679O8aEmk/pub?w=599&h=413)\n",
        "\n",
        "隠れ層を５０とした２層のニューラルネットワークを実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1DwAqg0gg8j",
        "colab_type": "code",
        "outputId": "95284cbd-a4bf-40c7-e374-65d262862fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/deep-learning-from-scratch/ch04\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 重みの初期化\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    # x:入力データ, t:教師データ\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x:入力データ, t:教師データ\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeqOtvwzgjW_",
        "colab_type": "code",
        "outputId": "66aee875-537c-4986-f834-39c84db1aa2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 勾配\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 更新\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.11236666666666667 0.1135\n",
            "0.7775333333333333 0.7832\n",
            "0.87695 0.8798\n",
            "0.8990166666666667 0.9027\n",
            "0.90955 0.9127\n",
            "0.9147833333333333 0.9183\n",
            "0.9198833333333334 0.9206\n",
            "0.9241333333333334 0.9269\n",
            "0.928 0.9299\n",
            "0.9312833333333334 0.9324\n",
            "0.9340166666666667 0.9352\n",
            "0.9367166666666666 0.9373\n",
            "0.9387166666666666 0.938\n",
            "0.9412166666666667 0.9395\n",
            "0.9424666666666667 0.9421\n",
            "0.9435666666666667 0.9423\n",
            "0.94585 0.9455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHSKcdTAh34r",
        "colab_type": "text"
      },
      "source": [
        "# 実装上の課題\n",
        "これまでのところで大まかなAIの仕組みについて学習を行った。ここではAIを実際に実装する場合に問題となる諸課題について概覧する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Nhb3Cbip3l",
        "colab_type": "text"
      },
      "source": [
        "## 勾配降下法の問題点\n",
        "\n",
        "勾配降下法によって最も損失関数が小さくなる値を求める場合、問題としてはその出発点によっては目標の最小値にたどり着けない点である。\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vQhdzzAYjQ8Uxgs4-7bZ1soV_wPt1M47ll436PPh2y6d-rOE9cU4voWgVO9QcMIFCq5V6-OV9D7DWbf/pub?w=791&h=386)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNxOjZlCkGnQ",
        "colab_type": "text"
      },
      "source": [
        "### SGD(確率的勾配降下法）\n",
        "先に実装した例は確率的勾配降下法と呼ばれる手法である。これは全体の中の一部をランダムに選び出すことで、計算の起点となる点Pの位置を変えることで、確率的に目標の近辺からスタートさせる事を目標としている。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHKb_BQxmkXk",
        "colab_type": "text"
      },
      "source": [
        "#### 標準的な探索方法\n",
        "\n",
        "基本的には微分値がマイナスとなる方向に予め定められた量を移動させることで、最適解を探索する。\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vRwN3IoFQ1PPyVor6NoQbXQohRqEc9S1SRLO7zHR2KbbgxkR7EpC-GUqUzKazvEsrUhJ3dH6M3Gb3Ze/pub?w=791&h=386)\n",
        "\n",
        "この予め定めらた量は学習率（Learning Rate）ηで示される。この学習率の設定は経験的に定められる。上図の場合では学習率が大きいため、一旦かなり最適解に近づくものの、その次では最適解を通り越してしまい、いわゆる振り子状態が長く続く点が問題となる。\n",
        "\n",
        "#### Momentumによる探索\n",
        "これは常にひつ前の移動量を保存しておき、その前回移動量の一定割合をマイナスに作用させる探索方法であり、これにより見かけ上は慣性的な力が加わることで、自由振り子状態を緩和させることができる。\n",
        "\n",
        "#### AdaGrad\n",
        "これは各学習段階の履歴を保存し、過去の学習量に比例して移動量を減らしてゆく手法である。\n",
        "\n",
        "#### Adam\n",
        "Momentum + AdaGradによる探索法\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNwK5Dh6wpQZ",
        "colab_type": "text"
      },
      "source": [
        "## 重みの初期値\n",
        "例題では重みの初期値はガウス分布を用いたが、代表的な初期値は２つ。\n",
        "\n",
        "### Xaivierの初期値　ザビエル\n",
        "　Xaivierの初期値は様々なニューラルネットで用いられている。sigmoid関数やtanh関数を活性化関数として用いる時、このXavierの初期値を用いるとよい。\n",
        "\n",
        "### Heの初期値\n",
        "　Heの初期値はReLU関数を活性化関数とするときに用いるとよい。ReLU関数を活性化関数としたモデルにXavierの初期値を用いるとアクティベーション分布に偏りが生じやすくなることが知られている。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwLF9myly9Bq",
        "colab_type": "text"
      },
      "source": [
        "## Batch Normalization\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vSa-QlDlMx_WFHsisso0ulsManhVjcWxVxWfbZtlVesNnlaSnlK2gOyLk-dS8DPbWuHMMUexEWTwhqL/pub?w=397&h=206)\n",
        "\n",
        "これはミニバッチ単位に入力データの値を平均がゼロ、分散が１となるように統計的に調整するレイヤーを挟み込むことで下記のような効果が得られる。\n",
        "* 学習が早く進む\n",
        "* 初期パラメータへの依存度が下がる\n",
        "* 過学習を抑制できる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNsvfi0O01dO",
        "colab_type": "text"
      },
      "source": [
        "## 過学習への対応\n",
        "　過学習とは訓練データにフィットしすぎて未知データに対してはかえって認識率が低くなる状態をさす。この過学習を防ぐための仕組みとしてWeight DecayとDropoutについて述べる\n",
        "\n",
        "### Weight Decay\n",
        "　過学習の原因として重みの値が極端に大きくなる事が知られており、これに対応する方法として構築された手法。  \n",
        "　損失関数に重みWを要素とした値を加えることで、大きな重みが損失を大きくするように調整する。\n",
        "\n",
        "### Drop Out\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vRhsO9inUmkCqld72pxTJNfqejbccG7V0WeBBZq-TmtrEWpv_shAhVb_O-8KbRPD0FXDwxX99iIBhYj/pub?w=595&h=229)\n",
        "\n",
        "　途中のニューロンをミニバッチ毎にランダムに間引くことで、信号の経路を複数パターンで学習させる。その際に飛ばされたニューロンを代替するニューロンは余計に信号を学習する？　ことで全体の効率が上がる？　らしい。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMHip9gy0EvT",
        "colab_type": "text"
      },
      "source": [
        "# CNN畳み込みニューラルネットワーク\n",
        "　畳み込みニューラルネットワークとは、画像処理のような縦、横、色深度などの次元をもったデータの解析を行う場合に、一旦その次元を保ったまま画像の特徴を抽出・正規化し、これを最終層ではこれまでに学んだ隠れ層による一次元のニューラルネットワークに接続させる構造となっている。\n",
        "\n",
        "　画像の処理はそれまでOpenCVなどにより様々な成果が挙げられてきた。画像の圧縮、明暗や色相の変換、画像のエッジ強調、画像のぼかしなどである。\n",
        "\n",
        "　CNNではこれらの画像に対するフィルターを機械学習により作り出すところが大きなポイントとなる。この画像フィルターをCNNではカーネルとも呼ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzb7_Q0vc14z",
        "colab_type": "text"
      },
      "source": [
        "## 全体構成\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vSh0uknkhuKIBI4ZsJDiGhyMKjtwaKRGIHPfgMrHOagTetYhyOttnLpMnF0rf5ntM4ofp16EmUjPk3x/pub?w=678&h=204)\n",
        "\n",
        "　CNNでは新たに　Convolution　畳込みレイヤ　と　Pooling　プーリングレイヤが加わる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TILtn869B4Vf",
        "colab_type": "text"
      },
      "source": [
        "## 畳込み層\n",
        "\n",
        "### フィルター演算の基本\n",
        "\n",
        "![代替テキスト](https://kenyu-life.com/wp-content/uploads/2019/03/cnn.gif)\n",
        "\n",
        "　画像データにフィルターとなる数字群を重ね合わせ、それぞれを掛け算した合計値を「特徴マップ」として出力する。\n",
        "\n",
        "　このフィルター（カーネル）の数値が機械学習によって決定されてゆく。これは先のニューラルネットワークの重みWと同様の手順（バックプロパゲーション）で求める。\n",
        "\n",
        "### ストライド\n",
        "　フィルターの移動量をストライドと呼ぶ。上図ではストライド１である。ストライドを２とすると当然であるが、出力される特徴マップはサイズが小さくなる。\n",
        "\n",
        "\n",
        "### パディング\n",
        "　単純にフィルターを適用すると、必ず画像のサイズが小さくなってしまう。これを防ぐためにパディングとよばれる値がゼロのデータを画像データに予め付加して、フィルター演算によるサイズ低下を防ぐ事ができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZONoSP__gMQ",
        "colab_type": "text"
      },
      "source": [
        "## ３次元での畳込み層\n",
        "\n",
        "![代替テキスト](https://kenyu-life.com/wp-content/uploads/2019/03/3cnn.gif)\n",
        "\n",
        "　縦、横、色深度の三次元データでは、フィルターも３次元必要となる。なお、各フィルターの内容は異なるが、フィルターのサイズはすべて同じである。\n",
        "\n",
        "　このフィルターの出力値のそれぞれにバイアス値を掛け、合計した結果が特徴マップとして出力される。\n",
        "\n",
        "　画像データ的に言うと、カラーがグレースケールに変換されるイメージ。\n",
        "\n",
        "\n",
        "### 複数フィルター\n",
        "　フィルターは通常、複数設定される。上記の例ではRGBのフィルターが複数設定されることとなる。\n",
        "\n",
        "\n",
        "### バッチ処理\n",
        "　ここでもバッチ処理を実施するために、複数枚の画像を一度に処理する。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uHib5aF_mnY",
        "colab_type": "text"
      },
      "source": [
        "## プーリング層\n",
        "\n",
        "![代替テキスト](https://kenyu-life.com/wp-content/uploads/2019/03/pooling.gif)\n",
        "\n",
        "　プーリング層は画像を圧縮する。\n",
        "\n",
        "### maxスプーリング\n",
        "　指定された領域の最大値をマッピングする\n",
        "\n",
        "### avgプーリング\n",
        "　指定された領域の平均値をマッピングする\n",
        "\n",
        "\n",
        "　このプーリング層により画像の圧縮が行われ、細かい部分が削られる。全体としての情報量は下がるが、より汎化された画像を得ることができる。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qREZUPzQeEF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9e76ea0-97b1-4f21-d3cd-c5aedd0d373e"
      },
      "source": [
        "%cd /content/deep-learning-from-scratch/ch07\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"単純なConvNet\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 入力サイズ（MNISTの場合は784）\n",
        "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
        "    output_size : 出力サイズ（MNISTの場合は10）\n",
        "    activation : 'relu' or 'sigmoid'\n",
        "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
        "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
        "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 重みの初期化\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"損失関数を求める\n",
        "        引数のxは入力データ、tは教師ラベル\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"勾配を求める（数値微分）\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 入力データ\n",
        "        t : 教師ラベル\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        各層の勾配を持ったディクショナリ変数\n",
        "            grads['W1']、grads['W2']、...は各層の重み\n",
        "            grads['b1']、grads['b2']、...は各層のバイアス\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 入力データ\n",
        "        t : 教師ラベル\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        各層の勾配を持ったディクショナリ変数\n",
        "            grads['W1']、grads['W2']、...は各層の重み\n",
        "            grads['b1']、grads['b2']、...は各層のバイアス\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0mZMnhGdo0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1e122ed-7d55-44dd-fc46-2516b45471c6"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 処理に時間のかかる場合はデータを削減 \n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# パラメータの保存\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# グラフの描画\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "train loss:2.2990962880040677\n",
            "=== epoch:1, train acc:0.379, test acc:0.344 ===\n",
            "train loss:2.2983394560149724\n",
            "train loss:2.2954258514013612\n",
            "train loss:2.2873407319352164\n",
            "train loss:2.2752188958443647\n",
            "train loss:2.2676612858151426\n",
            "train loss:2.252990884070142\n",
            "train loss:2.226888791876089\n",
            "train loss:2.215640492884249\n",
            "train loss:2.1774163712516104\n",
            "train loss:2.161102932258876\n",
            "train loss:2.153863800218567\n",
            "train loss:2.1080912904664246\n",
            "train loss:2.001805308781037\n",
            "train loss:2.027252351720895\n",
            "train loss:1.9781660753987296\n",
            "train loss:1.8987125000094311\n",
            "train loss:1.8531704260962454\n",
            "train loss:1.7049387059341263\n",
            "train loss:1.7111996130177993\n",
            "train loss:1.5847220984351267\n",
            "train loss:1.5517279862291682\n",
            "train loss:1.4932275196383815\n",
            "train loss:1.3730396880531475\n",
            "train loss:1.248698249995873\n",
            "train loss:1.1196417538195367\n",
            "train loss:1.1569085264771217\n",
            "train loss:1.0008798787562767\n",
            "train loss:1.0254915123972979\n",
            "train loss:0.9097518735101279\n",
            "train loss:0.8253062003866003\n",
            "train loss:0.8323833225871309\n",
            "train loss:0.8137016279693112\n",
            "train loss:0.9817093668854782\n",
            "train loss:0.737613483211292\n",
            "train loss:0.7874701018046995\n",
            "train loss:0.7109925968308611\n",
            "train loss:0.74455250731328\n",
            "train loss:0.6512113754070671\n",
            "train loss:0.7867415556165624\n",
            "train loss:0.6857798089587348\n",
            "train loss:0.7146055491910779\n",
            "train loss:0.5702412076886175\n",
            "train loss:0.6374499057324838\n",
            "train loss:0.6967165740825966\n",
            "train loss:0.5230452927167721\n",
            "train loss:0.5052273656040951\n",
            "train loss:0.6022854436897136\n",
            "train loss:0.8015892464762112\n",
            "train loss:0.6801000031682045\n",
            "train loss:0.40835376473718826\n",
            "train loss:0.5153058162119991\n",
            "train loss:0.5143232674655148\n",
            "train loss:0.41308969188139\n",
            "train loss:0.6870094209700777\n",
            "train loss:0.5008962761951093\n",
            "train loss:0.6843089897047245\n",
            "train loss:0.5727468505195922\n",
            "train loss:0.376727564771977\n",
            "train loss:0.5004156672557863\n",
            "train loss:0.5022349986386501\n",
            "train loss:0.45519389024972595\n",
            "train loss:0.32631604069341885\n",
            "train loss:0.5860159778264531\n",
            "train loss:0.4376280179034279\n",
            "train loss:0.3928747070842472\n",
            "train loss:0.5417733163768962\n",
            "train loss:0.5001412620263417\n",
            "train loss:0.3676237033560668\n",
            "train loss:0.3911141093322799\n",
            "train loss:0.5664427541803746\n",
            "train loss:0.3595443412316935\n",
            "train loss:0.46194614568001535\n",
            "train loss:0.4468510956572483\n",
            "train loss:0.4255269596787031\n",
            "train loss:0.4292097379870494\n",
            "train loss:0.36581245646090316\n",
            "train loss:0.4482365576854491\n",
            "train loss:0.42564831804858655\n",
            "train loss:0.4996681877826494\n",
            "train loss:0.38008448504404796\n",
            "train loss:0.5345044926457552\n",
            "train loss:0.34720141774844815\n",
            "train loss:0.4158824748710259\n",
            "train loss:0.4156925312442164\n",
            "train loss:0.29326821943716647\n",
            "train loss:0.24812867893953558\n",
            "train loss:0.4124374708491365\n",
            "train loss:0.3318345901131527\n",
            "train loss:0.3281278822820448\n",
            "train loss:0.3828474811270761\n",
            "train loss:0.32575497074499005\n",
            "train loss:0.45826748516598814\n",
            "train loss:0.3901801188752805\n",
            "train loss:0.351365201201961\n",
            "train loss:0.3579725142181843\n",
            "train loss:0.4240739282391712\n",
            "train loss:0.4771550108840874\n",
            "train loss:0.43891503967432394\n",
            "train loss:0.35350296450034624\n",
            "train loss:0.4259248933053813\n",
            "train loss:0.5031837893641723\n",
            "train loss:0.30394208486390445\n",
            "train loss:0.5495563277657487\n",
            "train loss:0.3797745176727759\n",
            "train loss:0.3266395741201195\n",
            "train loss:0.41122693884568307\n",
            "train loss:0.426701273001329\n",
            "train loss:0.3130226274069389\n",
            "train loss:0.6571035070658712\n",
            "train loss:0.3995847560047011\n",
            "train loss:0.22398691675106644\n",
            "train loss:0.5532584005140836\n",
            "train loss:0.2888415017167674\n",
            "train loss:0.4605957039462422\n",
            "train loss:0.5634243628949266\n",
            "train loss:0.44587847870375064\n",
            "train loss:0.5038123180356976\n",
            "train loss:0.37799050893558106\n",
            "train loss:0.3290549505558618\n",
            "train loss:0.5645098520705655\n",
            "train loss:0.4125417999676294\n",
            "train loss:0.29309218319423924\n",
            "train loss:0.5000273533287265\n",
            "train loss:0.3800072310517975\n",
            "train loss:0.44599392922196307\n",
            "train loss:0.4414307581684443\n",
            "train loss:0.37111130529473263\n",
            "train loss:0.48787423628449345\n",
            "train loss:0.44245299461273296\n",
            "train loss:0.5766482525009726\n",
            "train loss:0.2831324582855783\n",
            "train loss:0.25309977200192557\n",
            "train loss:0.24993713380271448\n",
            "train loss:0.35167677269835673\n",
            "train loss:0.30170282013835076\n",
            "train loss:0.30264566305311846\n",
            "train loss:0.32342667020145816\n",
            "train loss:0.32585602110226775\n",
            "train loss:0.23240960114953382\n",
            "train loss:0.32609726654803844\n",
            "train loss:0.3450831145008247\n",
            "train loss:0.4107937495539804\n",
            "train loss:0.4254004642877856\n",
            "train loss:0.3022151288652294\n",
            "train loss:0.38000210361292786\n",
            "train loss:0.3994143625167272\n",
            "train loss:0.4379750029821595\n",
            "train loss:0.36693634184451995\n",
            "train loss:0.36270773215518654\n",
            "train loss:0.26005382249473025\n",
            "train loss:0.3202666164110686\n",
            "train loss:0.4878705278105847\n",
            "train loss:0.23016988567507798\n",
            "train loss:0.23340947719628052\n",
            "train loss:0.33042736163424224\n",
            "train loss:0.311551951497584\n",
            "train loss:0.3738345176546337\n",
            "train loss:0.21092817990015192\n",
            "train loss:0.3443804670829813\n",
            "train loss:0.2914820730530678\n",
            "train loss:0.31694659922958385\n",
            "train loss:0.39756882261948034\n",
            "train loss:0.2221267565889165\n",
            "train loss:0.20553259737563911\n",
            "train loss:0.4696791460997144\n",
            "train loss:0.3207429319213461\n",
            "train loss:0.18502621665578012\n",
            "train loss:0.22791788127950646\n",
            "train loss:0.25355623443726927\n",
            "train loss:0.2620163054204675\n",
            "train loss:0.32785307326611396\n",
            "train loss:0.3154251992763179\n",
            "train loss:0.17755253243894142\n",
            "train loss:0.43229733893554295\n",
            "train loss:0.32083232148108254\n",
            "train loss:0.3410099817732392\n",
            "train loss:0.2779901768119736\n",
            "train loss:0.2478681543063765\n",
            "train loss:0.20889095789399012\n",
            "train loss:0.3102251531120837\n",
            "train loss:0.1945042374552381\n",
            "train loss:0.2631821852642271\n",
            "train loss:0.4467344899094458\n",
            "train loss:0.3789804380794626\n",
            "train loss:0.44974596845558706\n",
            "train loss:0.22836762432334776\n",
            "train loss:0.3515462460905251\n",
            "train loss:0.3014692286096965\n",
            "train loss:0.3470412170548439\n",
            "train loss:0.31245824520788107\n",
            "train loss:0.25008102468139276\n",
            "train loss:0.28015069678743143\n",
            "train loss:0.16974747509164256\n",
            "train loss:0.20081856602417583\n",
            "train loss:0.21469539403928603\n",
            "train loss:0.3451133001468057\n",
            "train loss:0.27300664405319713\n",
            "train loss:0.2245794838273084\n",
            "train loss:0.2313732943864699\n",
            "train loss:0.2804558261862367\n",
            "train loss:0.29491568879639307\n",
            "train loss:0.22116510382128207\n",
            "train loss:0.10225628590881847\n",
            "train loss:0.22138639268998592\n",
            "train loss:0.31264273547235455\n",
            "train loss:0.3385264462646087\n",
            "train loss:0.3624109168683475\n",
            "train loss:0.2824836112401501\n",
            "train loss:0.2983709710950131\n",
            "train loss:0.30147308714732923\n",
            "train loss:0.3220500027654572\n",
            "train loss:0.27803859921504914\n",
            "train loss:0.2780898995135588\n",
            "train loss:0.18381959114299107\n",
            "train loss:0.3132962513206361\n",
            "train loss:0.19905175231654662\n",
            "train loss:0.14718259940785505\n",
            "train loss:0.08948767238371358\n",
            "train loss:0.3728967586900275\n",
            "train loss:0.22381021140670065\n",
            "train loss:0.3174907700274529\n",
            "train loss:0.15984200401109136\n",
            "train loss:0.23893487662083054\n",
            "train loss:0.3743356355607328\n",
            "train loss:0.22145333850804946\n",
            "train loss:0.27928776541\n",
            "train loss:0.3136229998826894\n",
            "train loss:0.17711851158815403\n",
            "train loss:0.21794954940423025\n",
            "train loss:0.1965466491523135\n",
            "train loss:0.10738586098352318\n",
            "train loss:0.17626288026466708\n",
            "train loss:0.3363134635408159\n",
            "train loss:0.241339091762296\n",
            "train loss:0.30219942440733805\n",
            "train loss:0.2734231703572734\n",
            "train loss:0.19622209886505504\n",
            "train loss:0.2670863366790429\n",
            "train loss:0.29112613356626565\n",
            "train loss:0.19693956859213313\n",
            "train loss:0.18697452406733156\n",
            "train loss:0.22761114518317835\n",
            "train loss:0.24521440657174334\n",
            "train loss:0.2314157128915551\n",
            "train loss:0.24855007087816047\n",
            "train loss:0.19488926650947902\n",
            "train loss:0.2185410879047964\n",
            "train loss:0.23325059413354549\n",
            "train loss:0.4151830691554562\n",
            "train loss:0.2330025156186485\n",
            "train loss:0.3575056503262205\n",
            "train loss:0.20500055480346002\n",
            "train loss:0.25342132235986553\n",
            "train loss:0.2887499829639479\n",
            "train loss:0.32570417999943074\n",
            "train loss:0.1950664972184526\n",
            "train loss:0.20001315552740595\n",
            "train loss:0.37852399263183045\n",
            "train loss:0.30267125165050834\n",
            "train loss:0.28153998482928355\n",
            "train loss:0.2147993801792239\n",
            "train loss:0.1861059263579826\n",
            "train loss:0.26313895652081387\n",
            "train loss:0.27829089975981036\n",
            "train loss:0.2083693273060862\n",
            "train loss:0.4116249414048187\n",
            "train loss:0.18414296435325003\n",
            "train loss:0.3088081298221297\n",
            "train loss:0.42438965087935465\n",
            "train loss:0.26928175983697733\n",
            "train loss:0.22797184435837017\n",
            "train loss:0.17820230712186613\n",
            "train loss:0.1622138946197856\n",
            "train loss:0.14736576949510627\n",
            "train loss:0.2916680317759527\n",
            "train loss:0.17893560684962773\n",
            "train loss:0.22981644231335327\n",
            "train loss:0.18676267943913877\n",
            "train loss:0.19164431922807193\n",
            "train loss:0.1517633956782735\n",
            "train loss:0.11431369844424429\n",
            "train loss:0.2198705014936569\n",
            "train loss:0.22488824271892235\n",
            "train loss:0.19593685439225578\n",
            "train loss:0.23638919724377988\n",
            "train loss:0.21670941433067095\n",
            "train loss:0.13723716950901488\n",
            "train loss:0.3102729738986343\n",
            "train loss:0.37670019251462816\n",
            "train loss:0.2331679898318216\n",
            "train loss:0.1851578797382628\n",
            "train loss:0.14660298096484375\n",
            "train loss:0.2525422439485958\n",
            "train loss:0.14031851930662761\n",
            "train loss:0.225825453639864\n",
            "train loss:0.24814474558875\n",
            "train loss:0.18679343912347982\n",
            "train loss:0.23763326608381735\n",
            "train loss:0.17827575714965188\n",
            "train loss:0.33975874386108756\n",
            "train loss:0.2129195378390012\n",
            "train loss:0.1230717123464065\n",
            "train loss:0.2733608186738008\n",
            "train loss:0.20636243706304808\n",
            "train loss:0.2493815272081494\n",
            "train loss:0.19308626958030128\n",
            "train loss:0.08547483937549982\n",
            "train loss:0.2922956159765013\n",
            "train loss:0.1760757873914965\n",
            "train loss:0.1669079361098956\n",
            "train loss:0.3582767595743141\n",
            "train loss:0.14560362180916742\n",
            "train loss:0.23071376529645238\n",
            "train loss:0.26254020351047336\n",
            "train loss:0.22582048618379783\n",
            "train loss:0.15102196633033665\n",
            "train loss:0.1653925817745498\n",
            "train loss:0.19780550702025007\n",
            "train loss:0.11627367338855704\n",
            "train loss:0.2283021876838608\n",
            "train loss:0.21394997209746766\n",
            "train loss:0.13271633931072344\n",
            "train loss:0.18587250663909027\n",
            "train loss:0.12747576698491048\n",
            "train loss:0.2408656150410668\n",
            "train loss:0.15200652243117296\n",
            "train loss:0.2935185185227911\n",
            "train loss:0.2582656066058043\n",
            "train loss:0.27890092111875514\n",
            "train loss:0.33402633303390217\n",
            "train loss:0.1681145377178514\n",
            "train loss:0.1761458322440907\n",
            "train loss:0.402153793651149\n",
            "train loss:0.22589658069051005\n",
            "train loss:0.22490582836519343\n",
            "train loss:0.19483725752495742\n",
            "train loss:0.3001244810574668\n",
            "train loss:0.17367410196074157\n",
            "train loss:0.2694514233650662\n",
            "train loss:0.12629046267731928\n",
            "train loss:0.20770192834342566\n",
            "train loss:0.21695198787042255\n",
            "train loss:0.18886879002298673\n",
            "train loss:0.2189526097307625\n",
            "train loss:0.1712986202852421\n",
            "train loss:0.13027832945461879\n",
            "train loss:0.24381938851649704\n",
            "train loss:0.15561041445996066\n",
            "train loss:0.125019140066836\n",
            "train loss:0.14745162818989765\n",
            "train loss:0.1247605200437374\n",
            "train loss:0.13920031286283235\n",
            "train loss:0.26333699336386784\n",
            "train loss:0.21564112759189638\n",
            "train loss:0.17525400480132333\n",
            "train loss:0.17882261703950714\n",
            "train loss:0.2496758106006714\n",
            "train loss:0.14920420811320015\n",
            "train loss:0.21177026296407575\n",
            "train loss:0.3056344304825102\n",
            "train loss:0.1890081054411097\n",
            "train loss:0.13370985345707193\n",
            "train loss:0.13547767038869826\n",
            "train loss:0.12380616681847442\n",
            "train loss:0.1575525795144349\n",
            "train loss:0.13908264634681045\n",
            "train loss:0.09152855279183089\n",
            "train loss:0.23081085276019825\n",
            "train loss:0.12396102161373684\n",
            "train loss:0.09558274788476741\n",
            "train loss:0.14203435362742045\n",
            "train loss:0.22419329867428497\n",
            "train loss:0.15783107953789327\n",
            "train loss:0.17690491067503772\n",
            "train loss:0.14526143856029042\n",
            "train loss:0.18860594372267847\n",
            "train loss:0.07430424924377495\n",
            "train loss:0.13421303034592397\n",
            "train loss:0.17459219552480995\n",
            "train loss:0.1642807718966256\n",
            "train loss:0.11331058736748315\n",
            "train loss:0.2607781410207566\n",
            "train loss:0.22029198553306054\n",
            "train loss:0.13704327229467483\n",
            "train loss:0.17379556709800678\n",
            "train loss:0.2276970691618561\n",
            "train loss:0.20915430157228573\n",
            "train loss:0.1953331745478092\n",
            "train loss:0.13556456312662563\n",
            "train loss:0.2362173641490412\n",
            "train loss:0.22340067743462344\n",
            "train loss:0.21920034539791672\n",
            "train loss:0.16459262095413663\n",
            "train loss:0.16200242392643469\n",
            "train loss:0.0992581775460751\n",
            "train loss:0.2601932966436669\n",
            "train loss:0.1240438816364192\n",
            "train loss:0.23529179865816693\n",
            "train loss:0.15008480373917657\n",
            "train loss:0.11936363155044445\n",
            "train loss:0.060980851106095094\n",
            "train loss:0.24157527054074446\n",
            "train loss:0.13882457404029033\n",
            "train loss:0.18913130042139836\n",
            "train loss:0.17526502854801393\n",
            "train loss:0.11060875835905852\n",
            "train loss:0.13346434189198766\n",
            "train loss:0.1636176000580281\n",
            "train loss:0.17210372781961683\n",
            "train loss:0.17088913834244818\n",
            "train loss:0.12420808501840876\n",
            "train loss:0.12780684421529545\n",
            "train loss:0.11637284588221641\n",
            "train loss:0.20745978759949055\n",
            "train loss:0.2187499242011256\n",
            "train loss:0.16538274486289337\n",
            "train loss:0.19507722092046595\n",
            "train loss:0.15910865875135843\n",
            "train loss:0.08508216096564478\n",
            "train loss:0.1272643740243276\n",
            "train loss:0.16989155462485836\n",
            "train loss:0.13796495478948018\n",
            "train loss:0.10154460782759502\n",
            "train loss:0.16647411173766324\n",
            "train loss:0.1979027530783332\n",
            "train loss:0.14999482864092284\n",
            "train loss:0.268636089778045\n",
            "train loss:0.18587113637206648\n",
            "train loss:0.07023269627590438\n",
            "train loss:0.17850937668766018\n",
            "train loss:0.24851121082670258\n",
            "train loss:0.09134933494561474\n",
            "train loss:0.14173438715088493\n",
            "train loss:0.18245598020931472\n",
            "train loss:0.1328385842092916\n",
            "train loss:0.16192334981333317\n",
            "train loss:0.21162818838284317\n",
            "train loss:0.1749025598007824\n",
            "train loss:0.2087974342992581\n",
            "train loss:0.20995304644772228\n",
            "train loss:0.13221968151919472\n",
            "train loss:0.19284605574683206\n",
            "train loss:0.08811445468861394\n",
            "train loss:0.23523177248584365\n",
            "train loss:0.18766338252889342\n",
            "train loss:0.13754654583852596\n",
            "train loss:0.3130444863634902\n",
            "train loss:0.11964377301613467\n",
            "train loss:0.12318839633350542\n",
            "train loss:0.14523602091604226\n",
            "train loss:0.053902123608196836\n",
            "train loss:0.15629257624068013\n",
            "train loss:0.1527865082338022\n",
            "train loss:0.07438989002746056\n",
            "train loss:0.1142018986429271\n",
            "train loss:0.16069418880897082\n",
            "train loss:0.17388314141425615\n",
            "train loss:0.1225887800710488\n",
            "train loss:0.1428604244462795\n",
            "train loss:0.08443726781383487\n",
            "train loss:0.12386262251627571\n",
            "train loss:0.10300529510419346\n",
            "train loss:0.0772048808960435\n",
            "train loss:0.09683579740639595\n",
            "train loss:0.09506674638699543\n",
            "train loss:0.16289988519457949\n",
            "train loss:0.15542026016056906\n",
            "train loss:0.17057223601790292\n",
            "train loss:0.20362496722027792\n",
            "train loss:0.18309738844679213\n",
            "train loss:0.16314216998974707\n",
            "train loss:0.14140233498234334\n",
            "train loss:0.18941387628878253\n",
            "train loss:0.10366278545770408\n",
            "train loss:0.18359286939685457\n",
            "train loss:0.15994560693537457\n",
            "train loss:0.12017881134732822\n",
            "train loss:0.08960055585697538\n",
            "train loss:0.2143362791858485\n",
            "train loss:0.2012957770008747\n",
            "train loss:0.096932887364861\n",
            "train loss:0.15203094479799148\n",
            "train loss:0.05863749361139842\n",
            "train loss:0.08009288373034748\n",
            "train loss:0.2563192642371105\n",
            "train loss:0.17621803898480884\n",
            "train loss:0.17158510593430695\n",
            "train loss:0.22477878796560002\n",
            "train loss:0.09142860663331721\n",
            "train loss:0.14945579664753977\n",
            "train loss:0.11941459129640984\n",
            "train loss:0.1379756879882279\n",
            "train loss:0.15531353669706102\n",
            "train loss:0.1756690646662803\n",
            "train loss:0.11243303613508876\n",
            "train loss:0.12646132595475287\n",
            "train loss:0.08212576206551969\n",
            "train loss:0.13485077124628092\n",
            "train loss:0.200599776907947\n",
            "train loss:0.17484272826605235\n",
            "train loss:0.05159410019170267\n",
            "train loss:0.10930611737364616\n",
            "train loss:0.18348731078162797\n",
            "train loss:0.08684832697746504\n",
            "train loss:0.18807432561968995\n",
            "train loss:0.13934243251504175\n",
            "train loss:0.1284254765083032\n",
            "train loss:0.09248212285363656\n",
            "train loss:0.10858457803575043\n",
            "train loss:0.16723766144406188\n",
            "train loss:0.17760858307468894\n",
            "train loss:0.06900236864474985\n",
            "train loss:0.12775930299104707\n",
            "train loss:0.11356674880701587\n",
            "train loss:0.21038290884456592\n",
            "train loss:0.19128527737775175\n",
            "train loss:0.26210895759082276\n",
            "train loss:0.22240071143171009\n",
            "train loss:0.13836759476340005\n",
            "train loss:0.20917405929722604\n",
            "train loss:0.11794564474059184\n",
            "train loss:0.09937096270674042\n",
            "train loss:0.10163019955720794\n",
            "train loss:0.07546957097549431\n",
            "train loss:0.1527466562162864\n",
            "train loss:0.17949788199386277\n",
            "train loss:0.1043726476047555\n",
            "train loss:0.10979846946899308\n",
            "train loss:0.16967175994033337\n",
            "train loss:0.10628427951693854\n",
            "train loss:0.16579629685927202\n",
            "train loss:0.20942263572601058\n",
            "train loss:0.10594155253366694\n",
            "train loss:0.20966102677033877\n",
            "train loss:0.11532588956194155\n",
            "train loss:0.14937415940655258\n",
            "train loss:0.06324741041851378\n",
            "train loss:0.08582443363461162\n",
            "train loss:0.09688299690766117\n",
            "train loss:0.07539829883333811\n",
            "train loss:0.0640527202245654\n",
            "train loss:0.09460474964343468\n",
            "train loss:0.14413240354047446\n",
            "train loss:0.09617769517759159\n",
            "train loss:0.10760835169412182\n",
            "train loss:0.1786558491453378\n",
            "train loss:0.1863163351213225\n",
            "train loss:0.07687257722437214\n",
            "train loss:0.09112513627787612\n",
            "train loss:0.2736975421909782\n",
            "train loss:0.23464334300002496\n",
            "train loss:0.10666543071201645\n",
            "train loss:0.11654212895413397\n",
            "train loss:0.08577353677655437\n",
            "train loss:0.16677888851046047\n",
            "train loss:0.18597113235865032\n",
            "train loss:0.1728539987857053\n",
            "train loss:0.15960632991529825\n",
            "train loss:0.06934498733473364\n",
            "train loss:0.10199735764737605\n",
            "train loss:0.09438967286380658\n",
            "train loss:0.17476616605568765\n",
            "train loss:0.14951249227313265\n",
            "train loss:0.13953328383106267\n",
            "train loss:0.12704071285003402\n",
            "train loss:0.18826351967741264\n",
            "train loss:0.1839884787398111\n",
            "train loss:0.1601145294952925\n",
            "train loss:0.19536411847887478\n",
            "train loss:0.12603810055875153\n",
            "train loss:0.1736908472100884\n",
            "train loss:0.11679813071520284\n",
            "train loss:0.14563786504216888\n",
            "train loss:0.09461840245756481\n",
            "train loss:0.07072225962064428\n",
            "train loss:0.14397336222498008\n",
            "train loss:0.08127276910219562\n",
            "train loss:0.16978936272157505\n",
            "train loss:0.10906905705512829\n",
            "train loss:0.19092719924907553\n",
            "train loss:0.20078365178087865\n",
            "train loss:0.16367045136457212\n",
            "train loss:0.12148925465421101\n",
            "train loss:0.08227771298319939\n",
            "train loss:0.06912451923926582\n",
            "train loss:0.10802234977646787\n",
            "train loss:0.13020692700863776\n",
            "train loss:0.098632335272626\n",
            "train loss:0.08157416841055254\n",
            "train loss:0.2122678354117293\n",
            "train loss:0.15701934562151917\n",
            "train loss:0.16960856739337782\n",
            "train loss:0.052782766594935436\n",
            "train loss:0.0823224027305184\n",
            "train loss:0.08028385255626704\n",
            "train loss:0.13622186112451737\n",
            "train loss:0.08592883824020583\n",
            "train loss:0.08027923285830146\n",
            "train loss:0.16514197120138502\n",
            "train loss:0.05615007518302936\n",
            "=== epoch:2, train acc:0.967, test acc:0.962 ===\n",
            "train loss:0.05471751719124676\n",
            "train loss:0.04771046782080088\n",
            "train loss:0.041624200623917\n",
            "train loss:0.19832269074512893\n",
            "train loss:0.09171636761413389\n",
            "train loss:0.09347152962178287\n",
            "train loss:0.08501197045184375\n",
            "train loss:0.07665083986100481\n",
            "train loss:0.033015071023025484\n",
            "train loss:0.08296768638150848\n",
            "train loss:0.20871782969739008\n",
            "train loss:0.14462876937515884\n",
            "train loss:0.05198220871600201\n",
            "train loss:0.08282534459803813\n",
            "train loss:0.14082444060852542\n",
            "train loss:0.09891185812527369\n",
            "train loss:0.18163957843257172\n",
            "train loss:0.18467128223969076\n",
            "train loss:0.05399581009532055\n",
            "train loss:0.14216769694559442\n",
            "train loss:0.06303111257591759\n",
            "train loss:0.06817671683556258\n",
            "train loss:0.13976171589207656\n",
            "train loss:0.11913987519964209\n",
            "train loss:0.11606568313438041\n",
            "train loss:0.16347627956763802\n",
            "train loss:0.026757459213521476\n",
            "train loss:0.04320217520942639\n",
            "train loss:0.08143534276903636\n",
            "train loss:0.09359005361586792\n",
            "train loss:0.10457330092729517\n",
            "train loss:0.17925236615070317\n",
            "train loss:0.04550454816329872\n",
            "train loss:0.08822466820516173\n",
            "train loss:0.08230129965438027\n",
            "train loss:0.09381820262551589\n",
            "train loss:0.07720687947706208\n",
            "train loss:0.11922675273392473\n",
            "train loss:0.054873000123702706\n",
            "train loss:0.058866060937418696\n",
            "train loss:0.20577523199571016\n",
            "train loss:0.08087084571779896\n",
            "train loss:0.03632414364786076\n",
            "train loss:0.08277507613563223\n",
            "train loss:0.040972880860219316\n",
            "train loss:0.10026333779343012\n",
            "train loss:0.12506982814731019\n",
            "train loss:0.10174960471738187\n",
            "train loss:0.12305551961015473\n",
            "train loss:0.11727001216257657\n",
            "train loss:0.030599411766930533\n",
            "train loss:0.0825373279775144\n",
            "train loss:0.06025461476466813\n",
            "train loss:0.09061341854412686\n",
            "train loss:0.15500627871689182\n",
            "train loss:0.17859369613359166\n",
            "train loss:0.06240972252464254\n",
            "train loss:0.04301403965392647\n",
            "train loss:0.08145262830989342\n",
            "train loss:0.06558576066569698\n",
            "train loss:0.20756955194658577\n",
            "train loss:0.11866957458621792\n",
            "train loss:0.08046064303440964\n",
            "train loss:0.04369726834204186\n",
            "train loss:0.11732779135538601\n",
            "train loss:0.053622062171015006\n",
            "train loss:0.22913579913239832\n",
            "train loss:0.08856153172916033\n",
            "train loss:0.056278324707407276\n",
            "train loss:0.12209682751682807\n",
            "train loss:0.08929185564693257\n",
            "train loss:0.058589993795146365\n",
            "train loss:0.06451266750061473\n",
            "train loss:0.05802304328245225\n",
            "train loss:0.06702394003077648\n",
            "train loss:0.07431878150613415\n",
            "train loss:0.036005503952606974\n",
            "train loss:0.024356067154766677\n",
            "train loss:0.05480545742112283\n",
            "train loss:0.07354206506205418\n",
            "train loss:0.1261810586290878\n",
            "train loss:0.06471714863179172\n",
            "train loss:0.11649648318316676\n",
            "train loss:0.18016163520487236\n",
            "train loss:0.09332403094694271\n",
            "train loss:0.08664275551593664\n",
            "train loss:0.10739792516131635\n",
            "train loss:0.05330607751850767\n",
            "train loss:0.13326630126008715\n",
            "train loss:0.11214960344132281\n",
            "train loss:0.057416734664414985\n",
            "train loss:0.12575077698637205\n",
            "train loss:0.09253411900131152\n",
            "train loss:0.13820084434875285\n",
            "train loss:0.10342310651406755\n",
            "train loss:0.07020510052897015\n",
            "train loss:0.07521506411104158\n",
            "train loss:0.06891769364044242\n",
            "train loss:0.09007466745021318\n",
            "train loss:0.06620297944435595\n",
            "train loss:0.20127561490547843\n",
            "train loss:0.15175895736478956\n",
            "train loss:0.12122295012426208\n",
            "train loss:0.08901345920670682\n",
            "train loss:0.060548242310809276\n",
            "train loss:0.18219540541931692\n",
            "train loss:0.04681725301944351\n",
            "train loss:0.073118300994098\n",
            "train loss:0.1520509118527309\n",
            "train loss:0.14087302940457305\n",
            "train loss:0.13100300148797486\n",
            "train loss:0.23677799718654127\n",
            "train loss:0.09117158758911267\n",
            "train loss:0.06794415044521879\n",
            "train loss:0.0437559893716975\n",
            "train loss:0.07071647287514492\n",
            "train loss:0.08924062760097819\n",
            "train loss:0.09305627868627539\n",
            "train loss:0.06696101034412161\n",
            "train loss:0.06368076936191334\n",
            "train loss:0.060223149637789575\n",
            "train loss:0.04301745938029144\n",
            "train loss:0.03915261160527362\n",
            "train loss:0.16661760773442238\n",
            "train loss:0.11849560393327317\n",
            "train loss:0.18298895967598378\n",
            "train loss:0.07131005220076733\n",
            "train loss:0.036836826203728516\n",
            "train loss:0.13902433954640298\n",
            "train loss:0.07139410819117091\n",
            "train loss:0.15067385479775613\n",
            "train loss:0.09430293384927248\n",
            "train loss:0.05799744774340236\n",
            "train loss:0.08759733290106496\n",
            "train loss:0.09738709009874218\n",
            "train loss:0.11607183281552233\n",
            "train loss:0.05559635001889968\n",
            "train loss:0.07946788118620556\n",
            "train loss:0.07631100649098368\n",
            "train loss:0.09995883890204738\n",
            "train loss:0.08101034150766802\n",
            "train loss:0.07802602469291213\n",
            "train loss:0.12168118938469391\n",
            "train loss:0.08909783216265892\n",
            "train loss:0.09901618565951781\n",
            "train loss:0.0482178976717561\n",
            "train loss:0.047125951932663314\n",
            "train loss:0.12005858648673236\n",
            "train loss:0.09492389526373031\n",
            "train loss:0.10476759685203627\n",
            "train loss:0.04505398653901525\n",
            "train loss:0.0927743327108601\n",
            "train loss:0.07226676045401867\n",
            "train loss:0.014602477415500472\n",
            "train loss:0.016726598641481696\n",
            "train loss:0.06695471810068838\n",
            "train loss:0.07523966808307021\n",
            "train loss:0.08336490911863946\n",
            "train loss:0.031758153615772965\n",
            "train loss:0.07243686867251027\n",
            "train loss:0.03912465333815938\n",
            "train loss:0.04120689148927024\n",
            "train loss:0.07060511058825605\n",
            "train loss:0.03163794647073685\n",
            "train loss:0.1552081948401024\n",
            "train loss:0.12364773971766224\n",
            "train loss:0.09084655783300226\n",
            "train loss:0.07801685551639607\n",
            "train loss:0.10682181040673305\n",
            "train loss:0.022790259584713736\n",
            "train loss:0.08678673346588091\n",
            "train loss:0.12780853404046635\n",
            "train loss:0.051087577793798786\n",
            "train loss:0.08738592357094246\n",
            "train loss:0.06311816503021286\n",
            "train loss:0.06169040364924306\n",
            "train loss:0.0568442316144629\n",
            "train loss:0.08035534510897671\n",
            "train loss:0.05106820500229305\n",
            "train loss:0.03904350617859727\n",
            "train loss:0.09220452662509292\n",
            "train loss:0.13614970068094698\n",
            "train loss:0.10907019187486383\n",
            "train loss:0.06048991504312389\n",
            "train loss:0.04018239795815668\n",
            "train loss:0.09612589241922827\n",
            "train loss:0.10546215108388948\n",
            "train loss:0.08718402644957239\n",
            "train loss:0.10490454363811255\n",
            "train loss:0.11142493014135786\n",
            "train loss:0.13358448524286096\n",
            "train loss:0.11930040158001999\n",
            "train loss:0.037901640393615994\n",
            "train loss:0.11343113221209584\n",
            "train loss:0.0494222788504203\n",
            "train loss:0.03594458604456348\n",
            "train loss:0.10622917799533094\n",
            "train loss:0.09763939784235134\n",
            "train loss:0.0880126651218485\n",
            "train loss:0.06998793217050105\n",
            "train loss:0.08320910569706935\n",
            "train loss:0.08165905542601046\n",
            "train loss:0.12891114073001547\n",
            "train loss:0.08017224239382832\n",
            "train loss:0.05512822903972743\n",
            "train loss:0.2281039342457875\n",
            "train loss:0.1842306581201912\n",
            "train loss:0.07278156975356281\n",
            "train loss:0.15627635953578195\n",
            "train loss:0.06422919364272042\n",
            "train loss:0.10837087723022071\n",
            "train loss:0.10698571339093588\n",
            "train loss:0.0781321857911337\n",
            "train loss:0.0388846034723173\n",
            "train loss:0.12886326077847784\n",
            "train loss:0.12089853242898427\n",
            "train loss:0.05869524468685987\n",
            "train loss:0.1251097597873779\n",
            "train loss:0.05657332296098889\n",
            "train loss:0.02532108541771589\n",
            "train loss:0.05340689405934132\n",
            "train loss:0.06182292639775518\n",
            "train loss:0.13728405551053607\n",
            "train loss:0.10656395420434894\n",
            "train loss:0.06291421689982027\n",
            "train loss:0.11103383960099976\n",
            "train loss:0.04885717704818842\n",
            "train loss:0.1356632950054375\n",
            "train loss:0.0790653629545569\n",
            "train loss:0.08044288424637162\n",
            "train loss:0.056654058562790814\n",
            "train loss:0.090293473078997\n",
            "train loss:0.03667990130266599\n",
            "train loss:0.13878055283713142\n",
            "train loss:0.15140737811023855\n",
            "train loss:0.03129400945787597\n",
            "train loss:0.04246024922081719\n",
            "train loss:0.02920087721833199\n",
            "train loss:0.048991718601761367\n",
            "train loss:0.09731119696811183\n",
            "train loss:0.1561253636081668\n",
            "train loss:0.09434748964542057\n",
            "train loss:0.14591871347678737\n",
            "train loss:0.07491317281630633\n",
            "train loss:0.09696421200500988\n",
            "train loss:0.05192147511256601\n",
            "train loss:0.12488613395784193\n",
            "train loss:0.10264164700804056\n",
            "train loss:0.04073937597414283\n",
            "train loss:0.11608190576776263\n",
            "train loss:0.09395261931631106\n",
            "train loss:0.06203614386698446\n",
            "train loss:0.08367653165133947\n",
            "train loss:0.05823073800756167\n",
            "train loss:0.03681685138264922\n",
            "train loss:0.0917449918110131\n",
            "train loss:0.04447857715963483\n",
            "train loss:0.10646940618390803\n",
            "train loss:0.06394357848396051\n",
            "train loss:0.07355835666538219\n",
            "train loss:0.08545959446918586\n",
            "train loss:0.10196228164818467\n",
            "train loss:0.05571524094600699\n",
            "train loss:0.05856505291707356\n",
            "train loss:0.04251062497208383\n",
            "train loss:0.09436508921005765\n",
            "train loss:0.11685301361498134\n",
            "train loss:0.0905619604151713\n",
            "train loss:0.03942272959085712\n",
            "train loss:0.0647083998633627\n",
            "train loss:0.09749946740987371\n",
            "train loss:0.05990029515881009\n",
            "train loss:0.0667959116919123\n",
            "train loss:0.0672880315866013\n",
            "train loss:0.04033741623031759\n",
            "train loss:0.06647704816876704\n",
            "train loss:0.04620066067358555\n",
            "train loss:0.05349679342001841\n",
            "train loss:0.038768007897297924\n",
            "train loss:0.15782527231223895\n",
            "train loss:0.15737449710689236\n",
            "train loss:0.04422447798289993\n",
            "train loss:0.019450148233134287\n",
            "train loss:0.0661521057806003\n",
            "train loss:0.0334605511005559\n",
            "train loss:0.022886660665821467\n",
            "train loss:0.037427700426056525\n",
            "train loss:0.08044103730942717\n",
            "train loss:0.0576767637564811\n",
            "train loss:0.1285963661666682\n",
            "train loss:0.04640906763710724\n",
            "train loss:0.081832713787458\n",
            "train loss:0.11429747593563543\n",
            "train loss:0.04811825978190095\n",
            "train loss:0.048100839058633375\n",
            "train loss:0.0961907927385069\n",
            "train loss:0.1409401139807958\n",
            "train loss:0.03277036178792774\n",
            "train loss:0.12814556465612537\n",
            "train loss:0.03903608944204478\n",
            "train loss:0.02049644714009945\n",
            "train loss:0.0442939878816925\n",
            "train loss:0.07228850847639444\n",
            "train loss:0.05261458365444189\n",
            "train loss:0.0544512736412829\n",
            "train loss:0.0970250381331643\n",
            "train loss:0.05441460933608098\n",
            "train loss:0.05116979619130227\n",
            "train loss:0.061610823558080376\n",
            "train loss:0.01895431629376199\n",
            "train loss:0.05096891452957331\n",
            "train loss:0.0630903769943846\n",
            "train loss:0.023632943790320538\n",
            "train loss:0.06430965549991331\n",
            "train loss:0.06894911719515902\n",
            "train loss:0.037332993895664184\n",
            "train loss:0.09011375685689044\n",
            "train loss:0.10011592559633935\n",
            "train loss:0.13064387086280974\n",
            "train loss:0.06773466888992698\n",
            "train loss:0.13727127706356718\n",
            "train loss:0.12046584196887516\n",
            "train loss:0.05075747185434534\n",
            "train loss:0.13014742445690564\n",
            "train loss:0.04208019736247739\n",
            "train loss:0.05595118997168197\n",
            "train loss:0.021555308983802614\n",
            "train loss:0.022632618748330325\n",
            "train loss:0.05498156087730972\n",
            "train loss:0.0639908004918142\n",
            "train loss:0.11497263600368214\n",
            "train loss:0.06226444402307277\n",
            "train loss:0.0570702471746167\n",
            "train loss:0.030619827309181648\n",
            "train loss:0.060489695827188335\n",
            "train loss:0.06625898705253955\n",
            "train loss:0.05843473069351135\n",
            "train loss:0.035421375063342975\n",
            "train loss:0.09100093248256683\n",
            "train loss:0.15596834287314232\n",
            "train loss:0.12830104130267458\n",
            "train loss:0.04183857499325505\n",
            "train loss:0.06950197878941895\n",
            "train loss:0.11528781035402279\n",
            "train loss:0.06195554459947627\n",
            "train loss:0.06172925714757705\n",
            "train loss:0.018050589440591163\n",
            "train loss:0.0625497364855073\n",
            "train loss:0.04293074059706398\n",
            "train loss:0.09099676514556548\n",
            "train loss:0.07522790676013741\n",
            "train loss:0.0888038905103443\n",
            "train loss:0.04459847684319871\n",
            "train loss:0.09272150368482693\n",
            "train loss:0.06139533467905523\n",
            "train loss:0.15068432490882142\n",
            "train loss:0.07171404431490135\n",
            "train loss:0.0588207447478945\n",
            "train loss:0.06588548272879946\n",
            "train loss:0.10559881815772756\n",
            "train loss:0.0739724485157094\n",
            "train loss:0.04239111553446009\n",
            "train loss:0.05632207334582672\n",
            "train loss:0.09927281456963234\n",
            "train loss:0.023726806609176157\n",
            "train loss:0.05966503881148924\n",
            "train loss:0.1193369433661722\n",
            "train loss:0.2492525899065944\n",
            "train loss:0.18644099437646694\n",
            "train loss:0.09389297858818345\n",
            "train loss:0.07292920677415879\n",
            "train loss:0.050894185010152214\n",
            "train loss:0.11938019227980178\n",
            "train loss:0.056392743224521806\n",
            "train loss:0.053744550025777216\n",
            "train loss:0.05251962427008022\n",
            "train loss:0.07568850319811445\n",
            "train loss:0.14548783977343618\n",
            "train loss:0.12480018148867034\n",
            "train loss:0.06710046014897351\n",
            "train loss:0.11556139446005181\n",
            "train loss:0.046808658084248174\n",
            "train loss:0.1291852118745721\n",
            "train loss:0.13286858851521657\n",
            "train loss:0.18137485290343022\n",
            "train loss:0.06189329280857829\n",
            "train loss:0.09339086349290932\n",
            "train loss:0.12711961607949307\n",
            "train loss:0.09890487885123102\n",
            "train loss:0.02491179207121535\n",
            "train loss:0.0410468504943493\n",
            "train loss:0.041686052701305244\n",
            "train loss:0.10817458177821467\n",
            "train loss:0.05051140389075445\n",
            "train loss:0.04977984170323127\n",
            "train loss:0.04740888109846457\n",
            "train loss:0.037927864883891486\n",
            "train loss:0.022638462107642893\n",
            "train loss:0.02183871653635578\n",
            "train loss:0.03436544490777042\n",
            "train loss:0.08698507357664771\n",
            "train loss:0.03286124501974829\n",
            "train loss:0.019091138863302135\n",
            "train loss:0.033004669624992954\n",
            "train loss:0.09811709267219108\n",
            "train loss:0.08899007584907043\n",
            "train loss:0.08436473563332793\n",
            "train loss:0.07693030702829956\n",
            "train loss:0.020388269137462067\n",
            "train loss:0.11489910868012009\n",
            "train loss:0.08350472951903117\n",
            "train loss:0.10551793948954712\n",
            "train loss:0.017548388088085228\n",
            "train loss:0.051690340178050714\n",
            "train loss:0.0777165970784971\n",
            "train loss:0.03960833835409314\n",
            "train loss:0.0735931460942642\n",
            "train loss:0.06127553057023975\n",
            "train loss:0.03095739022240324\n",
            "train loss:0.07604719331575331\n",
            "train loss:0.062107056869662045\n",
            "train loss:0.03484073501992545\n",
            "train loss:0.054261174583309604\n",
            "train loss:0.13972430740306685\n",
            "train loss:0.0564516559067648\n",
            "train loss:0.14059052696291213\n",
            "train loss:0.061975752325572946\n",
            "train loss:0.038237653632972074\n",
            "train loss:0.07876530707984536\n",
            "train loss:0.03891118647278156\n",
            "train loss:0.10345496460496245\n",
            "train loss:0.04373928447746724\n",
            "train loss:0.057840656554603014\n",
            "train loss:0.09842969250556974\n",
            "train loss:0.05980375002762432\n",
            "train loss:0.09470898236041021\n",
            "train loss:0.05988638637119886\n",
            "train loss:0.07852411318319694\n",
            "train loss:0.08651537286485801\n",
            "train loss:0.059001169406890396\n",
            "train loss:0.027223500814600742\n",
            "train loss:0.06206052003555589\n",
            "train loss:0.0682260329965685\n",
            "train loss:0.05759451011408213\n",
            "train loss:0.09335599275821738\n",
            "train loss:0.0692492633146074\n",
            "train loss:0.08234415920817377\n",
            "train loss:0.036623421799384645\n",
            "train loss:0.2173753218780923\n",
            "train loss:0.09420309644926812\n",
            "train loss:0.04206363166123097\n",
            "train loss:0.09033121601573407\n",
            "train loss:0.02233300302271052\n",
            "train loss:0.1068855836097039\n",
            "train loss:0.07138240937207364\n",
            "train loss:0.07185149693969645\n",
            "train loss:0.055545049912450725\n",
            "train loss:0.1217398033890872\n",
            "train loss:0.023453920580520014\n",
            "train loss:0.05366801051546178\n",
            "train loss:0.0959279742427774\n",
            "train loss:0.04106852301651065\n",
            "train loss:0.04394223085373194\n",
            "train loss:0.06917735633976005\n",
            "train loss:0.02386418531266771\n",
            "train loss:0.026421129159264117\n",
            "train loss:0.12108476704876672\n",
            "train loss:0.05188534522497065\n",
            "train loss:0.13648496905680046\n",
            "train loss:0.03470713986979421\n",
            "train loss:0.07715532291554981\n",
            "train loss:0.04509139890492522\n",
            "train loss:0.06752301701957214\n",
            "train loss:0.03956820388605067\n",
            "train loss:0.13843648784078794\n",
            "train loss:0.18015164514609158\n",
            "train loss:0.017708378649079013\n",
            "train loss:0.04147782052153978\n",
            "train loss:0.10185570203402261\n",
            "train loss:0.07647636081104896\n",
            "train loss:0.05046753896279401\n",
            "train loss:0.05694956538142194\n",
            "train loss:0.05896465445578283\n",
            "train loss:0.05375564030950488\n",
            "train loss:0.05085698984497476\n",
            "train loss:0.03585033891734712\n",
            "train loss:0.06251892661758299\n",
            "train loss:0.038231068381921876\n",
            "train loss:0.1648526609118267\n",
            "train loss:0.037536140008138756\n",
            "train loss:0.037555109568272876\n",
            "train loss:0.026404352415529325\n",
            "train loss:0.027665085071348773\n",
            "train loss:0.16286194150238736\n",
            "train loss:0.029560348864865357\n",
            "train loss:0.05072305343109198\n",
            "train loss:0.02980961692779498\n",
            "train loss:0.11068470161682509\n",
            "train loss:0.0345197182941296\n",
            "train loss:0.03881248807353332\n",
            "train loss:0.05681654432072133\n",
            "train loss:0.033596528544810864\n",
            "train loss:0.09773463906480222\n",
            "train loss:0.06471752917779461\n",
            "train loss:0.0667290815127436\n",
            "train loss:0.02967661784080619\n",
            "train loss:0.04664752141720069\n",
            "train loss:0.07899001418858763\n",
            "train loss:0.036148657259108305\n",
            "train loss:0.04437744852877236\n",
            "train loss:0.0787722728226763\n",
            "train loss:0.05522454043656359\n",
            "train loss:0.01370479172017829\n",
            "train loss:0.02275598485379474\n",
            "train loss:0.043747550557229724\n",
            "train loss:0.08026458790702395\n",
            "train loss:0.1763880632326452\n",
            "train loss:0.16375135440828234\n",
            "train loss:0.03552336763001549\n",
            "train loss:0.09198007314541266\n",
            "train loss:0.0841594174851258\n",
            "train loss:0.13821369138734257\n",
            "train loss:0.037739727813002805\n",
            "train loss:0.01882711954050366\n",
            "train loss:0.1255851722622701\n",
            "train loss:0.048061229551265044\n",
            "train loss:0.06539701495592025\n",
            "train loss:0.16127178148176508\n",
            "train loss:0.1437706588867222\n",
            "train loss:0.06604100107104499\n",
            "train loss:0.04529539189940558\n",
            "train loss:0.1131300364386611\n",
            "train loss:0.1277015594307569\n",
            "train loss:0.0409093589543996\n",
            "train loss:0.036645805596135485\n",
            "train loss:0.04001717164054745\n",
            "train loss:0.022849909137574974\n",
            "train loss:0.08181821363842236\n",
            "train loss:0.06792044617281005\n",
            "train loss:0.0478189920065962\n",
            "train loss:0.054449300649804536\n",
            "train loss:0.021105846837348133\n",
            "train loss:0.0868013936198221\n",
            "train loss:0.09693393168750772\n",
            "train loss:0.07576069167183147\n",
            "train loss:0.09390222159270126\n",
            "train loss:0.08506367542200383\n",
            "train loss:0.06316668586899928\n",
            "train loss:0.1303672664689855\n",
            "train loss:0.07230129454292698\n",
            "train loss:0.09226848478816566\n",
            "train loss:0.07478445006013208\n",
            "train loss:0.03682305768143792\n",
            "train loss:0.057683961091352806\n",
            "train loss:0.03329264513444719\n",
            "train loss:0.11522479608918887\n",
            "train loss:0.038127353413872\n",
            "train loss:0.06646056038387534\n",
            "train loss:0.06079590143030985\n",
            "train loss:0.06572488348881426\n",
            "train loss:0.046038377136669716\n",
            "train loss:0.02742845019560444\n",
            "train loss:0.04116014605945845\n",
            "train loss:0.13975259953548075\n",
            "train loss:0.03291340228827559\n",
            "train loss:0.05323682007703419\n",
            "train loss:0.038745242752374655\n",
            "train loss:0.10224760296705354\n",
            "train loss:0.029811738317827467\n",
            "train loss:0.08671897495415774\n",
            "train loss:0.03303318240414889\n",
            "train loss:0.072156240140751\n",
            "train loss:0.04200802251772256\n",
            "train loss:0.13952850913519935\n",
            "train loss:0.026097341865465397\n",
            "train loss:0.032880453064671446\n",
            "train loss:0.03279148530607816\n",
            "train loss:0.11213787587135222\n",
            "train loss:0.05534413966585971\n",
            "train loss:0.07270839365359211\n",
            "train loss:0.01847409126016832\n",
            "train loss:0.029089200366887953\n",
            "train loss:0.10398452151954514\n",
            "train loss:0.04084876516138978\n",
            "train loss:0.018656838453122007\n",
            "train loss:0.03399663519500559\n",
            "train loss:0.09757965788652402\n",
            "train loss:0.03412902628753738\n",
            "train loss:0.04314400557942052\n",
            "train loss:0.07081714578000739\n",
            "train loss:0.16479729135225937\n",
            "train loss:0.021153477904587478\n",
            "train loss:0.027014521362226582\n",
            "train loss:0.03871292244040659\n",
            "train loss:0.05156586949900498\n",
            "train loss:0.0398144860003791\n",
            "train loss:0.09008709221344842\n",
            "train loss:0.11055923143694964\n",
            "train loss:0.028695942993997393\n",
            "train loss:0.02550717939900496\n",
            "=== epoch:3, train acc:0.974, test acc:0.974 ===\n",
            "train loss:0.04935722440575763\n",
            "train loss:0.036281204213135\n",
            "train loss:0.047083057968762734\n",
            "train loss:0.05844389691839073\n",
            "train loss:0.03501992290667943\n",
            "train loss:0.035298453309129174\n",
            "train loss:0.06887531616487848\n",
            "train loss:0.11045434600181124\n",
            "train loss:0.047372630840354334\n",
            "train loss:0.05365321555172022\n",
            "train loss:0.052451422473749\n",
            "train loss:0.034513842398884054\n",
            "train loss:0.053288170769845815\n",
            "train loss:0.07173738894331654\n",
            "train loss:0.06068877125907576\n",
            "train loss:0.02166689903576091\n",
            "train loss:0.11200119490923782\n",
            "train loss:0.03418016738446666\n",
            "train loss:0.06771672452174136\n",
            "train loss:0.02168599524080042\n",
            "train loss:0.04423360686315211\n",
            "train loss:0.07487689935812843\n",
            "train loss:0.04010532352232108\n",
            "train loss:0.03141773393097472\n",
            "train loss:0.032374393849899666\n",
            "train loss:0.10041259585100468\n",
            "train loss:0.07176923587619805\n",
            "train loss:0.08072952141003269\n",
            "train loss:0.038109627650747414\n",
            "train loss:0.10534511308736391\n",
            "train loss:0.06986006948159057\n",
            "train loss:0.09466154277701617\n",
            "train loss:0.04161020453904207\n",
            "train loss:0.03254590131461026\n",
            "train loss:0.02721742469667402\n",
            "train loss:0.026314557478204895\n",
            "train loss:0.0831030988030569\n",
            "train loss:0.06611803812763183\n",
            "train loss:0.09372139793364882\n",
            "train loss:0.05414615666511932\n",
            "train loss:0.08709491914128653\n",
            "train loss:0.01066346439662152\n",
            "train loss:0.07881583730579277\n",
            "train loss:0.0218261792781314\n",
            "train loss:0.13634062958545623\n",
            "train loss:0.04940328910688731\n",
            "train loss:0.03657578914489365\n",
            "train loss:0.07592594704811609\n",
            "train loss:0.0707379528758556\n",
            "train loss:0.056005880182079366\n",
            "train loss:0.0729792519289481\n",
            "train loss:0.03178446205757253\n",
            "train loss:0.06682049556370083\n",
            "train loss:0.04054553166762765\n",
            "train loss:0.027476935811219235\n",
            "train loss:0.04171910204727713\n",
            "train loss:0.08359900641821662\n",
            "train loss:0.12008544794227571\n",
            "train loss:0.11784709198080538\n",
            "train loss:0.03529174940851814\n",
            "train loss:0.015579702509559692\n",
            "train loss:0.06171243240118856\n",
            "train loss:0.020921937998987943\n",
            "train loss:0.038290540155676976\n",
            "train loss:0.039809066980570844\n",
            "train loss:0.06107159384028177\n",
            "train loss:0.02788284588275953\n",
            "train loss:0.04244653286921103\n",
            "train loss:0.04666787202909493\n",
            "train loss:0.029623274021053005\n",
            "train loss:0.04946072261660911\n",
            "train loss:0.03180206050510143\n",
            "train loss:0.04586620132287081\n",
            "train loss:0.036102445251518914\n",
            "train loss:0.06774628049474789\n",
            "train loss:0.024018059722207413\n",
            "train loss:0.012343888992217857\n",
            "train loss:0.03371054557604317\n",
            "train loss:0.061110241731253304\n",
            "train loss:0.04628466111753316\n",
            "train loss:0.02453864643186046\n",
            "train loss:0.055387826350391176\n",
            "train loss:0.04692172270385844\n",
            "train loss:0.0609138710732967\n",
            "train loss:0.09338379613771544\n",
            "train loss:0.12429320617161213\n",
            "train loss:0.09413425676252712\n",
            "train loss:0.018516076185004865\n",
            "train loss:0.042368211916357944\n",
            "train loss:0.0140634019878721\n",
            "train loss:0.10108656384870963\n",
            "train loss:0.0523001979912741\n",
            "train loss:0.060491996978674176\n",
            "train loss:0.07708315307334507\n",
            "train loss:0.021613278637113802\n",
            "train loss:0.04195080560811342\n",
            "train loss:0.0667632243633061\n",
            "train loss:0.04028097124899724\n",
            "train loss:0.019301521119871736\n",
            "train loss:0.012302431254084978\n",
            "train loss:0.019292523168372853\n",
            "train loss:0.07618673075789886\n",
            "train loss:0.09768219754514872\n",
            "train loss:0.031864157188583465\n",
            "train loss:0.04955671700352597\n",
            "train loss:0.047505652252778495\n",
            "train loss:0.04857940844629399\n",
            "train loss:0.0545920707625198\n",
            "train loss:0.020946597230644782\n",
            "train loss:0.01624110733136806\n",
            "train loss:0.014319864263449788\n",
            "train loss:0.017880927815669894\n",
            "train loss:0.06057095282225424\n",
            "train loss:0.08888383015136972\n",
            "train loss:0.023127434024089896\n",
            "train loss:0.08979854048199122\n",
            "train loss:0.05395825370718579\n",
            "train loss:0.018071184750662425\n",
            "train loss:0.040993256636149285\n",
            "train loss:0.028476422397037484\n",
            "train loss:0.02943062371217886\n",
            "train loss:0.1555462080563397\n",
            "train loss:0.0237450929603561\n",
            "train loss:0.027191527762601458\n",
            "train loss:0.03918433783436995\n",
            "train loss:0.21168983154832482\n",
            "train loss:0.04690882818347377\n",
            "train loss:0.04571180249922854\n",
            "train loss:0.06946885210140277\n",
            "train loss:0.11574151667859804\n",
            "train loss:0.11938850775424495\n",
            "train loss:0.031318617874901984\n",
            "train loss:0.04578607050807466\n",
            "train loss:0.08688424959467027\n",
            "train loss:0.05370060046372985\n",
            "train loss:0.017372888199809877\n",
            "train loss:0.07141022924999367\n",
            "train loss:0.08894843982360418\n",
            "train loss:0.048663847375599076\n",
            "train loss:0.048070043150040126\n",
            "train loss:0.09901290163658018\n",
            "train loss:0.055124169522094736\n",
            "train loss:0.07620330017923013\n",
            "train loss:0.046302428032097484\n",
            "train loss:0.057911527741463534\n",
            "train loss:0.08380317375994772\n",
            "train loss:0.03917207965640245\n",
            "train loss:0.06684995948447489\n",
            "train loss:0.07961610065763891\n",
            "train loss:0.0777948452285495\n",
            "train loss:0.0572518387875842\n",
            "train loss:0.04929468670790296\n",
            "train loss:0.06609247367549108\n",
            "train loss:0.027531512556311078\n",
            "train loss:0.08233635838660469\n",
            "train loss:0.06721285462350374\n",
            "train loss:0.041074533180447424\n",
            "train loss:0.021308850384273274\n",
            "train loss:0.04863052654394205\n",
            "train loss:0.08831293718443828\n",
            "train loss:0.1114733433366097\n",
            "train loss:0.08061256152894983\n",
            "train loss:0.03208961604940059\n",
            "train loss:0.03558837182824793\n",
            "train loss:0.029576013776025277\n",
            "train loss:0.08417795203145992\n",
            "train loss:0.08486620882055212\n",
            "train loss:0.010105491741791766\n",
            "train loss:0.019596632031431183\n",
            "train loss:0.04521072063238051\n",
            "train loss:0.05590350664186693\n",
            "train loss:0.044257802976396095\n",
            "train loss:0.05098667938664827\n",
            "train loss:0.031155742195256103\n",
            "train loss:0.012565696719520476\n",
            "train loss:0.02187139426622218\n",
            "train loss:0.08032649835702335\n",
            "train loss:0.02891354207828948\n",
            "train loss:0.045906479513071766\n",
            "train loss:0.04057593008442629\n",
            "train loss:0.05214352462904844\n",
            "train loss:0.07595560013312727\n",
            "train loss:0.028454386968806925\n",
            "train loss:0.016481389405288437\n",
            "train loss:0.11308273642202323\n",
            "train loss:0.03935533831986106\n",
            "train loss:0.046027579678613394\n",
            "train loss:0.01452211379326199\n",
            "train loss:0.05459221523155633\n",
            "train loss:0.06141662104013558\n",
            "train loss:0.024645522906610345\n",
            "train loss:0.04966301370133389\n",
            "train loss:0.009639326971984462\n",
            "train loss:0.09712713863546257\n",
            "train loss:0.026984104021406146\n",
            "train loss:0.02245181384722966\n",
            "train loss:0.08368802127701304\n",
            "train loss:0.03432053297271064\n",
            "train loss:0.023340286273122453\n",
            "train loss:0.16599916368912904\n",
            "train loss:0.08054788813638263\n",
            "train loss:0.010343636296834666\n",
            "train loss:0.009822015324745834\n",
            "train loss:0.013400132386336402\n",
            "train loss:0.013773947257569146\n",
            "train loss:0.05706550089518379\n",
            "train loss:0.03980280675703728\n",
            "train loss:0.01574158233824589\n",
            "train loss:0.06326354935951546\n",
            "train loss:0.06576347498025537\n",
            "train loss:0.07523235988873817\n",
            "train loss:0.042039998189100744\n",
            "train loss:0.02444148713759166\n",
            "train loss:0.03003300298496281\n",
            "train loss:0.0724346882001835\n",
            "train loss:0.029987399892874488\n",
            "train loss:0.11812376267922929\n",
            "train loss:0.12455159065919208\n",
            "train loss:0.018380625755081217\n",
            "train loss:0.11060478010571735\n",
            "train loss:0.019476640839539497\n",
            "train loss:0.017395821370888777\n",
            "train loss:0.027077266600897102\n",
            "train loss:0.026286223938112693\n",
            "train loss:0.029805873570605906\n",
            "train loss:0.06344527710426813\n",
            "train loss:0.03523575322366146\n",
            "train loss:0.0661145186370805\n",
            "train loss:0.01970831508887874\n",
            "train loss:0.016527639413496328\n",
            "train loss:0.02719585597091315\n",
            "train loss:0.020819693235587532\n",
            "train loss:0.06868758506841943\n",
            "train loss:0.03326802986409063\n",
            "train loss:0.03368697470812312\n",
            "train loss:0.017640796921854895\n",
            "train loss:0.05887222111396709\n",
            "train loss:0.0419298257461997\n",
            "train loss:0.07497097850016138\n",
            "train loss:0.09097980465409745\n",
            "train loss:0.040839863580582156\n",
            "train loss:0.011836426181892543\n",
            "train loss:0.1277605672392275\n",
            "train loss:0.06457570879554342\n",
            "train loss:0.034347720126405676\n",
            "train loss:0.07030807888251481\n",
            "train loss:0.04996619591673665\n",
            "train loss:0.05080251004975264\n",
            "train loss:0.05114914452432874\n",
            "train loss:0.12746688799893835\n",
            "train loss:0.03039703720478107\n",
            "train loss:0.025758707784036522\n",
            "train loss:0.1021745005210562\n",
            "train loss:0.10111812251463871\n",
            "train loss:0.10567564647838502\n",
            "train loss:0.05573030047484567\n",
            "train loss:0.08720249350103948\n",
            "train loss:0.025222798184591843\n",
            "train loss:0.03623679698853202\n",
            "train loss:0.04187517889611076\n",
            "train loss:0.022514782194951493\n",
            "train loss:0.05789838123452795\n",
            "train loss:0.04722799853126862\n",
            "train loss:0.11771957495191049\n",
            "train loss:0.03981289275911075\n",
            "train loss:0.022363952667462713\n",
            "train loss:0.05621236826257542\n",
            "train loss:0.13192964387694212\n",
            "train loss:0.02435529892638916\n",
            "train loss:0.027701719308958737\n",
            "train loss:0.042780317794787484\n",
            "train loss:0.01999701237109916\n",
            "train loss:0.024364143886821128\n",
            "train loss:0.04534416639482882\n",
            "train loss:0.08381330595654697\n",
            "train loss:0.05335679035055379\n",
            "train loss:0.03305305577585183\n",
            "train loss:0.1090159922882949\n",
            "train loss:0.04408171187988543\n",
            "train loss:0.0637719147172141\n",
            "train loss:0.05909339499421305\n",
            "train loss:0.03730302398327286\n",
            "train loss:0.11647434670800022\n",
            "train loss:0.04065679778225746\n",
            "train loss:0.07512727228272525\n",
            "train loss:0.024014457707391693\n",
            "train loss:0.04419999010299832\n",
            "train loss:0.026015981800532485\n",
            "train loss:0.050410497060862695\n",
            "train loss:0.07253480493257732\n",
            "train loss:0.051898473151944\n",
            "train loss:0.10434819132550657\n",
            "train loss:0.07638120187800956\n",
            "train loss:0.048393238781278373\n",
            "train loss:0.08895795324088578\n",
            "train loss:0.015642742525934422\n",
            "train loss:0.029123613346874702\n",
            "train loss:0.040768006169099244\n",
            "train loss:0.040962175810523645\n",
            "train loss:0.0359059006072682\n",
            "train loss:0.03706222084755318\n",
            "train loss:0.04866721554588094\n",
            "train loss:0.03927624838303511\n",
            "train loss:0.050312854975976117\n",
            "train loss:0.035293857942384536\n",
            "train loss:0.03738626696887418\n",
            "train loss:0.025334469353066907\n",
            "train loss:0.026715498131499395\n",
            "train loss:0.07432748304390352\n",
            "train loss:0.025434200775053678\n",
            "train loss:0.02894800782828258\n",
            "train loss:0.022929716718088557\n",
            "train loss:0.0808784847434042\n",
            "train loss:0.03550139836431729\n",
            "train loss:0.03696689299497704\n",
            "train loss:0.04107549547872393\n",
            "train loss:0.03278617782278104\n",
            "train loss:0.06208920812781219\n",
            "train loss:0.05198960014250861\n",
            "train loss:0.014501009182382917\n",
            "train loss:0.015102002813148479\n",
            "train loss:0.07655359066450212\n",
            "train loss:0.04372820941856123\n",
            "train loss:0.013396238133554412\n",
            "train loss:0.005198780677211237\n",
            "train loss:0.12139960663899987\n",
            "train loss:0.10303082837966768\n",
            "train loss:0.015355753989800405\n",
            "train loss:0.024789829312048196\n",
            "train loss:0.07590713326017376\n",
            "train loss:0.026221771978779066\n",
            "train loss:0.03228823776565592\n",
            "train loss:0.04081754321805826\n",
            "train loss:0.0363680284751869\n",
            "train loss:0.05886887133788579\n",
            "train loss:0.038557906869688206\n",
            "train loss:0.0772454811671256\n",
            "train loss:0.0379101543404872\n",
            "train loss:0.029711249632859475\n",
            "train loss:0.02136555391410289\n",
            "train loss:0.0219679085426584\n",
            "train loss:0.03669564664473069\n",
            "train loss:0.0364296339579262\n",
            "train loss:0.08468518059915947\n",
            "train loss:0.13833243549165453\n",
            "train loss:0.012909440560595438\n",
            "train loss:0.07163906178440724\n",
            "train loss:0.1863112101620326\n",
            "train loss:0.02408795011823043\n",
            "train loss:0.09909779172521452\n",
            "train loss:0.08942351342897792\n",
            "train loss:0.07516680260948368\n",
            "train loss:0.13896255317197567\n",
            "train loss:0.05212027502326115\n",
            "train loss:0.018794275573559102\n",
            "train loss:0.031900668917533154\n",
            "train loss:0.028231436564766354\n",
            "train loss:0.07484425550018152\n",
            "train loss:0.04157002007130175\n",
            "train loss:0.010548986436354473\n",
            "train loss:0.02446826377231891\n",
            "train loss:0.05973926214426162\n",
            "train loss:0.04102469578121465\n",
            "train loss:0.07668547456998165\n",
            "train loss:0.05573347991516862\n",
            "train loss:0.03351447852792488\n",
            "train loss:0.09321818071314601\n",
            "train loss:0.021166742032842224\n",
            "train loss:0.024354925499961592\n",
            "train loss:0.02554554809985595\n",
            "train loss:0.04689436963841697\n",
            "train loss:0.060797887702970746\n",
            "train loss:0.045966445941271555\n",
            "train loss:0.03234606804866119\n",
            "train loss:0.016395172097866312\n",
            "train loss:0.053301512591067676\n",
            "train loss:0.076600639108598\n",
            "train loss:0.03135067502528194\n",
            "train loss:0.02107260245515212\n",
            "train loss:0.04864366625308242\n",
            "train loss:0.12275651408121217\n",
            "train loss:0.036016133495644545\n",
            "train loss:0.017265343368209295\n",
            "train loss:0.07996762821052442\n",
            "train loss:0.04173416455552041\n",
            "train loss:0.09410880026044914\n",
            "train loss:0.05167514818113144\n",
            "train loss:0.05454685274626599\n",
            "train loss:0.08347034552041199\n",
            "train loss:0.13440352330251432\n",
            "train loss:0.16980538701203798\n",
            "train loss:0.019146789557031883\n",
            "train loss:0.08458388359106024\n",
            "train loss:0.09581190161767435\n",
            "train loss:0.03827356476227141\n",
            "train loss:0.033800644452476776\n",
            "train loss:0.0130360847308666\n",
            "train loss:0.02191891932634998\n",
            "train loss:0.030978260950687092\n",
            "train loss:0.01770729588612419\n",
            "train loss:0.033669075300720594\n",
            "train loss:0.030034430298721517\n",
            "train loss:0.021311541739494174\n",
            "train loss:0.054671476763718314\n",
            "train loss:0.03523701185472083\n",
            "train loss:0.037545750626460206\n",
            "train loss:0.014553450441190243\n",
            "train loss:0.0727365597858575\n",
            "train loss:0.038674315763518693\n",
            "train loss:0.060288718932744845\n",
            "train loss:0.057326806368652485\n",
            "train loss:0.08493347213715624\n",
            "train loss:0.007734227603218135\n",
            "train loss:0.08529366923390157\n",
            "train loss:0.011311827509740047\n",
            "train loss:0.019866272921026416\n",
            "train loss:0.036642104512627796\n",
            "train loss:0.01978764475902066\n",
            "train loss:0.06562237966395772\n",
            "train loss:0.05613142549337641\n",
            "train loss:0.04971332300522463\n",
            "train loss:0.06644694376542165\n",
            "train loss:0.03900780882031902\n",
            "train loss:0.09966077518538052\n",
            "train loss:0.08861552302744945\n",
            "train loss:0.043537677237537355\n",
            "train loss:0.02256636278523718\n",
            "train loss:0.07894396683849948\n",
            "train loss:0.03361220893638217\n",
            "train loss:0.03317377754407921\n",
            "train loss:0.02262139576126254\n",
            "train loss:0.04045783659876801\n",
            "train loss:0.03379388104372533\n",
            "train loss:0.09161090461164476\n",
            "train loss:0.031865510750431196\n",
            "train loss:0.049263790508803716\n",
            "train loss:0.10464588635014571\n",
            "train loss:0.04530025963922112\n",
            "train loss:0.020935504989182962\n",
            "train loss:0.024295882126317617\n",
            "train loss:0.014535646474705564\n",
            "train loss:0.048070697448604295\n",
            "train loss:0.11261778098143513\n",
            "train loss:0.013026603003002417\n",
            "train loss:0.1034742945224054\n",
            "train loss:0.01688842197600631\n",
            "train loss:0.027512510012962964\n",
            "train loss:0.03666540240855999\n",
            "train loss:0.05729966897489756\n",
            "train loss:0.027185904012153154\n",
            "train loss:0.010193613759636882\n",
            "train loss:0.0648211204277086\n",
            "train loss:0.07217365463377848\n",
            "train loss:0.06034279250426362\n",
            "train loss:0.028585347199408785\n",
            "train loss:0.1009957615414637\n",
            "train loss:0.013131016054056633\n",
            "train loss:0.056741167399258184\n",
            "train loss:0.054029342194301215\n",
            "train loss:0.07359980588054472\n",
            "train loss:0.03807490362102819\n",
            "train loss:0.033084810771897566\n",
            "train loss:0.026942546075565586\n",
            "train loss:0.05556983717917562\n",
            "train loss:0.021428964431444878\n",
            "train loss:0.022913880965112345\n",
            "train loss:0.05227864239630581\n",
            "train loss:0.012635573588419794\n",
            "train loss:0.04604966231707599\n",
            "train loss:0.04019247372234716\n",
            "train loss:0.03371932897104814\n",
            "train loss:0.0454790925993336\n",
            "train loss:0.04756995776824767\n",
            "train loss:0.038288366546861796\n",
            "train loss:0.04287734036131649\n",
            "train loss:0.02306455157412218\n",
            "train loss:0.048055095133715746\n",
            "train loss:0.033254320975022525\n",
            "train loss:0.009717725785705802\n",
            "train loss:0.016380661193007243\n",
            "train loss:0.10083531301241729\n",
            "train loss:0.07095753323051239\n",
            "train loss:0.06878996347582618\n",
            "train loss:0.028599393579116287\n",
            "train loss:0.11698854837223724\n",
            "train loss:0.020297396551899723\n",
            "train loss:0.022783963917212637\n",
            "train loss:0.0529236886706315\n",
            "train loss:0.01448567156711505\n",
            "train loss:0.021547460910501477\n",
            "train loss:0.060368708220583926\n",
            "train loss:0.03340693097652784\n",
            "train loss:0.08040524490817176\n",
            "train loss:0.018957165750745197\n",
            "train loss:0.01518661508392788\n",
            "train loss:0.06027897461226684\n",
            "train loss:0.03914964719594247\n",
            "train loss:0.006959789930877968\n",
            "train loss:0.10002649490608162\n",
            "train loss:0.010404384083122456\n",
            "train loss:0.02910762322584115\n",
            "train loss:0.013706995044870418\n",
            "train loss:0.006120631376336465\n",
            "train loss:0.03597826282016486\n",
            "train loss:0.03076180736197394\n",
            "train loss:0.03620798065166234\n",
            "train loss:0.027982205118149178\n",
            "train loss:0.029850680616502895\n",
            "train loss:0.08921480929796859\n",
            "train loss:0.03899358895188659\n",
            "train loss:0.010843908862288466\n",
            "train loss:0.014128712769512228\n",
            "train loss:0.032544674218670536\n",
            "train loss:0.015381732835537574\n",
            "train loss:0.039578165645865096\n",
            "train loss:0.1331587057661086\n",
            "train loss:0.02471240016700952\n",
            "train loss:0.05420856897690701\n",
            "train loss:0.029582294704656197\n",
            "train loss:0.0808278768237689\n",
            "train loss:0.017253708169527503\n",
            "train loss:0.03430461931047088\n",
            "train loss:0.050714280448124836\n",
            "train loss:0.01928245118813475\n",
            "train loss:0.1030689629806455\n",
            "train loss:0.054684830089546554\n",
            "train loss:0.06171216856805845\n",
            "train loss:0.030838318140799473\n",
            "train loss:0.08250296585553882\n",
            "train loss:0.007117457050782977\n",
            "train loss:0.05606398565658654\n",
            "train loss:0.03208232828245035\n",
            "train loss:0.04338291704810791\n",
            "train loss:0.010130722398658676\n",
            "train loss:0.02010592743740475\n",
            "train loss:0.03456674311803729\n",
            "train loss:0.03186070560725099\n",
            "train loss:0.04042461453098198\n",
            "train loss:0.022980008173500807\n",
            "train loss:0.03119701316215731\n",
            "train loss:0.033479602659267076\n",
            "train loss:0.03286488757475161\n",
            "train loss:0.014865372782230156\n",
            "train loss:0.020705753126117603\n",
            "train loss:0.03372179048861572\n",
            "train loss:0.026922313218851773\n",
            "train loss:0.02671772438038284\n",
            "train loss:0.13586662142393405\n",
            "train loss:0.0373714539436174\n",
            "train loss:0.023437102500466125\n",
            "train loss:0.04834550971284906\n",
            "train loss:0.01443747555783236\n",
            "train loss:0.022666491284499525\n",
            "train loss:0.026583895859027092\n",
            "train loss:0.023561136112760925\n",
            "train loss:0.05882725304636878\n",
            "train loss:0.06411550710389141\n",
            "train loss:0.09607050560992374\n",
            "train loss:0.015840455925192547\n",
            "train loss:0.07184650418873019\n",
            "train loss:0.04137803951965274\n",
            "train loss:0.027526758680766376\n",
            "train loss:0.102053254137271\n",
            "train loss:0.031052000100836932\n",
            "train loss:0.07270874238061223\n",
            "train loss:0.05557376284260571\n",
            "train loss:0.05267880741538872\n",
            "train loss:0.09217797998251581\n",
            "train loss:0.0974108729110914\n",
            "train loss:0.024053177131240906\n",
            "train loss:0.056967501674894123\n",
            "train loss:0.03826575655349933\n",
            "train loss:0.06326324340031236\n",
            "train loss:0.11560549600427779\n",
            "train loss:0.06619483242255679\n",
            "train loss:0.029910197415254726\n",
            "train loss:0.07180671045281396\n",
            "train loss:0.009265155833758972\n",
            "train loss:0.04584972295015021\n",
            "train loss:0.016423730987048732\n",
            "train loss:0.038931119821573795\n",
            "train loss:0.024562476443562496\n",
            "train loss:0.007032157704527963\n",
            "train loss:0.00971338621286528\n",
            "train loss:0.027554404531077048\n",
            "train loss:0.027863523173744367\n",
            "train loss:0.016217117050237758\n",
            "train loss:0.009578073780702068\n",
            "train loss:0.00671321422020464\n",
            "train loss:0.012759293832732353\n",
            "train loss:0.058752516120611366\n",
            "train loss:0.023067048622280817\n",
            "train loss:0.03471126260264694\n",
            "train loss:0.031732369969320356\n",
            "train loss:0.020014367743504832\n",
            "train loss:0.05547554682204136\n",
            "train loss:0.03477678379243678\n",
            "train loss:0.03424757575700386\n",
            "train loss:0.03160103981654054\n",
            "train loss:0.09519792977739838\n",
            "=== epoch:4, train acc:0.987, test acc:0.983 ===\n",
            "train loss:0.015812710117694587\n",
            "train loss:0.057517859171596034\n",
            "train loss:0.04901956295026344\n",
            "train loss:0.014702661962484891\n",
            "train loss:0.014678280376247846\n",
            "train loss:0.05216363938320436\n",
            "train loss:0.026096657709138393\n",
            "train loss:0.03534540162027356\n",
            "train loss:0.012459735058075468\n",
            "train loss:0.05177796903550129\n",
            "train loss:0.020424099528756696\n",
            "train loss:0.1726279937120587\n",
            "train loss:0.0143832688897649\n",
            "train loss:0.014355574341922743\n",
            "train loss:0.03208885432584848\n",
            "train loss:0.10861326305717318\n",
            "train loss:0.0372719280439521\n",
            "train loss:0.08971193976346077\n",
            "train loss:0.03728242225290359\n",
            "train loss:0.03392218303310732\n",
            "train loss:0.021705606016449757\n",
            "train loss:0.02405642300394721\n",
            "train loss:0.02449820334851113\n",
            "train loss:0.023149436141570542\n",
            "train loss:0.01313414201539958\n",
            "train loss:0.019602136230276658\n",
            "train loss:0.010470254257054126\n",
            "train loss:0.03955919219882473\n",
            "train loss:0.029940303372390415\n",
            "train loss:0.05333259937578363\n",
            "train loss:0.010389389858314346\n",
            "train loss:0.02085373329478755\n",
            "train loss:0.10878438261622536\n",
            "train loss:0.04205168887133048\n",
            "train loss:0.01995758114073235\n",
            "train loss:0.05009564288301923\n",
            "train loss:0.0662198195829894\n",
            "train loss:0.08364142247056046\n",
            "train loss:0.013926230790610833\n",
            "train loss:0.03281950468277608\n",
            "train loss:0.012267579152480194\n",
            "train loss:0.04325970340733748\n",
            "train loss:0.024543641991749975\n",
            "train loss:0.02439277247006778\n",
            "train loss:0.04769141809893124\n",
            "train loss:0.02121158077106417\n",
            "train loss:0.125591882087371\n",
            "train loss:0.052712584635236245\n",
            "train loss:0.029013259577787588\n",
            "train loss:0.054170886630336146\n",
            "train loss:0.0256893965821812\n",
            "train loss:0.037108453870917574\n",
            "train loss:0.01803854735687765\n",
            "train loss:0.03349171453679919\n",
            "train loss:0.0509505716789667\n",
            "train loss:0.044658629851153785\n",
            "train loss:0.07393195248178852\n",
            "train loss:0.07807445491719837\n",
            "train loss:0.022579491222352657\n",
            "train loss:0.009428457273791827\n",
            "train loss:0.020360017976139712\n",
            "train loss:0.025340976818563843\n",
            "train loss:0.17234076912584365\n",
            "train loss:0.010038663679412385\n",
            "train loss:0.04068659002319726\n",
            "train loss:0.07749278668126036\n",
            "train loss:0.03929492713858262\n",
            "train loss:0.07795688687771722\n",
            "train loss:0.00914824488751416\n",
            "train loss:0.014511136856137374\n",
            "train loss:0.04514421134362138\n",
            "train loss:0.010781444455322777\n",
            "train loss:0.05042578298037638\n",
            "train loss:0.016545343161209772\n",
            "train loss:0.060155173928768776\n",
            "train loss:0.08339203977913787\n",
            "train loss:0.012984204130379358\n",
            "train loss:0.07595264188080188\n",
            "train loss:0.10565664919846356\n",
            "train loss:0.003161366312579917\n",
            "train loss:0.05314206967136365\n",
            "train loss:0.093470045037234\n",
            "train loss:0.011972102277213738\n",
            "train loss:0.06977835396091085\n",
            "train loss:0.01098591711930396\n",
            "train loss:0.016919747380558438\n",
            "train loss:0.035696891057792154\n",
            "train loss:0.047897836862479434\n",
            "train loss:0.09286922668948046\n",
            "train loss:0.011275067937140221\n",
            "train loss:0.03823496667904012\n",
            "train loss:0.05704645659370064\n",
            "train loss:0.07254858636770836\n",
            "train loss:0.024060672620696454\n",
            "train loss:0.04694956996247848\n",
            "train loss:0.027180317084365767\n",
            "train loss:0.023661268633895553\n",
            "train loss:0.04394733818313511\n",
            "train loss:0.0189966396451367\n",
            "train loss:0.07121346273583468\n",
            "train loss:0.03713296412237336\n",
            "train loss:0.05758110675746184\n",
            "train loss:0.07494022863836287\n",
            "train loss:0.06581883603268043\n",
            "train loss:0.024844526293600497\n",
            "train loss:0.021689421371117137\n",
            "train loss:0.047542323436170646\n",
            "train loss:0.08903555016569939\n",
            "train loss:0.01185644625473564\n",
            "train loss:0.07938416713860905\n",
            "train loss:0.04037161814197641\n",
            "train loss:0.05238724177710044\n",
            "train loss:0.10763060214012098\n",
            "train loss:0.010644179344033\n",
            "train loss:0.06627384132144148\n",
            "train loss:0.030043420107567997\n",
            "train loss:0.053958794700228206\n",
            "train loss:0.10561460157204378\n",
            "train loss:0.04516088216810114\n",
            "train loss:0.02583350141411977\n",
            "train loss:0.04369689932040051\n",
            "train loss:0.0278985381264678\n",
            "train loss:0.014583655826942312\n",
            "train loss:0.07759679511400773\n",
            "train loss:0.03489695034882129\n",
            "train loss:0.053680950567128585\n",
            "train loss:0.03834360945905033\n",
            "train loss:0.016984357042880344\n",
            "train loss:0.016299067842095796\n",
            "train loss:0.02874795429105741\n",
            "train loss:0.03506413385778841\n",
            "train loss:0.021988373628716685\n",
            "train loss:0.049747481154281695\n",
            "train loss:0.05156596412289338\n",
            "train loss:0.007180596219955298\n",
            "train loss:0.12886740816188713\n",
            "train loss:0.029875741356102856\n",
            "train loss:0.015738613557352905\n",
            "train loss:0.017290203770960026\n",
            "train loss:0.033892146316355884\n",
            "train loss:0.03045644916268699\n",
            "train loss:0.03506517122374233\n",
            "train loss:0.015848502768523537\n",
            "train loss:0.014909546760735486\n",
            "train loss:0.008760553704233783\n",
            "train loss:0.03451874152720945\n",
            "train loss:0.01092337964204174\n",
            "train loss:0.07071654628631467\n",
            "train loss:0.0079493959183462\n",
            "train loss:0.01787294892891031\n",
            "train loss:0.02444103217881683\n",
            "train loss:0.15662942003960845\n",
            "train loss:0.06006875412980008\n",
            "train loss:0.063442315270843\n",
            "train loss:0.04018423507336319\n",
            "train loss:0.07669308531285068\n",
            "train loss:0.0098065673778782\n",
            "train loss:0.05294494817228865\n",
            "train loss:0.01667145439256763\n",
            "train loss:0.10349195210305345\n",
            "train loss:0.034898225367390866\n",
            "train loss:0.10502568456386564\n",
            "train loss:0.010742688424266011\n",
            "train loss:0.03749128685306409\n",
            "train loss:0.043155091219504865\n",
            "train loss:0.013195193308546259\n",
            "train loss:0.02006200822038254\n",
            "train loss:0.037423438010543014\n",
            "train loss:0.022264154386803296\n",
            "train loss:0.06899051479523156\n",
            "train loss:0.016656988584030613\n",
            "train loss:0.055644464507697486\n",
            "train loss:0.07010899300788767\n",
            "train loss:0.046421192902877856\n",
            "train loss:0.08108633602372928\n",
            "train loss:0.0334140047957393\n",
            "train loss:0.05352210986407283\n",
            "train loss:0.05145352610532892\n",
            "train loss:0.013577315061491\n",
            "train loss:0.014203805939933036\n",
            "train loss:0.032110703197032714\n",
            "train loss:0.02387721986979228\n",
            "train loss:0.010431661473575864\n",
            "train loss:0.05009980799827192\n",
            "train loss:0.017135770123873222\n",
            "train loss:0.058283721693803206\n",
            "train loss:0.10178861890913689\n",
            "train loss:0.011520515887976075\n",
            "train loss:0.10824206209514928\n",
            "train loss:0.023705432691186287\n",
            "train loss:0.05880038391240914\n",
            "train loss:0.08170568771811207\n",
            "train loss:0.014124886734714877\n",
            "train loss:0.043041697153378446\n",
            "train loss:0.03509498142626746\n",
            "train loss:0.059575029578795576\n",
            "train loss:0.006188287132829872\n",
            "train loss:0.05492942067881966\n",
            "train loss:0.028109577703642658\n",
            "train loss:0.0034093818154798806\n",
            "train loss:0.053015504527420786\n",
            "train loss:0.035180153614814295\n",
            "train loss:0.01444951027203371\n",
            "train loss:0.014325515021980026\n",
            "train loss:0.07681832567706622\n",
            "train loss:0.032096404438887935\n",
            "train loss:0.08548496035622444\n",
            "train loss:0.02038855767726674\n",
            "train loss:0.048309287763645875\n",
            "train loss:0.07254012887640036\n",
            "train loss:0.029165886709650497\n",
            "train loss:0.005012534777033106\n",
            "train loss:0.032874755983756866\n",
            "train loss:0.02551200410373595\n",
            "train loss:0.029145588245262193\n",
            "train loss:0.029832825748082003\n",
            "train loss:0.05854519786617133\n",
            "train loss:0.030706734087338435\n",
            "train loss:0.07940828748128019\n",
            "train loss:0.02723092012262994\n",
            "train loss:0.030379369012346072\n",
            "train loss:0.036611790167628847\n",
            "train loss:0.007810942147218229\n",
            "train loss:0.016597179008643447\n",
            "train loss:0.016343860564832256\n",
            "train loss:0.006247855898936807\n",
            "train loss:0.03307890995355765\n",
            "train loss:0.056658360350397174\n",
            "train loss:0.01548283235976486\n",
            "train loss:0.013204052976374551\n",
            "train loss:0.008911055973716955\n",
            "train loss:0.045285839742514086\n",
            "train loss:0.018684171302772234\n",
            "train loss:0.07353602385576136\n",
            "train loss:0.016062083316775964\n",
            "train loss:0.050343216321076495\n",
            "train loss:0.02787903217932993\n",
            "train loss:0.03987649468465998\n",
            "train loss:0.036783765781887016\n",
            "train loss:0.033687864094339194\n",
            "train loss:0.05126386410679691\n",
            "train loss:0.05735902494329757\n",
            "train loss:0.04642144859383742\n",
            "train loss:0.015455282814076711\n",
            "train loss:0.04504244834189204\n",
            "train loss:0.06070600745543397\n",
            "train loss:0.011960391171809864\n",
            "train loss:0.019080333353149848\n",
            "train loss:0.019247597724074562\n",
            "train loss:0.034051499304177016\n",
            "train loss:0.03687192065238074\n",
            "train loss:0.14165891333078715\n",
            "train loss:0.013188860951664317\n",
            "train loss:0.07099952863892436\n",
            "train loss:0.020349027615838546\n",
            "train loss:0.07050148375698774\n",
            "train loss:0.010347700716873996\n",
            "train loss:0.02634001250436875\n",
            "train loss:0.03791986365819219\n",
            "train loss:0.04474586704463019\n",
            "train loss:0.04263164513724329\n",
            "train loss:0.024343645723287746\n",
            "train loss:0.06599576443826428\n",
            "train loss:0.02699920033311625\n",
            "train loss:0.01126323841349548\n",
            "train loss:0.018281387781960316\n",
            "train loss:0.029604021190003698\n",
            "train loss:0.007313577497273738\n",
            "train loss:0.06212976187558064\n",
            "train loss:0.018485477502052122\n",
            "train loss:0.005715340178551457\n",
            "train loss:0.026338879571169233\n",
            "train loss:0.07651049473903015\n",
            "train loss:0.037844807897448415\n",
            "train loss:0.02390970687171376\n",
            "train loss:0.0486251190633909\n",
            "train loss:0.010694331295192604\n",
            "train loss:0.021197044141026362\n",
            "train loss:0.0389645252203444\n",
            "train loss:0.022805288584937036\n",
            "train loss:0.0466862954258383\n",
            "train loss:0.0410526641603052\n",
            "train loss:0.05594869942622594\n",
            "train loss:0.02604951702158298\n",
            "train loss:0.0646451655855655\n",
            "train loss:0.016036640070903076\n",
            "train loss:0.01955956299290845\n",
            "train loss:0.006445159641420144\n",
            "train loss:0.05846072531078269\n",
            "train loss:0.07246881178703973\n",
            "train loss:0.1032499854863111\n",
            "train loss:0.01148858440679368\n",
            "train loss:0.005646558172817383\n",
            "train loss:0.025846034759899866\n",
            "train loss:0.014929976824728098\n",
            "train loss:0.06855970537276035\n",
            "train loss:0.017203740786405287\n",
            "train loss:0.014919262216058535\n",
            "train loss:0.015395626955886\n",
            "train loss:0.01153673919011495\n",
            "train loss:0.034365524130880586\n",
            "train loss:0.03266091495822651\n",
            "train loss:0.01479801600768153\n",
            "train loss:0.01781337240328346\n",
            "train loss:0.025141416038849382\n",
            "train loss:0.060107579560396314\n",
            "train loss:0.017682507986987225\n",
            "train loss:0.113350233981981\n",
            "train loss:0.01717742730360493\n",
            "train loss:0.031195913429536044\n",
            "train loss:0.048760048174113446\n",
            "train loss:0.03743405386856494\n",
            "train loss:0.010344339381594887\n",
            "train loss:0.020401408329338486\n",
            "train loss:0.013189644003608017\n",
            "train loss:0.04327005232983248\n",
            "train loss:0.013328068996902545\n",
            "train loss:0.022054969043089633\n",
            "train loss:0.006225804828265196\n",
            "train loss:0.018394123188268153\n",
            "train loss:0.017922401455667012\n",
            "train loss:0.03743416739587184\n",
            "train loss:0.015297184699573838\n",
            "train loss:0.007833169400424831\n",
            "train loss:0.03366804774862778\n",
            "train loss:0.012939447697313844\n",
            "train loss:0.045661324755921794\n",
            "train loss:0.04471876560158458\n",
            "train loss:0.03536514832312777\n",
            "train loss:0.021291534448659756\n",
            "train loss:0.004235945071716308\n",
            "train loss:0.013596880861117856\n",
            "train loss:0.004204899624966549\n",
            "train loss:0.026960311216706678\n",
            "train loss:0.039014673640262634\n",
            "train loss:0.032771198061035046\n",
            "train loss:0.038311408971697464\n",
            "train loss:0.016327555033890856\n",
            "train loss:0.011934908274940361\n",
            "train loss:0.00969642782106938\n",
            "train loss:0.042932670225688974\n",
            "train loss:0.006733142688575718\n",
            "train loss:0.007359669719021661\n",
            "train loss:0.005297466592140765\n",
            "train loss:0.05145396916853023\n",
            "train loss:0.04786676625802208\n",
            "train loss:0.0068804152595835135\n",
            "train loss:0.01000426993943435\n",
            "train loss:0.08024393319476601\n",
            "train loss:0.019493719936311764\n",
            "train loss:0.03962078192173834\n",
            "train loss:0.025103090537924005\n",
            "train loss:0.014366846469488801\n",
            "train loss:0.01534666429877552\n",
            "train loss:0.0371062335674978\n",
            "train loss:0.0609503461186556\n",
            "train loss:0.03847916967430876\n",
            "train loss:0.03944036197691361\n",
            "train loss:0.026125441104768447\n",
            "train loss:0.015300378767173682\n",
            "train loss:0.09311274398481403\n",
            "train loss:0.051940487358940655\n",
            "train loss:0.003594303277033843\n",
            "train loss:0.015045002742212588\n",
            "train loss:0.03954407242139391\n",
            "train loss:0.01769340589622258\n",
            "train loss:0.11765147912809726\n",
            "train loss:0.03367568965885735\n",
            "train loss:0.01583471821551856\n",
            "train loss:0.014028482658337525\n",
            "train loss:0.02388927014245198\n",
            "train loss:0.003945694101308791\n",
            "train loss:0.009212413503239363\n",
            "train loss:0.017127516604110726\n",
            "train loss:0.026292154492011346\n",
            "train loss:0.02330725495206276\n",
            "train loss:0.07850214094182284\n",
            "train loss:0.005444945123949067\n",
            "train loss:0.06101667444244436\n",
            "train loss:0.006215369876005401\n",
            "train loss:0.03738872009108251\n",
            "train loss:0.021900988202188956\n",
            "train loss:0.006880321015993142\n",
            "train loss:0.04212908510565952\n",
            "train loss:0.04810834648056752\n",
            "train loss:0.029072539400770432\n",
            "train loss:0.024013292402017005\n",
            "train loss:0.06395286558162193\n",
            "train loss:0.017750143618427458\n",
            "train loss:0.04418839118983457\n",
            "train loss:0.03307608892502261\n",
            "train loss:0.043425253366838105\n",
            "train loss:0.013040348518941368\n",
            "train loss:0.0593858991413408\n",
            "train loss:0.026602500939878593\n",
            "train loss:0.0255392507880507\n",
            "train loss:0.04427465559295575\n",
            "train loss:0.012288501179538853\n",
            "train loss:0.02804840411685943\n",
            "train loss:0.03724683416674914\n",
            "train loss:0.024236793026694882\n",
            "train loss:0.032463442020408774\n",
            "train loss:0.09015945406195519\n",
            "train loss:0.01563036386125666\n",
            "train loss:0.02070660113048617\n",
            "train loss:0.026444200523176645\n",
            "train loss:0.032671959399426104\n",
            "train loss:0.056090111602550154\n",
            "train loss:0.03262386680685414\n",
            "train loss:0.03217378190364347\n",
            "train loss:0.012925566957185388\n",
            "train loss:0.10394088337972308\n",
            "train loss:0.03426865713741703\n",
            "train loss:0.021178090368846533\n",
            "train loss:0.039500950324547875\n",
            "train loss:0.036777697208466874\n",
            "train loss:0.01793372509277167\n",
            "train loss:0.03840095998662938\n",
            "train loss:0.00449832501886766\n",
            "train loss:0.0072730704541686475\n",
            "train loss:0.04346964821143346\n",
            "train loss:0.03132342902206756\n",
            "train loss:0.014932050890083098\n",
            "train loss:0.02673491239670283\n",
            "train loss:0.021477417826489386\n",
            "train loss:0.09415978860275485\n",
            "train loss:0.0495828521557414\n",
            "train loss:0.05707487783551737\n",
            "train loss:0.03178580229383318\n",
            "train loss:0.013269962385173518\n",
            "train loss:0.028915450009309125\n",
            "train loss:0.03071864046307278\n",
            "train loss:0.023127172625921552\n",
            "train loss:0.03367729020889859\n",
            "train loss:0.014461231025646613\n",
            "train loss:0.03180221733057513\n",
            "train loss:0.030950607080295293\n",
            "train loss:0.05066677807329176\n",
            "train loss:0.008397164673551793\n",
            "train loss:0.021762475242155688\n",
            "train loss:0.010801649137156916\n",
            "train loss:0.038404585331739224\n",
            "train loss:0.01789386584375158\n",
            "train loss:0.05692632828832122\n",
            "train loss:0.03195043188468435\n",
            "train loss:0.016633285809825718\n",
            "train loss:0.03467629845388241\n",
            "train loss:0.03015969513855357\n",
            "train loss:0.02149194220463911\n",
            "train loss:0.0518087115294631\n",
            "train loss:0.02774379021717709\n",
            "train loss:0.030358597482206217\n",
            "train loss:0.012596025455197115\n",
            "train loss:0.006465294510170035\n",
            "train loss:0.015062609968722584\n",
            "train loss:0.006706133798174831\n",
            "train loss:0.03627775352559568\n",
            "train loss:0.016663533156407017\n",
            "train loss:0.019547336377543103\n",
            "train loss:0.04635540472228186\n",
            "train loss:0.027523122149470673\n",
            "train loss:0.04586478818150403\n",
            "train loss:0.008431676626745236\n",
            "train loss:0.020267922024805384\n",
            "train loss:0.01676083569173382\n",
            "train loss:0.012868412826947244\n",
            "train loss:0.030250635450689102\n",
            "train loss:0.023652088352777073\n",
            "train loss:0.029481597552989736\n",
            "train loss:0.024768562195496793\n",
            "train loss:0.04091210360661639\n",
            "train loss:0.02090632771389453\n",
            "train loss:0.008729916304257835\n",
            "train loss:0.006202898931609313\n",
            "train loss:0.03179879904651833\n",
            "train loss:0.08754753804912348\n",
            "train loss:0.02811906934644554\n",
            "train loss:0.018294664926304157\n",
            "train loss:0.017858993430218708\n",
            "train loss:0.019955726176202566\n",
            "train loss:0.0976914742979979\n",
            "train loss:0.016365323419488112\n",
            "train loss:0.02083094313438683\n",
            "train loss:0.022976344702844594\n",
            "train loss:0.06062428529812138\n",
            "train loss:0.025725760037325754\n",
            "train loss:0.04038826609831685\n",
            "train loss:0.01788817815380959\n",
            "train loss:0.048255387860768675\n",
            "train loss:0.005831590094540155\n",
            "train loss:0.015593146094222507\n",
            "train loss:0.035970879648427184\n",
            "train loss:0.012504710913505888\n",
            "train loss:0.028121879107976703\n",
            "train loss:0.013076366383681214\n",
            "train loss:0.018987903444621656\n",
            "train loss:0.13732373949090512\n",
            "train loss:0.01229909197148482\n",
            "train loss:0.014449695267788477\n",
            "train loss:0.0567911941927135\n",
            "train loss:0.08909141240658026\n",
            "train loss:0.01531404786328007\n",
            "train loss:0.05498992054047693\n",
            "train loss:0.02946879840823873\n",
            "train loss:0.04748938573126868\n",
            "train loss:0.03375015829363123\n",
            "train loss:0.014118531934705601\n",
            "train loss:0.03146155081134743\n",
            "train loss:0.03607952930799075\n",
            "train loss:0.08677963177535913\n",
            "train loss:0.050405790344671866\n",
            "train loss:0.056569388666988074\n",
            "train loss:0.010746602064223105\n",
            "train loss:0.06524014908056891\n",
            "train loss:0.011010585053729434\n",
            "train loss:0.07682509392209164\n",
            "train loss:0.03983246522686242\n",
            "train loss:0.04683366437854572\n",
            "train loss:0.04433669601389613\n",
            "train loss:0.015791423314863544\n",
            "train loss:0.012659649764181316\n",
            "train loss:0.03458752887920266\n",
            "train loss:0.04001813769739172\n",
            "train loss:0.03787718879414903\n",
            "train loss:0.04788622604288965\n",
            "train loss:0.010842681899934979\n",
            "train loss:0.03964232154578537\n",
            "train loss:0.018753356615049725\n",
            "train loss:0.015987275908098203\n",
            "train loss:0.016936803740632635\n",
            "train loss:0.02050889796179035\n",
            "train loss:0.10981035641887721\n",
            "train loss:0.025096379982811685\n",
            "train loss:0.07937290338464069\n",
            "train loss:0.06992568918232682\n",
            "train loss:0.02487173142778086\n",
            "train loss:0.01320018253233642\n",
            "train loss:0.019710102658331966\n",
            "train loss:0.02977823642028987\n",
            "train loss:0.028308281810654586\n",
            "train loss:0.03731049869734119\n",
            "train loss:0.01841857010773532\n",
            "train loss:0.09323531688792182\n",
            "train loss:0.07464427348901159\n",
            "train loss:0.025523928785230177\n",
            "train loss:0.007507219004144056\n",
            "train loss:0.013255202463980861\n",
            "train loss:0.004346250282763982\n",
            "train loss:0.036531290018377315\n",
            "train loss:0.013964376184457592\n",
            "train loss:0.02785784311398792\n",
            "train loss:0.017242849416211393\n",
            "train loss:0.024579421551631186\n",
            "train loss:0.04432482696237852\n",
            "train loss:0.03307698999169344\n",
            "train loss:0.036646980865710374\n",
            "train loss:0.012699242476970326\n",
            "train loss:0.05922687459330017\n",
            "train loss:0.03226035319924941\n",
            "train loss:0.016687043735440797\n",
            "train loss:0.06779855073326219\n",
            "train loss:0.03573562488110623\n",
            "train loss:0.026560339180722644\n",
            "train loss:0.02774904747325314\n",
            "train loss:0.04918279432679288\n",
            "train loss:0.02187259855799396\n",
            "train loss:0.06555444982054373\n",
            "train loss:0.01449589191549815\n",
            "train loss:0.010017766107642498\n",
            "train loss:0.0851360173903189\n",
            "train loss:0.049560011964351204\n",
            "train loss:0.03367542601003193\n",
            "train loss:0.025397966985809647\n",
            "train loss:0.013783802671509688\n",
            "train loss:0.022522383550458294\n",
            "train loss:0.01674809329627269\n",
            "train loss:0.01804627629423068\n",
            "train loss:0.0051104194462469775\n",
            "train loss:0.021824420677729935\n",
            "train loss:0.032165323395123874\n",
            "train loss:0.014705295062712705\n",
            "train loss:0.026070274924210456\n",
            "train loss:0.013939588239212628\n",
            "train loss:0.034744117573343794\n",
            "train loss:0.03710954061639023\n",
            "train loss:0.025667083874586338\n",
            "train loss:0.03441578515691458\n",
            "train loss:0.02995572949440719\n",
            "train loss:0.06633089847563223\n",
            "train loss:0.028905549205786696\n",
            "train loss:0.007348357643471451\n",
            "train loss:0.029812231666903735\n",
            "train loss:0.037401719116419006\n",
            "train loss:0.015108596679754343\n",
            "train loss:0.014953955041999666\n",
            "train loss:0.005433928073192088\n",
            "train loss:0.024187806838136493\n",
            "train loss:0.026232185004409162\n",
            "train loss:0.04935737850754494\n",
            "train loss:0.017038757162147367\n",
            "=== epoch:5, train acc:0.984, test acc:0.985 ===\n",
            "train loss:0.01973885055819638\n",
            "train loss:0.04417719396070683\n",
            "train loss:0.025183827770665423\n",
            "train loss:0.009674298779406272\n",
            "train loss:0.04746145759941605\n",
            "train loss:0.01161567162157898\n",
            "train loss:0.03139626834672542\n",
            "train loss:0.016798558715548327\n",
            "train loss:0.022091663025720534\n",
            "train loss:0.01837004295385626\n",
            "train loss:0.024065748072343532\n",
            "train loss:0.03522690811360349\n",
            "train loss:0.012987426372094118\n",
            "train loss:0.01809724293239143\n",
            "train loss:0.018421740648446212\n",
            "train loss:0.01906233483443273\n",
            "train loss:0.01470423769334047\n",
            "train loss:0.003572519295791647\n",
            "train loss:0.02610961602831566\n",
            "train loss:0.01708967808438262\n",
            "train loss:0.014949636108622162\n",
            "train loss:0.0018457889609429286\n",
            "train loss:0.017515746279767614\n",
            "train loss:0.03232078540321366\n",
            "train loss:0.004533020089782748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-814f6d024930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# パラメータの保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-learning-from-scratch/common/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-learning-from-scratch/common/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-learning-from-scratch/ch07/simple_convnet.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-learning-from-scratch/ch07/simple_convnet.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0m引数のxは入力データ\u001b[0m\u001b[0;31m、\u001b[0m\u001b[0mtは教師ラベル\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-learning-from-scratch/ch07/simple_convnet.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deep-learning-from-scratch/common/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0marg_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2619\u001b[0m     \"\"\"\n\u001b[1;32m   2620\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2621\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN0rR80O_8-K",
        "colab_type": "text"
      },
      "source": [
        "## ALEXNET\n",
        "　CNNの代表的なネットワーク。２０１２年に発表されAI画像解析のブームの火付け役となった。\n",
        "\n",
        "　　　参考　　[深層学習論文の読解（AlexNet)](https://qiita.com/ttomomasa/items/8243f6a29a024a02512f)\n",
        "\n",
        "\n",
        "　このALEXNETの特徴は\n",
        "* 活性化関数にReLUを用いた\n",
        "* LRNというReLUの値を正規化した\n",
        "* DropOUTを用いた\n",
        "* GPUを使った\n",
        "\n",
        "\n",
        "![代替テキスト](https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F209049%2Fbbf90731-67c0-2d75-f64e-1272ddebf653.jpeg?ixlib=rb-1.2.2&auto=compress%2Cformat&gif-q=60&w=1400&fit=max&s=dedabee207698ba84f04984640e3c60e)"
      ]
    }
  ]
}