{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI勉強会V2.1",
      "provenance": [],
      "collapsed_sections": [
        "Mmx0g2eG6NXt",
        "g5PqkPWW1nz7"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nstshirotays/Study-AI/blob/master/AI%E5%8B%89%E5%BC%B7%E4%BC%9AV2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-USdFhOfX6MW",
        "colab_type": "text"
      },
      "source": [
        "# AI勉強会\n",
        "この勉強会では O'REILLY「ゼロから作る Deep Learning」について解説を行い内容の理解を深めることを目的としています。\n",
        "\n",
        "## 本日のキーワード\n",
        "* 活性化関数\n",
        "* ソフトマックス\n",
        "* 交差エントロピー\n",
        "* 勾配降下法\n",
        "* 学習率\n",
        "* イテレーション、エポック数\n",
        "* ミニバッチ\n",
        "* バックプロパゲーション\n",
        "\n",
        "<hr>\n",
        "学習の準備として下記を実行して下さい\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-PZLcS7KCHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-from-scratch.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfqLQnfynLnC",
        "colab_type": "text"
      },
      "source": [
        "# パーセプトロン\n",
        "ハイ・ローの電気信号による演算を行うCPUの内部構成は、ANDやOR演算を行う論理素子の集合体である。心理学者・計算機科学者のフランク・ローゼンブラットは、人間の脳内でもこのような演算素子と同様のプロセスが、生物学的原理で動作すると想定した。そして実際に回路を作成し画像の学習処理を行った（1957年）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ihZ8xE6aCN",
        "colab_type": "text"
      },
      "source": [
        "## パーセプトロンの概要\n",
        "\n",
        "\n",
        "![論理記号](https://docs.google.com/drawings/d/e/2PACX-1vQ2SKuRbOhYXSsd1vbFCWV3iJzEBX2LUoM6QSHY6jpWt1hiOzBMgbzeRm-Pqs5yS-qdAoZBO57Jxr4G/pub?w=390&h=365)\n",
        "\n",
        "電気回路的な演算素子はそれぞれに実体が異なるが、脳内ではすべて同様の組織体から構成されると想定し、同一の細胞（ニューロン）が外部のコントロールにより様々な２値（真偽値）を出力すると考察し、まずはニューロンを下記のようにモデル化した。\n",
        "\n",
        "![ニューロンモデル](https://docs.google.com/drawings/d/e/2PACX-1vSinEodXTx3n5L_cMTGqzbDcKC1y6ncEjm0vjk5wHCiDPIT476aqJxuNiiniflzJvy0zjylvyEz0vTP/pub?w=302&h=75)\n",
        "\n",
        "\n",
        "この円形はニューロンを表す。ニューロンへの入力は矢印で表され、それぞれの入力信号に対して重み付けがされる。これらの情報を元にニューロンYは入力データに重みを乗算した値を閾値θと比較して出力を０か１の二値に振り分ける。これを一般化し下記のような構成にしたものがパーセプトロンである。\n",
        "\n",
        "![パーセプトロン](https://docs.google.com/drawings/d/e/2PACX-1vSpMVD85wOprci70XqNYFpaA3gNI_rTRVezumjEJ6trAHQ6qMq7gPA-PH7bDzHfOkMb-pEKQXxSOvs_/pub?w=302&h=220)\n",
        "\n",
        "パーセプトロンでは、複数のニューロンからの入力値（X1、X2、、、）にそれぞれの重み（W1、W2、、、、）を掛けた値を合算し、さらに前述のモデルで閾値θを外部からの入力値として引き出す。これをバイアスと改めて再定義し、それらの総合計がゼロ以下であればゼロを、そうでなければ１を結果として出力するというモデルである。\n",
        "\n",
        "CPUにおける演算素子はそれぞれ固有のデバイスであるが、このパーセプトロンでは信号の重みであるWとバイアス値であるｂの値を工夫することで、ニューロンの構成を変えずにAND、OR、NANDの演算を行うことができる。\n",
        "\n",
        "この発見により、神経伝達をそのモデルとしたパーセプトロンはコンピューターのCPUと等価と見なせる事となり、人間の脳活動をコンピューターで摸倣できる可能性が示された。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-RfG8FikpGb",
        "colab_type": "text"
      },
      "source": [
        "## ANDパーセプトロンの例\n",
        "X1とX2のAND（論理積）Yは以下の表の通りとなる。\n",
        "\n",
        "![真理値（AND)](https://docs.google.com/drawings/d/e/2PACX-1vRBDAVLdjYPZdwKgzW7KjJChXCcVpuf2F4BV37uJsMeq9RiKaEOiBm1ku6IWWPfIIAoYMAsXsFP5y4r/pub?w=110&h=104)\n",
        "\n",
        "これをパーセプトロンで実現する場合は例として下記のように重みWとバイアスｂを設定する。\n",
        "\n",
        "![ANDパーセプトロン](https://docs.google.com/drawings/d/e/2PACX-1vS0_fzuAskoeaRAzwEG5YW7YVbXQbVppfbPCA64l-KFjYmMjRuCpsCgvR1Y-k2p24j3Ods_0WE_NumA/pub?w=302&h=220)\n",
        "\n",
        "\n",
        "実際にpythonで実装を行う。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwcofQZYpQ7i",
        "colab_type": "code",
        "outputId": "52e4bb87-ab6a-4770-a53b-ecb941aebd5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np    # 行列演算の為のライブラリの読み込み\n",
        "\n",
        "#---------------------------\n",
        "#  論理積を計算する\n",
        "#---------------------------\n",
        "def AND(x1,x2):\n",
        "  x = np.array([x1,x2])\n",
        "  w = np.array([0.5,0.5])\n",
        "  b = -0.7\n",
        "  result = b + np.sum(w*x)\n",
        "  if result <= 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "  \n",
        "print(AND(0,0))\n",
        "print(AND(0,1))\n",
        "print(AND(1,0))\n",
        "print(AND(1,1))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ycu4loorf6r",
        "colab_type": "text"
      },
      "source": [
        "## ORパーセプトロン\n",
        "先程のANDパーセプトロンの重みWとバイアスｂを変更することでor（論理和）パーセプトロンも実装できる。\n",
        "\n",
        "ORパーセプトロンの真偽値は下記の通り\n",
        "\n",
        "![ORパーセプトロン](https://docs.google.com/drawings/d/e/2PACX-1vT5n56HwMBreq9nmHCYS3rydXTPR3YQRXs62---PUy9PV6hyP6uSNDVN6A63I-KLN3whl6YPsMwCWYP/pub?w=110&h=104)\n",
        "\n",
        "これをパーセプトロンで実現する場合は例として下記のように重みWとバイアスｂを設定する。\n",
        "* W1 = 0.5\n",
        "* W2 = 0.5\n",
        "* b = -0.2\n",
        "\n",
        "これを同様にpythonで実装する\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzrYtfH9sySp",
        "colab_type": "code",
        "outputId": "a825c0e8-1a4a-4744-bff7-563ccd3dc32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#---------------------------\n",
        "#  論理和を計算する\n",
        "#---------------------------\n",
        "def OR(x1,x2):\n",
        "  x = np.array([x1,x2])\n",
        "  w = np.array([0.5,0.5])\n",
        "  b = -0.2\n",
        "  result = b + np.sum(w*x)\n",
        "  if result <= 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "  \n",
        "print(OR(0,0))\n",
        "print(OR(0,1))\n",
        "print(OR(1,0))\n",
        "print(OR(1,1))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbrDEEzDtYfg",
        "colab_type": "text"
      },
      "source": [
        "## アルゴリズムとしてのパーセプトロン\n",
        "上記と同様にNANDなども表現できる。さらにこれらのパーセプトロンを複数組み合わせることもできる。これにより単層では表現できないXORなどの論理演算も可能となる。\n",
        "\n",
        "最も重要な点は、このパーセプトロンを構成する一つ一つのニューロンは全て等価であり、単に入力値への重み付けが異なるだけという点である。\n",
        "\n",
        "よってパーセプトロンではこの重みの値を決定することで演算が可能となり、これは演算アルゴリズムをパーセプトロンの重みで表現しているとも言い換えることができる。\n",
        "\n",
        "これまでのコンピューティングはオブジェクト指向であれなんであれ、実体としては命令を逐次実行することで目的を達成している。AIでは情報を伝達するためのプログラムは存在するが、実際の判定ロジックそのものはこの「重み」が担っている。\n",
        "\n",
        "ただし単層のパーセプトロンではXOR回路が生成出来ないことが指摘され、多層パーセプトロンが開発されたものの、そのままでは線形分離不可能なパターンを識別できないことが指摘さた。さらに、重みパラメータの設定方法が不明であったことも大きな問題点であった。このためその後10年にわたってこのパーセプトロン理論は日の目を見ることができなかった。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL-874h9wjIz",
        "colab_type": "text"
      },
      "source": [
        "# ニューラルネットワーク\n",
        "パーセプトロンのコンセプトを元に、これに改良を加えたものがニューラルネットワークである。\n",
        "* 活性化関数の導入\n",
        "* ３層構造の導入（出力層の工夫）\n",
        "* 機械学習(ディープラーニング）の導入\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmx0g2eG6NXt",
        "colab_type": "text"
      },
      "source": [
        "## 活性化関数の導入\n",
        "パーセプトロンはもともと二値演算を生物学的に実現しようという発想であったため、出力値は当然２値である。これをデジタル（離散値）ではなくアナログ（連続値）にもちこんだのが、このニューラルネットワークの優れた着想点である。\n",
        "\n",
        "ニューラルネットワークでは、これを活性化関数として再定義している。\n",
        "\n",
        "\n",
        "主な活性化関数を下記に示す\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQzVnI_66Oz1",
        "colab_type": "text"
      },
      "source": [
        "### ステップ関数\n",
        "パーセプトロンで実装されている方式。\n",
        "\n",
        "評価式の結果がゼロ以下かそれ以外で０、１の二値を返す\n",
        "\n",
        "ステップ関数をグラフに表示する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzO1oCsJ1ykU",
        "colab_type": "code",
        "outputId": "784b906b-95d6-4569-fb51-86479d40caa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "#------------------------\n",
        "# ステップ関数\n",
        "#------------------------\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "#------------------------\n",
        "# グラフへの表示\n",
        "#------------------------\n",
        "X = np.arange(-5.0, 5.0, 0.1)\n",
        "Y = step_function(X)\n",
        "plt.plot(X, Y)\n",
        "plt.ylim(-0.1, 1.1)  # 図で描画するy軸の範囲を指定\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3AGghQh3AGghwh0AWohwB4AW+n9SUyHBkOuTZAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5PqkPWW1nz7",
        "colab_type": "text"
      },
      "source": [
        "### シグモイド関数\n",
        "初期のニューラルネットワークで広く用いられた関数。\n",
        "\n",
        "（城田説）ステップ関数に近い連続的で微分しやすい関数を使ったと思ってる  \n",
        "\n",
        "\n",
        "$h(x)=\\frac{1}{1+e^{-x}}$ \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "##### （参考）ネイピア数 $e$ について\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![代替テキスト](https://mathwords.net/wp-content/uploads/2016/10/enoxnogurahu1-300x260.png)\n",
        "\n",
        "**特徴**\n",
        "* $y=e^x$ のグラフの概形は上図の通り。\n",
        "* $x=0$の時、$e^x=1$\n",
        "* $\\frac{d}{dx}e^x = e^x$ 微分しても同じ形\n",
        "* $y=e^x$はプログラム上では  y=exp(x)と記述される。\n",
        "* expはエクスポーネンシャルの略\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "グラフに表示する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0yf1TAL2g0P",
        "colab_type": "code",
        "outputId": "d09fbb8d-62b5-41e3-de34-85da2edab645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "#------------------------\n",
        "# シグモイド関数\n",
        "#------------------------\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "#------------------------\n",
        "# グラフへの表示\n",
        "#------------------------\n",
        "X = np.arange(-5.0, 5.0, 0.1)\n",
        "Y = sigmoid(X)\n",
        "plt.plot(X, Y)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biLbM0kG1qqO",
        "colab_type": "text"
      },
      "source": [
        "上記のようにシグモイド関数では、結果は２値ではなく０から１までの実数が返却される。\n",
        "\n",
        "これは出力値を２つに限定したパーセプトロンからの大きな飛躍であり、これにより複雑な（曖昧な）入力と出力が可能となった。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLC6sU0y4ARZ",
        "colab_type": "text"
      },
      "source": [
        "### ReLU関数　（別名：ランプ関数　　ramp func 傾斜関数）\n",
        "最近のAIではシグモイド関数の代わりに用いられる事が多い。\n",
        "\n",
        "ReLUは入力値がゼロを超えていればそのまま出力される。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEAtGJId5cji",
        "colab_type": "code",
        "outputId": "2c3f6598-8082-4ffc-efd6-9365d6e8baa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "#------------------------\n",
        "# ReLU関数\n",
        "#------------------------\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "#------------------------\n",
        "# グラフへの表示\n",
        "#------------------------\n",
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "y = relu(x)\n",
        "plt.plot(x, y)\n",
        "plt.ylim(-1.0, 5.5)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8VyVbpH6pvs",
        "colab_type": "text"
      },
      "source": [
        "## 3層構造の導入\n",
        "ニューラルネットワークでは、全体のネットワーク構造（ニューロンの繋がり）を入力層、中間層、出力層の３種類で定義している。\n",
        "\n",
        "下記に３層構造のニューラルネットワークを図示する（入力層が０番目）。\n",
        "\n",
        "![ニューラルネットワーク図](https://docs.google.com/drawings/d/e/2PACX-1vTz9IG4wquQ4WPAyi6vaxoMVAnO_g1E7lA3We7u1x84faTg_pUQ3KWwEuyP-NDn9YQnPu694SUXM12t/pub?w=960&h=720)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFWbHevF_Gl-",
        "colab_type": "text"
      },
      "source": [
        "### 入力層\n",
        "ニューラルネットワークの入力となる層。\n",
        "\n",
        "各ニューロンへの値は一つなので、画像を認識しようとすれば、各画素毎の値が入力値となる。\n",
        "\n",
        "具体的には縦横１６ドットの画像であれば、１６×１６＝２５６の画素で構成されるため、入力層のニューロンは２５６個必要となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GTLGjAsABCf",
        "colab_type": "text"
      },
      "source": [
        "### 隠れ層\n",
        "この層がAIにおけるモデルの中心部分であり日々進化を遂げている。\n",
        "\n",
        "過去には計算機のパワー不足により隠れ層が１つでも多大な負荷であったが、現在ではこの隠れ層を数百に及んで実装することも可能となっている。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgO6jmr__wS0",
        "colab_type": "text"
      },
      "source": [
        "### 出力層\n",
        "ニューラルネットワークの出力となる層。この層の設計は説かれる問題によってニューロンの個数と活性化関数が決まってくる（このため、先程の図では出力層での活性化関数がh()ではなくσ()と記述している）。\n",
        "　　\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>問題の種類</th>\n",
        "    <th>問題の例</th>\n",
        "    <th>活性化関数</th>\n",
        "    <th>出力層の数</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>回帰問題</td>\n",
        "    <td>年齢推定など特定の値を求める</td>\n",
        "    <td>恒等関数</td>\n",
        "    <td>1個</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>２値分類</td>\n",
        "    <td>合格・不合格など２種類に分類する</td>\n",
        "    <td>シグモイド関数</td>\n",
        "    <td>2個</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>多クラス問題</td>\n",
        "    <td>文字認識など複数のクラスに分解する問題</td>\n",
        "    <td>ソフトマックス関数</td>\n",
        "    <td>n個</td>\n",
        "  </tr>\n",
        "</table>\n",
        "</br>\n",
        "\n",
        "\n",
        "今回の手書き数字認識であれば、結果は０から９のいずれかになるので、出力層は１０となり出力層での活性化関数はソフトマックス関数を用いる。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2hRLitDC5hY",
        "colab_type": "text"
      },
      "source": [
        "#### ソフトマックス関数\n",
        "![ソフトマックス関数](https://docs.google.com/drawings/d/e/2PACX-1vSzbhHIZVMWV4YAJgndvhJuHOmPNSNBJYlKqpLh1d0Ffl6Lfbr9HcQcczLk-oWQgYqcno_Mn6TuFAaM/pub?w=185&h=100)\n",
        "\n",
        "この関数は出力値を全体の合計値で割ることにより、必ず０～１の間における割合を示す。これは擬似的に出力値に対する期待度、信頼度を示しているとも言える。\n",
        "\n",
        "なお、expをしていることにより、出力値が高い場合はより大きい値をとることで、微細な違いもはっきりと表示する効果を持っている。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npN6fdBEGp20",
        "colab_type": "text"
      },
      "source": [
        "## 手書き数字（エムニスト）認識の実装\n",
        "それでは、実際にニューラルネットワークを使って手書き文字認識を実装してみる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsgiudRhHFSm",
        "colab_type": "text"
      },
      "source": [
        "### エムニストデータセット\n",
        "oreillyのデータを使ってMNISTデータの認識を行う\n",
        "\n",
        "![MNIST](https://docs.google.com/drawings/d/e/2PACX-1vRmKtGjrW_McthMyjLw9dj_zXxKaZ-Gca_HeNPKumeS7EIZ72ndBVYITGC0VLaRQOcayx97xOt_f40n/pub?w=289&h=173)\n",
        "\n",
        "Mixed Natioal Institute of Standards and Technology database. MNIST [エムニスト] [em-nist]\n",
        "\n",
        "\n",
        "![サンプル](https://docs.google.com/drawings/d/e/2PACX-1vQkslZu9xyboXsva9jgmQDVrynTZZeYnI6yMamdc012Y3z5kTrA68ePcAyBsagxSfyawIsTHhFE-b7j/pub?w=290&h=318)\n",
        "\n",
        "* サイズは２８×２８ドット\n",
        "* 白黒２５６階調（０白：２５５：黒）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I7etWIjtj4R",
        "colab_type": "text"
      },
      "source": [
        "### ニューラルネットワークの設計\n",
        "ここでは隠れ層を２つもつニューラルネットワークを構築する\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vQPJZS9d6upJkCh2yHAc7h5620ZZCjPvGNzHuOTyrXx1kYv8HCC5ORtbB-6FQC-E4oir0peQFe-Q5k9/pub?w=1291&h=743)\n",
        "\n",
        "* 入力層は２８×２８＝７８４個のニューロン\n",
        "* 隠れ層１は５０個のニューロン\n",
        "* 隠れ層２は１００個のニューロン\n",
        "* 出力層は（正解が0～9なので）１０個のニューロン\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJCqoMuMQgOG",
        "colab_type": "text"
      },
      "source": [
        "## 文字認識処理のみの実装\n",
        "上図において機械学習の処理を含まない文字認識処理の部分について実装を行う（損失関数：交差エントロピーについては後で述べる）。  \n",
        "  ここでは、事前に機械学習により導出された重みとバイアスパラメータを定義済みモデルとしてダウンロードして用いる。  \n",
        "　"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-JgC20YPdz5",
        "colab_type": "code",
        "outputId": "d069e6cb-0c48-4826-f1d3-196ccaeff454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "%cd /content/deep-learning-from-scratch/ch03\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import pickle\n",
        "from dataset.mnist import load_mnist\n",
        "from common.functions import sigmoid, softmax\n",
        "\n",
        "#-----------------------------\n",
        "# データの取得\n",
        "#-----------------------------\n",
        "def get_data():\n",
        "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
        "    return x_test, t_test\n",
        "\n",
        "\n",
        "#-----------------------------\n",
        "# 定義済みのニューラルネットワークの読み込み\n",
        "#-----------------------------\n",
        "def init_network():\n",
        "    with open(\"sample_weight.pkl\", 'rb') as f:\n",
        "        network = pickle.load(f)\n",
        "    return network\n",
        "\n",
        "\n",
        "#-----------------------------\n",
        "# 推論処理\n",
        "#-----------------------------\n",
        "def predict(network, x):\n",
        "    # 重み配列の読み込み\n",
        "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
        "    # バイアスの読み込み\n",
        "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
        "\n",
        "    # 入力層　-> 隠れ層1　の処理\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "\n",
        "    # 隠れ層1　-> 隠れ層2　の処理\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    z2 = sigmoid(a2)\n",
        "\n",
        "    # 隠れ層2　-> 出力層　の処理\n",
        "    a3 = np.dot(z2, W3) + b3\n",
        "    y = softmax(a3)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "#-----------------------------\n",
        "# メイン処理\n",
        "#-----------------------------\n",
        "x, t = get_data()             # 入力データx と正解データt を読み込む\n",
        "network = init_network()      # 訓練済みの重み、バイアスデータを読み込む\n",
        "accuracy_cnt = 0\n",
        "\n",
        "for i in range(len(x)):\n",
        "    # 一枚ごとに推論処理を行う\n",
        "    y = predict(network, x[i])\n",
        "    p= np.argmax(y)           # 最も確率の高い要素のインデックスを取得\n",
        "    if p == t[i]:\n",
        "        # 正解の場合\n",
        "        accuracy_cnt += 1\n",
        "\n",
        "#-----------------------------\n",
        "# 結果表示\n",
        "#-----------------------------\n",
        "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch03\n",
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "Accuracy:0.9352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em0jNpA-XQxM",
        "colab_type": "text"
      },
      "source": [
        "### バッチ処理の実装\n",
        "上記の例は６万枚の画像データについて一枚づつ処理を行っている。\n",
        "\n",
        "numpyなどの科学計算ライブラリは行列計算が高速に行えるようにチューニングされている。このため一般的にはロジックでループ処理を行うより、行列式として一度に計算する量を増やしてやるほうが結果として処理が早くなる。\n",
        "\n",
        "今回の例では一つの画像データ（画素数分の列データ）を複数画像分まとめて（行列データにして）渡すことで、計算が早くなる。\n",
        "\n",
        "このまとめる指定を「バッチ（束）」と呼ぶ。\n",
        "\n",
        "バッチ数を指定した場合のプログラムを下記に示す（結果は変わらない）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXwJLKQPXNKQ",
        "colab_type": "code",
        "outputId": "5efd2029-08ba-4833-f9bb-c4ec00a6c571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "x, t = get_data()\n",
        "network = init_network()\n",
        "\n",
        "batch_size = 100 # バッチの数\n",
        "accuracy_cnt = 0\n",
        "\n",
        "for i in range(0, len(x), batch_size):\n",
        "    x_batch = x[i:i+batch_size]\n",
        "    y_batch = predict(network, x_batch)\n",
        "    p = np.argmax(y_batch, axis=1)\n",
        "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
        "\n",
        "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:0.9352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-zpOiF0bBxT",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning\n",
        "ディープラーニングとは４層以上のニューラルネットワークにおける各パラメータ（重み、バイアス）を決定する作業である。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-sUaSzaX5Ej",
        "colab_type": "text"
      },
      "source": [
        "## Learning概要\n",
        "それでは、どのように重みやバイアスパラメータを決めてゆくのだろうか？　下記にそのポイントを説明する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPoM_jYM9060",
        "colab_type": "text"
      },
      "source": [
        "## 対象データの抽出-ミニバッチ\n",
        "ミニバッチとは全量データを使わずに一部のデータをランダムに抜き出すことで処理の高速化を図るテクニックである。これはTV局の視聴率調査と同様で母集団が一定のサイズであれば、一部の少量データでも統計的振る舞いが同様となる特性を利用している。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEl8IIEkdVl9",
        "colab_type": "text"
      },
      "source": [
        "## 損失関数の設定\n",
        "重みパラメータなどを推定する際の基準となりうのが、この損失関数である。この損失関数の値をゼロに近づける行為がディープラーニングである。  \n",
        "\n",
        "ソフトマックスにより計算された値がどの程度正解からズレているか（誤差）を計算する。この関数を損失関数と呼ぶ。\n",
        "\n",
        "損失関数は誤差を数値化することである。数学的に最も有名な誤差計測の方法は２乗和誤差と呼ばれるものであるが、AIの世界では交差エントロピー誤差という手法が利用される。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1y-z9kQAPRa",
        "colab_type": "text"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### 交差エントロピー誤差\n",
        "![交差エントロピー誤差](https://docs.google.com/drawings/d/e/2PACX-1vRkLbMfJmj-vUDumOXr1Lxk32H9Hu7CNryVJWggm48BZmN8pMkEfX94XdkzyaAiggbWiqrBRTdteCBv/pub?w=150&h=100)\n",
        "\n",
        "　先のmnistデータでは、各画像に対する正解データ（教師データ）は０から９までの配列で表されていた。  \n",
        "　一方、ニューラルネットワークを使って計算された出力結果はソフトマックス関数により０から１までの値として表現される。  \n",
        "　この例で交差エントロピー誤差を計算すると、教師データは正解以外はゼロであるため、結局の所　E=log(0.8)を計算するだけでよい。  \n",
        "\n",
        " \n",
        "<table>\n",
        "  <tr>\n",
        "    <th>ラベル</th>\n",
        "    <th>0</th>\n",
        "    <th>1</th>\n",
        "    <th>2</th>\n",
        "    <th>3</th>\n",
        "    <th>4</th>\n",
        "    <th>5</th>\n",
        "    <th>6</th>\n",
        "    <th>7</th>\n",
        "    <th>8</th>\n",
        "    <th>9</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>正解データ</th>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>SoftMax出力値</th>\n",
        "    <td>0</td>\n",
        "    <td>0.1</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0.8</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0.1</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>交差エントロピー誤差</th>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0.22</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "※このような正解データの表記方法をOne Hot表現と呼ぶ\n",
        "\n",
        "ちなみに、y = log(x)のグラフは"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfa-921_knha",
        "colab_type": "code",
        "outputId": "a4f9921c-7fed-4b05-baa0-d7f57db4fc1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "#------------------------\n",
        "# グラフ表示\n",
        "#------------------------\n",
        "x = np.arange(0.0001, 1.0, 0.01)\n",
        "y = np.log(x)\n",
        "plt.plot(x, y)\n",
        "plt.ylim(-5.0, 0.0)\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQW5iIjLKchFRFxOQS4i4nL/H9GNwUjJxw+EAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcNS1DahlVey",
        "colab_type": "text"
      },
      "source": [
        "となり、Xが１のときにYは０となる。なお、Xが０となるとlog(x)は-∞となるため、プログラミングする際はXがゼロとならないように極小のバイアス値を付加して算出する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUPCEsdrEXHV",
        "colab_type": "text"
      },
      "source": [
        "## パラメーターの再計算\n",
        "先に導出された損失関数を指標に各パラメーター（重みとバイアス）を決定してゆく。\n",
        "\n",
        "この値の決定方法として勾配降下法と呼ばれる手法が用いられる。\n",
        "\n",
        "### **勾配降下法**\n",
        "\n",
        "　まず一つのニューロンの重みWについて考えてみる。ニューロンの重みWが変化すれば損失関数の値Lossが変動する。仮にその変動が下図のグラフのようになった場合、最もLossが少ないWの値が求めたい目標の値である。  \n",
        "　今初期値Pからスタートしてゴールとなる目標地点にWを変化させたい場合、どのような操作をすればよいだろうか？　視覚的に記述するなら「傾きを下る方向にWを移動する」となる。これを数学的に記述すると、\n",
        "* 点Pでのグラフの傾きを求め、その値の定数倍をマイナスする\n",
        "\n",
        "これを繰り返して目標の値を求める方式を勾配降下法と呼ぶ。\n",
        "\n",
        "\n",
        "![勾配降下法](https://docs.google.com/drawings/d/e/2PACX-1vRN5EFodaLfgY0qFOPS9jQLNnJFa-e5JnrwZBAEAVqAtdxPtdnFIhW3xN0176LokS0BbOm3IsdLmCVn/pub?w=791&h=386)\n",
        "\n",
        "　"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yBy0Hh5v66J",
        "colab_type": "text"
      },
      "source": [
        "### 勾配降下法の例題\n",
        "実際に数値微分を使って勾配降下法の動きを見てみる。\n",
        "\n",
        "例として$f(x_1,x_2) = x_1^2 + x_2^2$ としたとき、関数$f$が最小の値となる$x_1,x_2$を求める。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLZnPuQPvcf5",
        "colab_type": "code",
        "outputId": "1cabfcab-93f7-4f79-b27a-6df644f4e697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def _numerical_gradient_no_batch(f, x):\n",
        "    h = 1e-4  # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)  # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x)  # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val  # 値を元に戻す\n",
        "        \n",
        "    return grad\n",
        "\n",
        "def numerical_gradient(f, X):\n",
        "    if X.ndim == 1:   #次元数が１の場合\n",
        "        return _numerical_gradient_no_batch(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x = init_x\n",
        "    x_history = []\n",
        "\n",
        "    for i in range(step_num):\n",
        "        x_history.append( x.copy() )\n",
        "\n",
        "        grad = numerical_gradient(f, x)   # 数値微分で傾きを求める\n",
        "        x -= lr * grad                    # 傾き×学習率を重みから引く\n",
        "\n",
        "    return x, np.array(x_history)\n",
        "\n",
        "\n",
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])    \n",
        "\n",
        "lr = 0.1               # Learning Rate 学習率\n",
        "step_num = 20\n",
        "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
        "\n",
        "plt.plot( [-5, 5], [0,0], '--b')\n",
        "plt.plot( [0,0], [-5, 5], '--b')\n",
        "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
        "\n",
        "plt.xlim(-3.5, 3.5)\n",
        "plt.ylim(-4.5, 4.5)\n",
        "plt.xlabel(\"X0\")\n",
        "plt.ylabel(\"X1\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpyIxXvhx6RC",
        "colab_type": "text"
      },
      "source": [
        "### 学習率(Learning Rate)η\n",
        "ここで重要なパラメータとして「学習率η（イータ）」がある。この学習率は傾きに乗じる定数であるが、これが大きすぎても小さすぎても正しい値を導くことができない。\n",
        "\n",
        "* lr=0.1であれば収束する\n",
        "* lr=0.01では収束しない\n",
        "* 逆にlr=0.8であると発散してしまう\n",
        "\n",
        "なお、この学習率は経験的に発見するしかない。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJHpaRxXy6SL",
        "colab_type": "text"
      },
      "source": [
        "## 数値微分によるLearningの実装\n",
        "これまでの説明でPDCAを見てきた。ここで実際にプログラムとしてニューラルネットワークの学習を実装する。\n",
        "\n",
        "なお、次に示すプログラムでは微分操作により傾きを求めるのではなく、点Pの前後のウェイト値を使って再度損失関数の値を求め、その差を使って傾きを求める方法「**数値微分**」で計算する。\n",
        "\n",
        "大まかなプログラムの流れとしては、\n",
        "* ミニバッチの取得\n",
        "* 数値微分（＋Δ、－Δとして2回の全量計算をする）\n",
        "* 上記を繰り返し、１エポックに達したらテストデータで精度を検証する\n",
        "\n",
        "---\n",
        "### 回数に関する用語\n",
        "\n",
        "学習の繰り返し回数を　**イテレーション数**と呼ぶ。\n",
        "\n",
        "\n",
        "対象としている訓練データの総数を１エポックという単位で表す。これは各ミニバッチでは、データがランダムに選ばれるため、便宜的に全量データについて学習したとみなすためである。\n",
        "\n",
        "それでは、なぜランダムに選ばずに順に全量について処理をしないのかというと、のちに述べる確率的勾配降下法を採用しているためである。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWfKRhNW-pab",
        "colab_type": "code",
        "outputId": "b4486530-1bd7-4653-eb26-7070b3448817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "%cd /content/deep-learning-from-scratch/ch04\n",
        "\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000  # 繰り返しの回数を適宜設定する\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # ミニバッチの取得\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 勾配の計算\n",
        "    grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    #grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # パラメータの更新\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    # 1エポック毎にテストを使って認識制度を計測する\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "# グラフの描画\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch04\n",
            "train acc, test acc | 0.09928333333333333, 0.1032\n",
            "train acc, test acc | 0.7875166666666666, 0.7908\n",
            "train acc, test acc | 0.8794166666666666, 0.8846\n",
            "train acc, test acc | 0.89785, 0.9004\n",
            "train acc, test acc | 0.9080666666666667, 0.9104\n",
            "train acc, test acc | 0.9146166666666666, 0.9148\n",
            "train acc, test acc | 0.9204666666666667, 0.921\n",
            "train acc, test acc | 0.9244, 0.925\n",
            "train acc, test acc | 0.92745, 0.9295\n",
            "train acc, test acc | 0.9317, 0.9319\n",
            "train acc, test acc | 0.9346666666666666, 0.9338\n",
            "train acc, test acc | 0.9380166666666667, 0.9362\n",
            "train acc, test acc | 0.9402666666666667, 0.9393\n",
            "train acc, test acc | 0.9433833333333334, 0.9416\n",
            "train acc, test acc | 0.9446333333333333, 0.9435\n",
            "train acc, test acc | 0.9467833333333333, 0.944\n",
            "train acc, test acc | 0.9482666666666667, 0.946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiI5ToVARCTHqRCIiOQ4FQIRkRz3/wFyHlxbPDTgBQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01_h0GkeKNBm",
        "colab_type": "text"
      },
      "source": [
        "# 誤差逆伝播法（バックプロパゲーション）\n",
        "　先程の機械学習では数値微分という方法で重みを計算した。この方法では一つの重みパラメータを求めるために２回の全量計算をおこなっており、これをすべてのニューロンについて計算し、やっと一回目のイテレーションが完了する。このように数値微分ではCPU処理量が多いため多層ネットワークの構築には不向きである。このため、計算が高速となる誤差逆伝播法（バックプロパゲーション）が開発された。  \n",
        "　多層に渡った活性化関数を微分するためには、連鎖率を利用して損失関数の値から逆向きに偏微分してゆくことで高速に計算できる。ここではそれを計算グラフにて説明する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI8kuqguNcIH",
        "colab_type": "text"
      },
      "source": [
        "## 計算グラフ\n",
        "　計算グラフという記法を使い、順方向の計算を確認する\n",
        "* 100円のリンゴ2個\n",
        "* 150円のみかん3個\n",
        "* 消費税は10%\n",
        "* 合計金額は？\n",
        "\n",
        "![計算グラフ](https://docs.google.com/drawings/d/e/2PACX-1vTwwOt7w-NMp-xaUtPVC7ItTlS5IWqvm0IGxWJAQuWD1WSDNrJdowzxfQ9ED53x05uX26WEmkQjt6BL/pub?w=813&h=521)\n",
        "\n",
        "　ここで、リンゴの値段の変化が合計金額に及ぼす影響を考えてみる。\n",
        "* リンゴが１円値上がりすると、合計金額は2.2円分増加する。\n",
        "* これはリンゴの個数が２であり、消費税が10％であるからである\n",
        "* この2.2という数値は、「リンゴの値段に対する合計金額の微分」である。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNWEqQ-oONRV",
        "colab_type": "text"
      },
      "source": [
        "### 連鎖律\n",
        "(wikipedia)  \n",
        "微分法において連鎖律（れんさりつ、英: chain rule）とは、複数の関数が合成された合成関数を微分するとき、その導関数がそれぞれの導関数の積で与えられるという関係式のこと\n",
        "\n",
        "<例>  \n",
        "　　$z = (x + y)^2$  について $\\frac{∂z}{∂x}$を求める\n",
        "---\n",
        "一般的な解き方\n",
        "\n",
        "$ \\frac{∂}{∂x}z = \\frac{∂}{∂x}(x + y)^2$  \n",
        "---\n",
        "\n",
        "　 　$ =  \\frac{∂}{∂x}( x^2 + 2xy + y^2)$  \n",
        "---\n",
        "\n",
        "　 　$ = 2x+2y$\n",
        "---\n",
        "---\n",
        "合成関数を使う解き方\n",
        "\n",
        "　　$t = x + y$ と置くと\n",
        "---\n",
        "　　$z = t^2$  \n",
        "---\n",
        "　　$t = x + y$\n",
        "---\n",
        "　　$\\frac{∂z}{∂x} = \\frac{∂z}{∂t}・\\frac{∂t}{∂x}$　　←これが連鎖律\n",
        "---\n",
        "　　　　$ = \\frac{∂}{∂t}t^2・\\frac{∂}{∂x}(x+y)$\n",
        "---\n",
        "　　　　$ = 2t・1$\n",
        "---\n",
        "　　　　$ = 2(x+y)$\n",
        "---\n",
        "---\n",
        "計算グラフを使った解き方\n",
        "\n",
        "\n",
        "![計算グラフ](https://docs.google.com/drawings/d/e/2PACX-1vTxzLeXXa9_xwUwgrG-2T-PYvAEc5_00MkhtXSRoJ5iS21MvaxgEsbmBf_rDTuidzni87d5iUibKlHB/pub?w=946&h=440)\n",
        "\n",
        "\n",
        "図に示すように、連鎖律に従って計算グラフの最後から最初に向かって順に計算することで、最終的な微分にたどり着くことができる。\n",
        "\n",
        "---\n",
        "改めてリンゴとミカンに戻ると\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vQRRJ8owch1DvuUawwqgJ_hLX61YB5Lyt4exjIwP6bNnlPawfjN12X2ngO3l0N_9RJ7OKtTBlJ4YBuF/pub?w=809&h=521)\n",
        "\n",
        "\n",
        "　まず、合計金額を１と置くと、消費税がこれにかかってくるので、1.1倍した1.1が一つ前のノードに渡される。次の足し算のノードでは値に変化がおこらず、もう一つ後ろのノードに1.1が渡され、そこに個数の２が掛けられて答えとなる2.2が導出される。この2.2はリンゴの値段に関する支払い金額の微分値にほかならない。  \n",
        "　同様にリンゴの個数に対する支払金額の微分値を求める場合は、直前の1.1までは同様で、その値にリンゴの値段となる100を掛けた110が求める値となる。\n",
        "\n",
        "　\n",
        "　このように、乗算ノードでは互いの数値を互い違いに掛け算し、加算ノードでは前段の値をそのまま使いながら逆の方向に計算を行うと、最終的に微分値が決定される。これが**逆伝播法（バックプロパゲーション）**である。ポイントは以下の通り。\n",
        "\n",
        "* 偏微分値は出力側から逆順に計算してゆく\n",
        "* 掛け算ノードは値を交差させる\n",
        "* 足し算ノードは値をそのまま渡す\n",
        "* 各ノードでは渡された値に微分値を掛ける\n",
        "\n",
        "各ニューロンの要素をそれぞれ順方向のプログラム（forward)と、逆方向のプログラム(backword)の２つで構成されるレイヤとして実装を行う。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhEqjSNybWR-",
        "colab_type": "text"
      },
      "source": [
        "## ReLUレイヤの実装\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vSvgNlyRTv8kiaVrxEk4MNumcGohtSS7DDfTkPpTAAUT-I0Ecjhi62dDU8RWQRIlz7eL4XBW2HoAP2g/pub?w=958&h=247)\n",
        "\n",
        "活性化関数ReLUの場合は入力値が０より大きい場合は、その値がそのまま渡される。このため、グラフの傾きすなわち微分値は１であり、逆伝播においては渡された値に１を掛けた値としてそのまま逆伝播させる。\n",
        "\n",
        "\n",
        "pythonでの実装としては、下記のようになる。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbOGTwl7neOP",
        "colab_type": "code",
        "outputId": "d30ea5a6-1bd1-4be8-daed-b8480947ce67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "from common.functions import *\n",
        "from common.util import im2col, col2im\n",
        "\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0  \n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "# ReLUレイヤーのコーディングサンプル\n",
        "x = np.array([[1.0,-1.2],[-2.2,1.3]])\n",
        "print(\"----------------\")\n",
        "print(\"もともとの入力値\")\n",
        "print(x)\n",
        "print(\"----------------\")\n",
        "print(\"mask=(x<=0)   マスク値  \")\n",
        "mask=(x<=0)\n",
        "print(mask)\n",
        "print(\"----------------\")\n",
        "print(\"x[mask]=0      Trueの部分をゼロで埋める\")\n",
        "x[mask]=0\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------\n",
            "もともとの入力値\n",
            "[[ 1.  -1.2]\n",
            " [-2.2  1.3]]\n",
            "----------------\n",
            "mask=(x<=0)   マスク値  \n",
            "[[False  True]\n",
            " [ True False]]\n",
            "----------------\n",
            "x[mask]=0      Trueの部分をゼロで埋める\n",
            "[[1.  0. ]\n",
            " [0.  1.3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjv9Jjb7sjJe",
        "colab_type": "text"
      },
      "source": [
        "## シグモイドレイヤの実装\n",
        "\n",
        "今回のMNSITデータ分析では利用しないが、シグモイド関数の逆伝播について記述する\n",
        "\n",
        "\n",
        "\n",
        "![シグモイド関数](https://docs.google.com/drawings/d/e/2PACX-1vSPiq7ffbs0PiyFDXrg93J1t3KdMq0eAnCoVC_JfD62LrszzR9t_4c3U76pF_SvNhDCxeKplvJrBIL_/pub?w=182&h=100)\n",
        "\n",
        "シグモイド関数の微分を解く\n",
        "$y = \\frac{1}{1 + e^{-x}}$\n",
        "---\n",
        "$t=1 + e^{-x}$と置くと  \n",
        "---\n",
        "<h1>\n",
        "&nbsp;&nbsp;$\\frac{∂y}{∂x} = \\frac{∂y}{∂t} \\frac{∂t}{∂x} $ </br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ = \\frac{∂}{∂t}\\frac{1}{t}・ \\frac{∂}{∂x}(1+e^{-x}) $</br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ = \\frac{∂}{∂t}t^{-1}・ \\frac{∂}{∂x}(1+e^{-x}) $ </br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ = -t^{-2}・{- e^{-x} }$ </br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ = -(1 + e^{-x})^{-2}・{- e^{-x} }$ </br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ = \\frac{1}{(1 + e^{-x})^2}・ e^{-x} $ 　　　　・・・①</br>\n",
        "\n",
        "</br>\n",
        "一方$e^{-x}$を$y$で表すと</br>\n",
        "\n",
        "$y = \\frac{1}{1 + e^{-x}}$\n",
        "---\n",
        "$y・(1 + e^{-x}) = 1$\n",
        "---\n",
        "$1 + e^{-x} = \\frac{1}{y}$\n",
        "---\n",
        "$e^{-x} = \\frac{1}{y}-1$　　　・・・②\n",
        "---\n",
        "\n",
        "①、②より\n",
        "\n",
        "&nbsp;&nbsp;$\\frac{∂y}{∂x} =  \\frac{1}{(1 + e^{-x})^2}・ e^{-x} $ </br>\n",
        "---\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ = y^2・ (\\frac{1}{y}-1) $ </br>\n",
        "---\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ = y-y^2 $ </br>\n",
        "---\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ = y(1-y) $ </br>\n",
        "---\n",
        "\n",
        "</h1>\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vQbSo7K6xNPYZ6uZsEVr25lCTd-IyL_CvHxUBz_qnTGHR6_9ccFON4N9tWrGwZwPsJJJwKhqVtOu9yE/pub?w=476&h=224)\n",
        "\n",
        "\n",
        "実装としては下記の通り\n",
        "```\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcXyEuBJOvZg",
        "colab_type": "text"
      },
      "source": [
        "## Affineレイヤの実装\n",
        "\n",
        "各ニューロンは複数からの信号と重み付けを受け取りそれらの総和を活性化関数に引き渡している。\n",
        "\n",
        "![パーセプトロン](https://docs.google.com/drawings/d/e/2PACX-1vSpMVD85wOprci70XqNYFpaA3gNI_rTRVezumjEJ6trAHQ6qMq7gPA-PH7bDzHfOkMb-pEKQXxSOvs_/pub?w=302&h=220)\n",
        "\n",
        "先程はこの活性化関数を一つのレイヤとして実装した。Affineレイヤはその前段として、各ノードからの値と重みを掛け合わせ、バイアスを付加する部分を受け持つ。\n",
        "\n",
        "\n",
        "* affineとは幾何学における写像を意味する。\n",
        "\n",
        "```\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W =W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        return dx\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6CydMIYl5i",
        "colab_type": "text"
      },
      "source": [
        "## Softmax with Lossレイヤの実装\n",
        "ニューラルネットワークの最終段階としてソフトマックス関数による確信度の計算とそれを評価する損失関数のレイヤを同時に定義する。なお、ここでは損失関数を交差エントロピーとしている。\n",
        "\n",
        "\n",
        "```\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None # softmaxの出力\n",
        "        self.t = None # 教師データ\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8t2eRU2cpbB",
        "colab_type": "text"
      },
      "source": [
        "## 全体実装\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vRz5JqHaZeJ2QZy31JlNOiROYOsYd5q1nZ29G79UH3MEM0cAN9zn_v717IeMl0_rUXsw82679O8aEmk/pub?w=599&h=413)\n",
        "\n",
        "隠れ層を５０とした２層のニューラルネットワークを実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1DwAqg0gg8j",
        "colab_type": "code",
        "outputId": "bb68526f-9c1b-4723-85c9-bb02b0a78e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/deep-learning-from-scratch/ch04\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 重みの初期化\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    # x:入力データ, t:教師データ\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x:入力データ, t:教師データ\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeqOtvwzgjW_",
        "colab_type": "code",
        "outputId": "64bb85f0-b28f-42f7-923a-9a9878ae5f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 勾配\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 更新\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.10218333333333333 0.101\n",
            "0.7830833333333334 0.788\n",
            "0.8773833333333333 0.8839\n",
            "0.8996833333333333 0.9024\n",
            "0.9082 0.9103\n",
            "0.9147333333333333 0.9176\n",
            "0.9202833333333333 0.9232\n",
            "0.9246333333333333 0.9261\n",
            "0.9279833333333334 0.9303\n",
            "0.9317333333333333 0.9328\n",
            "0.9340166666666667 0.9342\n",
            "0.9372166666666667 0.938\n",
            "0.9396166666666667 0.9405\n",
            "0.9422833333333334 0.9412\n",
            "0.9436833333333333 0.9427\n",
            "0.9447166666666666 0.9451\n",
            "0.9471166666666667 0.9449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH_Hoc7tz6iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tesnsorflow アップグレードツール\n",
        "!tf_upgrade_v2 --infile /content/TensorFlowDL-samples/nn/mnist.py --outfile /content/TensorFlowDL-samples/nn/mnist2.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou4kg_4hyHD6",
        "colab_type": "code",
        "outputId": "86d65149-f5c4-48cd-8b6b-757605051d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        }
      },
      "source": [
        "#---------------------------------------------------------------------------\n",
        "#   TensorFlowを使ったサンプル\n",
        "#---------------------------------------------------------------------------\n",
        "\n",
        "!git clone https://github.com/thinkitcojp/TensorFlowDL-samples.git\n",
        "\n",
        "#-*- coding:utf-8 -*-\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "\n",
        "#mnistデータを格納したオブジェクトを呼び出す\n",
        "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
        "\n",
        "#入力データを定義\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "#入力層から中間層\n",
        "w_1 = tf.Variable(tf.random.truncated_normal([784, 50], stddev=0.1), name=\"w1\")\n",
        "b_1 = tf.Variable(tf.zeros([50]), name=\"b1\")\n",
        "h_1 = tf.nn.relu(tf.matmul(x, w_1) + b_1)\n",
        "\n",
        "#中間層から出力層\n",
        "w_2 = tf.Variable(tf.random.truncated_normal([50, 10], stddev=0.1), name=\"w2\")\n",
        "b_2 = tf.Variable(tf.zeros([10]), name=\"b2\")\n",
        "out = tf.nn.softmax(tf.matmul(h_1, w_2) + b_2)\n",
        "\n",
        "#誤差関数\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, 10])\n",
        "#loss = tf.reduce_mean(input_tensor=tf.square(y - out))\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(out + 1e-5),axis=[1]))\n",
        "\n",
        "#訓練\n",
        "train_step = tf.compat.v1.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "\n",
        "#評価\n",
        "correct = tf.equal(tf.argmax(input=out,axis=1), tf.argmax(input=y,axis=1))\n",
        "accuracy = tf.reduce_mean(input_tensor=tf.cast(correct, tf.float32))\n",
        "\n",
        "#初期化\n",
        "init =tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        " \n",
        "    sess.run(init)    \n",
        "\n",
        "    #テストデータをロード    \n",
        "    test_images = mnist.test.images    \n",
        "    test_labels = mnist.test.labels    \n",
        "\n",
        "    step = 0    \n",
        "    for i in range(1000):\n",
        "        step = i + 1        \n",
        "        train_images, train_labels = mnist.train.next_batch(50)\n",
        "        sess.run(train_step, feed_dict={x:train_images ,y:train_labels})\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            acc_val = sess.run(accuracy ,feed_dict={x:test_images, y:test_labels})\n",
        "            print('Step %d: accuracy = %.3f' % (step, acc_val))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TensorFlowDL-samples'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Total 73 (delta 0), reused 0 (delta 0), pack-reused 73\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-c6156c8d9581>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Step 100: accuracy = 0.883\n",
            "Step 200: accuracy = 0.908\n",
            "Step 300: accuracy = 0.919\n",
            "Step 400: accuracy = 0.927\n",
            "Step 500: accuracy = 0.939\n",
            "Step 600: accuracy = 0.946\n",
            "Step 700: accuracy = 0.950\n",
            "Step 800: accuracy = 0.944\n",
            "Step 900: accuracy = 0.953\n",
            "Step 1000: accuracy = 0.941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHSKcdTAh34r",
        "colab_type": "text"
      },
      "source": [
        "# 実装上の課題\n",
        "これまでのところで大まかなAIの仕組みについて学習を行った。ここではAIを実際に実装する場合に問題となる諸課題について概覧する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Nhb3Cbip3l",
        "colab_type": "text"
      },
      "source": [
        "## 勾配降下法の問題点\n",
        "\n",
        "勾配降下法によって最も損失関数が小さくなる値を求める場合、問題としてはその出発点によっては目標の最小値にたどり着けない点である。\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vQhdzzAYjQ8Uxgs4-7bZ1soV_wPt1M47ll436PPh2y6d-rOE9cU4voWgVO9QcMIFCq5V6-OV9D7DWbf/pub?w=791&h=386)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNxOjZlCkGnQ",
        "colab_type": "text"
      },
      "source": [
        "### SGD(確率的勾配降下法）\n",
        "先に実装した例は確率的勾配降下法と呼ばれる手法である。これは与えられたデータ毎にWeightとLossのグラフ形状に揺らぎが生じることを利用している。この特性により、全体の中の一部をランダムに選び出すことで、落とし穴のないデータが適宜挿入されることにより、確率的に目標の値に着地させる方法である。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHKb_BQxmkXk",
        "colab_type": "text"
      },
      "source": [
        "#### 標準的な探索方法\n",
        "\n",
        "基本的には微分値がマイナスとなる方向に予め定められた量を移動させることで、最適解を探索する。\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vRwN3IoFQ1PPyVor6NoQbXQohRqEc9S1SRLO7zHR2KbbgxkR7EpC-GUqUzKazvEsrUhJ3dH6M3Gb3Ze/pub?w=791&h=386)\n",
        "\n",
        "この予め定めらた量は学習率（Learning Rate）ηで示される。この学習率の設定は経験的に定められる。上図の場合では学習率が大きいため、一旦かなり最適解に近づくものの、その次では最適解を通り越してしまい、いわゆる振り子状態が長く続く点が問題となる。\n",
        "\n",
        "#### Momentumによる探索\n",
        "これは常にひつ前の移動量を保存しておき、その前回移動量の一定割合をマイナスに作用させる探索方法であり、これにより見かけ上は慣性的な力が加わることで、自由振り子状態を緩和させることができる。\n",
        "\n",
        "#### AdaGrad\n",
        "これは各学習段階の履歴を保存し、過去の学習量に比例して移動量を減らしてゆく手法である。\n",
        "\n",
        "#### Adam\n",
        "Momentum + AdaGradによる探索法\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNwK5Dh6wpQZ",
        "colab_type": "text"
      },
      "source": [
        "## 重みの初期値\n",
        "例題では重みの初期値はガウス分布を用いたが、代表的な初期値は２つ。\n",
        "\n",
        "### Xaivierの初期値　ザビエル\n",
        "　Xaivierの初期値は様々なニューラルネットで用いられている。sigmoid関数やtanh関数を活性化関数として用いる時、このXavierの初期値を用いるとよい。\n",
        "\n",
        "### Heの初期値\n",
        "　Heの初期値はReLU関数を活性化関数とするときに用いるとよい。ReLU関数を活性化関数としたモデルにXavierの初期値を用いるとアクティベーション分布に偏りが生じやすくなることが知られている。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwLF9myly9Bq",
        "colab_type": "text"
      },
      "source": [
        "## Batch Normalization\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vSa-QlDlMx_WFHsisso0ulsManhVjcWxVxWfbZtlVesNnlaSnlK2gOyLk-dS8DPbWuHMMUexEWTwhqL/pub?w=397&h=206)\n",
        "\n",
        "これはミニバッチ単位に入力データの値を平均がゼロ、分散が１となるように統計的に調整するレイヤーを挟み込むことで下記のような効果が得られる。\n",
        "* 学習が早く進む\n",
        "* 初期パラメータへの依存度が下がる\n",
        "* 過学習を抑制できる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNsvfi0O01dO",
        "colab_type": "text"
      },
      "source": [
        "## 過学習への対応\n",
        "　過学習とは訓練データにフィットしすぎて未知データに対してはかえって認識率が低くなる状態をさす。この過学習を防ぐための仕組みとしてWeight DecayとDropoutについて述べる\n",
        "\n",
        "### Weight Decay\n",
        "　過学習の原因として重みの値が極端に大きくなる事が知られており、これに対応する方法として構築された手法。  \n",
        "　損失関数に重みWを要素とした値を加えることで、大きな重みが損失を大きくするように調整する。\n",
        "\n",
        "### Drop Out\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vRhsO9inUmkCqld72pxTJNfqejbccG7V0WeBBZq-TmtrEWpv_shAhVb_O-8KbRPD0FXDwxX99iIBhYj/pub?w=595&h=229)\n",
        "\n",
        "　途中のニューロンをミニバッチ毎にランダムに間引くことで、信号の経路を複数パターンで学習させる。その際に飛ばされたニューロンを代替するニューロンは余計に信号を学習する？　ことで全体の効率が上がる？　らしい。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMHip9gy0EvT",
        "colab_type": "text"
      },
      "source": [
        "# CNN畳み込みニューラルネットワーク\n",
        "　畳み込みニューラルネットワークとは、画像処理のような縦、横、色深度などの次元をもったデータの解析を行う場合に、一旦その次元を保ったまま画像の特徴を抽出・正規化し、これを最終層ではこれまでに学んだ隠れ層による一次元のニューラルネットワークに接続させる構造となっている。\n",
        "\n",
        "　画像の処理はそれまでOpenCVなどにより様々な成果が挙げられてきた。画像の圧縮、明暗や色相の変換、画像のエッジ強調、画像のぼかしなどである。\n",
        "\n",
        "　CNNではこれらの画像に対するフィルターを機械学習により作り出すところが大きなポイントとなる。この画像フィルターをCNNではカーネルとも呼ぶ。\n",
        "\n",
        "[CNN解説](http://sinhrks.hatenablog.com/entry/2014/12/07/203048)\n",
        "\n",
        "![代替テキスト](http://cdn-ak.f.st-hatena.com/images/fotolife/s/sinhrks/20141214/20141214220532.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TILtn869B4Vf",
        "colab_type": "text"
      },
      "source": [
        "## 畳込み層\n",
        "\n",
        "### フィルター演算の基本\n",
        "\n",
        "![代替テキスト](https://kenyu-life.com/wp-content/uploads/2019/03/cnn.gif)\n",
        "\n",
        "　画像データにフィルターとなる数字群を重ね合わせ、それぞれを掛け算した合計値を「特徴マップ」として出力する。\n",
        "\n",
        "　このフィルター（カーネル）の数値が機械学習によって決定されてゆく。これは先のニューラルネットワークの重みWと同様の手順（バックプロパゲーション）で求める。\n",
        "\n",
        "### ストライド\n",
        "　フィルターの移動量をストライドと呼ぶ。上図ではストライド１である。ストライドを２とすると当然であるが、出力される特徴マップはサイズが小さくなる。\n",
        "\n",
        "\n",
        "### パディング\n",
        "　単純にフィルターを適用すると、必ず画像のサイズが小さくなってしまう。これを防ぐためにパディングとよばれる値がゼロのデータを画像データに予め付加して、フィルター演算によるサイズ低下を防ぐ事ができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZONoSP__gMQ",
        "colab_type": "text"
      },
      "source": [
        "## ３次元での畳込み層\n",
        "\n",
        "![代替テキスト](https://kenyu-life.com/wp-content/uploads/2019/03/3cnn.gif)\n",
        "\n",
        "　縦、横、色深度の三次元データでは、フィルターも３次元必要となる。なお、各フィルターの内容は異なるが、フィルターのサイズはすべて同じである。\n",
        "\n",
        "　このフィルターの出力値のそれぞれにバイアス値を掛け、合計した結果が特徴マップとして出力される。\n",
        "\n",
        "　画像データ的に言うと、カラーがグレースケールに変換されるイメージ。\n",
        "\n",
        "\n",
        "### 複数フィルター\n",
        "　フィルターは通常、複数設定される。上記の例ではRGBのフィルターが複数設定されることとなる。\n",
        "\n",
        "\n",
        "### バッチ処理\n",
        "　ここでもバッチ処理を実施するために、複数枚の画像を一度に処理する。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uHib5aF_mnY",
        "colab_type": "text"
      },
      "source": [
        "## プーリング層\n",
        "\n",
        "![代替テキスト](https://kenyu-life.com/wp-content/uploads/2019/03/pooling.gif)\n",
        "\n",
        "　プーリング層は画像を圧縮する。\n",
        "\n",
        "### maxスプーリング\n",
        "　指定された領域の最大値をマッピングする\n",
        "\n",
        "### avgプーリング\n",
        "　指定された領域の平均値をマッピングする\n",
        "\n",
        "\n",
        "　このプーリング層により画像の圧縮が行われ、細かい部分が削られる。全体としての情報量は下がるが、より汎化された画像を得ることができる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzb7_Q0vc14z",
        "colab_type": "text"
      },
      "source": [
        "## 全体構成\n",
        "\n",
        "![代替テキスト](https://docs.google.com/drawings/d/e/2PACX-1vSh0uknkhuKIBI4ZsJDiGhyMKjtwaKRGIHPfgMrHOagTetYhyOttnLpMnF0rf5ntM4ofp16EmUjPk3x/pub?w=678&h=204)\n",
        "\n",
        "　CNNでは新たに　Convolution　畳込みレイヤ　と　Pooling　プーリングレイヤが加わる。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qREZUPzQeEF-",
        "colab_type": "code",
        "outputId": "70bcdae8-33e8-4066-c407-6a777f986e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/deep-learning-from-scratch/ch07\n",
        "\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"単純なConvNet\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 入力サイズ（MNISTの場合は784）\n",
        "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
        "    output_size : 出力サイズ（MNISTの場合は10）\n",
        "    activation : 'relu' or 'sigmoid'\n",
        "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
        "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
        "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 重みの初期化\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"損失関数を求める\n",
        "        引数のxは入力データ、tは教師ラベル\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"勾配を求める（数値微分）\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 入力データ\n",
        "        t : 教師ラベル\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        各層の勾配を持ったディクショナリ変数\n",
        "            grads['W1']、grads['W2']、...は各層の重み\n",
        "            grads['b1']、grads['b2']、...は各層のバイアス\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 入力データ\n",
        "        t : 教師ラベル\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        各層の勾配を持ったディクショナリ変数\n",
        "            grads['W1']、grads['W2']、...は各層の重み\n",
        "            grads['b1']、grads['b2']、...は各層のバイアス\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0mZMnhGdo0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 処理に時間のかかる場合はデータを削減 \n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# パラメータの保存\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# グラフの描画\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0G8cTdESSMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/thinkitcojp/TensorFlowDL-samples.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USPBs2V9RpKo",
        "colab_type": "code",
        "outputId": "3381d87e-bf45-46c3-f076-1305ae5a0195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#-*- coding:utf-8 -*-\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#mnistデータを格納したオブジェクトを呼び出す\n",
        "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
        "\n",
        "\"\"\"モデル構築開始\"\"\"\n",
        "#入力データを定義\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "#整形\n",
        "img = tf.reshape(x,[-1,28,28,1])\n",
        "\n",
        "#畳み込み層1\n",
        "f1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1))\n",
        "conv1 = tf.nn.conv2d(img, f1, strides=[1,1,1,1], padding='SAME')\n",
        "b1 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
        "h_conv1 = tf.nn.relu(conv1+b1)\n",
        "\n",
        "#プーリング層1\n",
        "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "#畳み込み層2\n",
        "f2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1))\n",
        "conv2 = tf.nn.conv2d(h_pool1, f2, strides=[1,1,1,1], padding='SAME')\n",
        "b2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
        "h_conv2 = tf.nn.relu(conv2+b2)\n",
        "#プーリング層2\n",
        "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "\n",
        "#畳み込まれているものをフラットな形に変換\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "\n",
        "#全結合層\n",
        "w_fc1 = tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1))\n",
        "b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
        "\n",
        "#出力層\n",
        "w_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
        "b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
        "out = tf.nn.softmax(tf.matmul(h_fc1, w_fc2) + b_fc2)\n",
        "\n",
        "#正解データの型を定義\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "#誤差関数（クロスエントロピー）\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(out + 1e-5), axis=[1]))\n",
        "\n",
        "#訓練\n",
        "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
        "\n",
        "#評価\n",
        "correct = tf.equal(tf.argmax(out,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init =tf.global_variables_initializer()\n",
        "\n",
        "\"\"\"実行部分\"\"\"\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    #テストデータをロード\n",
        "    test_images = mnist.test.images\n",
        "    test_labels = mnist.test.labels\n",
        "\n",
        "    for step in range(1000):\n",
        "        train_images, train_labels = mnist.train.next_batch(50)\n",
        "        sess.run(train_step, feed_dict={x:train_images ,y:train_labels})\n",
        "\n",
        "        #100ステップごとに精度を検証\n",
        "        if step % 100 == 0:\n",
        "            acc_val = sess.run( accuracy, feed_dict={x:test_images, y:test_labels})\n",
        "            print('Step %d: accuracy = %.2f' % (step, acc_val))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "Step 0: accuracy = 0.17\n",
            "Step 100: accuracy = 0.86\n",
            "Step 200: accuracy = 0.90\n",
            "Step 300: accuracy = 0.93\n",
            "Step 400: accuracy = 0.94\n",
            "Step 500: accuracy = 0.93\n",
            "Step 600: accuracy = 0.95\n",
            "Step 700: accuracy = 0.96\n",
            "Step 800: accuracy = 0.96\n",
            "Step 900: accuracy = 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K3ysJirhdOO",
        "colab_type": "text"
      },
      "source": [
        "## 代表的なCNN Lenet\n",
        "\n",
        "![代替テキスト](https://image.slidesharecdn.com/styletemp-161002182243/95/deep-learning-behind-prisma-17-638.jpg?cb=1475432629)\n",
        "\n",
        "「現在のCNN」と比較すると次の点が異なる\n",
        "* 活性化関数にシグモイド関数を使用(現在はReLU関数)\n",
        "* サブサンプリングによって中間データのサイズ縮小を行っている(現在はMaxプーリング)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN0rR80O_8-K",
        "colab_type": "text"
      },
      "source": [
        "## 代表的なCNN ALEXNET\n",
        "　CNNの代表的なネットワーク。２０１２年に発表されAI画像解析のブームの火付け役となった。\n",
        "\n",
        "　　　参考　　[深層学習論文の読解（AlexNet)](https://qiita.com/ttomomasa/items/8243f6a29a024a02512f)\n",
        "\n",
        "\n",
        "　このALEXNETの特徴は\n",
        "* 活性化関数にReLUを用いた\n",
        "* LRNというReLUの値を正規化した\n",
        "* DropOUTを用いた\n",
        "* GPUを使った\n",
        "\n",
        "\n",
        "![代替テキスト](https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F209049%2Fbbf90731-67c0-2d75-f64e-1272ddebf653.jpeg?ixlib=rb-1.2.2&auto=compress%2Cformat&gif-q=60&w=1400&fit=max&s=dedabee207698ba84f04984640e3c60e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZtCAnun5cjw",
        "colab_type": "text"
      },
      "source": [
        "* 入力画像：256×256を4隅＋中央　の５パターンを左右反転で１０倍にしている\n",
        "* 第1層: 224×224×3の画像を11×11×3の96個のkernelに変換\n",
        "* 第2層: LRN & poolingした結果を5×5×48の256個のkernelに変換\n",
        "* 第3層: LRN & poolingした結果を3×3×256の384個のkernelに変換\n",
        "* 第4層: 3×3×192の384個のkernelに変換\n",
        "* 第5層: 3×3×192の256個のkernelに変換\n",
        "* 2つの全結合層: 4096個のニューロン\n",
        "* 出力層: 1000個のsoft-maxニューロン\n",
        "\n",
        "* Per-pixel Mean Substraction: ピクセル・チャンネルごとに平均を出し入力から差し引く。つまり、256×256×3個の平均値を出し、各入力画像から差し引く。\n",
        "\n",
        "* Overlapping Pooling: Max Poolingを取る時に各unitを今までは「重複しない」ようにしていましたが、「少しかぶる」ようにすると過学習がおきにくくなり精度が向上した。\n",
        "\n",
        "* Local Response Normalization(LRN):不明"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMtKAclfmZbt",
        "colab_type": "text"
      },
      "source": [
        "# DeepLarning\n",
        "\n",
        "## 認識精度を高めるための手法\n",
        "\n",
        "### 1 Data Augmentation(データ拡張)\n",
        "\n",
        "Data Augmentaionは、上記の移動や回転の他にも、画像の中から一部を切り出す「crop 処理」や、 左右をひっくり返す「flip 処理」など様々な方法で拡張できる。\n",
        "\n",
        "### 2 層を深くする\n",
        " 層を深くすることの重要性について、理論的に多くのことはわからない\n",
        "ただ、層を深くすることで階層的に情報を渡せる(前の層で抽出した特徴を次層で情報として使えるためより高度な学習ができる)\n",
        "層を深くするメリットは、ネットワークのパラメータ数を少なくできる\n",
        "層を深くすれば学習データを少なく、高速に学習ができる(層を深くすると学習する問題を階層的に分解することができる)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSAnlL3Cmvly",
        "colab_type": "text"
      },
      "source": [
        "### ディープラーニングの小歴史\n",
        "2012 年に開催された大規模画像認識のコンペティション ILSVRC(ImageNet Large Scale Visual Recognition Challenge)で、ディープラーニングによる手法――通称、AlexNet――が、圧倒的な成績 で優勝し、これまでの画像認識に対するアプローチを根底から覆した。まさに、 転換点となった 2012 年のディープラーニングの逆襲により、それ以降のコンペティ ションでは、常にディープラーニングが主役に躍り出た。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2a69G6ylbUZ",
        "colab_type": "text"
      },
      "source": [
        "### VGG\n",
        "CNNのネットワークを深くしたものをVGGという\n",
        "\n",
        "* 3 × 3 の小さなフィルターによる畳み込み層\n",
        "* 活性化関数は ReLU\n",
        "* 全結合層の後に Dropout レイヤを使用\n",
        "* Adam による最適化\n",
        "* 重みの初期値として「He の初期値」を使用\n",
        "\n",
        "\n",
        "![代替テキスト](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcREueOd8UJWmzs6vrCSsOZL20Ysk1p4rqFuHS9hFAXudfu9Oh7r)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unscmVBwnwoS",
        "colab_type": "text"
      },
      "source": [
        "## GoogleNet\n",
        "\n",
        "\n",
        "ネットワークが縦方向の深さだけではなく、横方向にも深さ(広がり)を持っているという点が特徴\n",
        "\n",
        "![代替テキスト](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRQp6Oa6WE55-uZ0sftB4Vwph7kiRG31IszsjFcr7G1a96x3GvB)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leoZVYH9obRZ",
        "colab_type": "text"
      },
      "source": [
        "## Residual Network (ResNet)\n",
        "\n",
        "![代替テキスト](https://www.semiconportal.com/archive/contribution/img/COA160804-01b.gif)\n",
        "\n",
        "* Microsoftが開発\n",
        "* 層をまたいで出力に合算するスキップ構造が特徴\n",
        "\n",
        "* スキップ構造を取り入れることで、層を深くしても効率良く学習することができます。これは、逆伝播の際に、スキップ構造によって信号が減衰することなく伝わっていくから。\n",
        "\n",
        "* スキップ構造は入力データを“そのまま”流すだけ \n",
        "\n",
        "* 勾配が小さくなったり(または大きくなりすぎたり)する心配がなく、層を深くすることで勾配が小さくなる勾配消失問題が軽減\n",
        "\n",
        "* 150 層以上に深くしても認識精度は向上し続ける\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN_3yY0mp3IM",
        "colab_type": "text"
      },
      "source": [
        "## 転移学習\n",
        "\n",
        "* 学習済みの 重みの一部を別のニューラルネットワークにコピーして、再学習を行う。\n",
        "* データセットが少ない場合において有効\n",
        "\n",
        "## ディープラーニングの高速化\n",
        "\n",
        "* GPUによる高速化\n",
        "* 複数のマシンによる並列化\n",
        "* 演算制度の削減による低負荷化(64bit,32bitを16や8bitで演算）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_fs7-23q9uq",
        "colab_type": "text"
      },
      "source": [
        "## ディープラーニングの実用例\n",
        "### R-CNN 物体検出\n",
        "R-CNNと呼ばれる手法が有名で、最初に物体と物体以外を識別する「候補領域抽出」を行い、抽出された領域に対してCNNでクラス分類を行っている\n",
        "\n",
        "![代替テキスト](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQQBYrottMXMFEaLq05Axcv-Fq4j51rkgIjGso6CQLXJaAyxYPV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5TSlFxtrMWo",
        "colab_type": "text"
      },
      "source": [
        "### セグメンテーション\n",
        "画像に対してピクセルレベルでクラス分類を行う。\n",
        "\n",
        "![代替テキスト](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQzZzCHkaMcYOWDZSagGw440ME4Ba4eScEr4FjgGT4HXHWKtpc1)\n",
        "* FCN (Fully Convolutional Network＝全てが畳み込み層のネットワーク)という手法が有名\n",
        "* 1回のforward処理ですべてのピクセルに対してクラス分類を行い、中間データの空間ボリュームを保持したまま最後の出力まで処理する。\n",
        "* 最後にバイリニア補間による拡大をデコンボリューション（逆畳み込み演算）によって実現する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S6hmx-PsWTz",
        "colab_type": "text"
      },
      "source": [
        "## 画像キャプション生成\n",
        "\n",
        "\n",
        "画像を与えると、その画像を説明する文章を自動で生成する  \n",
        "\n",
        "NIC (Neural Image Caption) ではCNNで画像認識、RNN（再帰的なネットワーク）で言語生成を行っている\n",
        "\n",
        "![代替テキスト](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSyXIZmI2t6eOBvL9CC2d7S80fsQedZxmbXozb5Szw_IGGmqxGz)\n",
        "\n",
        "* 画像から CNN によって特徴を抽出し、その特徴をRNNに渡す。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_FF7-NdtOt9",
        "colab_type": "text"
      },
      "source": [
        "## 画像スタイル変換\n",
        "2つの画像を入力し、新しい画像を生成する\n",
        "\n",
        "![代替テキスト](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcR_peinKV3hwaxIgIEg2ZDbUcd9XsQk8NI5HAhMcQMEvMxCYAAy)\n",
        "\n",
        "* ゴッホの描画スタイルを、コンテンツ画像に適用するように指定すれば、ディープラーニングが、指定されたとおりに新しい絵画を描いてくれる。\n",
        "* A Neural Algorithm of Artistic Styleという論文"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJTsbp_NudHz",
        "colab_type": "text"
      },
      "source": [
        "## 画像生成\n",
        "\n",
        "DCGAN によって新たに生成されたベッドルームの画像 ![代替テキスト](https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F67217%2F45999acd-65f7-649a-1136-db1f2f1d6dea.png?ixlib=rb-1.2.2&auto=format&gif-q=60&q=75&s=fae4dc2198ddfef7facb3fb8a18b5cdf)\n",
        "\n",
        "* DCGAN は、Generator(生成する人)と Discriminator(識別する人)の 2 つのニューラルネットワークを利用している。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdNEvtgGu_4P",
        "colab_type": "text"
      },
      "source": [
        "## 自動運転\n",
        "CNNベースのSegNetは走路環境のセグメンテーションを高精度で行う。\n",
        "\n",
        "\n",
        "![代替テキスト](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRlmWRTULgm9E9rOFssg5_jxs2TQ32K-IQm636tGF59MiPvcf0o)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF59rrvCmVb-",
        "colab_type": "text"
      },
      "source": [
        "## Deep Q-Network(強化学習)　DQN\n",
        "強化学習では、エージェントと呼ばれるものが、環境の状況に応じて行動を選択し、その行動によって環境が変化するというのが基本的な枠組み。\n",
        "![代替テキスト](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRMacn8NEkjkEM6cFA-TW7OgMV4UyiFxuEQGXxAG9ZQxdYjxqCG)\n",
        "\n",
        "* 環境の変化によって、エージェントは何らかの報酬を得る。強化学習での目的は、より良い報酬が得られるようにエージェントの行動指針を決めるという点にある\n",
        "\n",
        "* AlphaGo が囲碁のチャンピオンを破った。この AlphaGoでも、強化学習が用いられてる。"
      ]
    }
  ]
}